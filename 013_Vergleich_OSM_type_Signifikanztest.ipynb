{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178cba9-d1a5-43e4-a3f4-00095a2e86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich zwischen den OSM \"type\" Kategorien: Signifikanztest mit Kruskal-Wallis Test, Dunn's post-hoc Test, Cohen's d Effektmatrix (Segmente gewichtet nach Länge)\n",
    "# Zur Erstellung des Codes wurde die generative Künstliche Intelligenz (KI) „Claude AI“ des Anbieters Anthropic in Version 3.7 genutzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9011ec8-8347-4f4b-b216-efecbce3491e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting consistent length-weighted analysis of parquet file: data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\n",
      "Parquet file has 1 row groups and approximately 592,136 rows\n",
      "Processing 1 row groups to extract speed data by street type\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1927f094763f46c8b143a50d194fbf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing row groups:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6034ae9a684c88ab97fb821ce386c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 0:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 587,967 total street segments across 15 different street types\n",
      "Analysis will include 15 street types with sufficient data\n",
      "1. footway: 236,139 segments\n",
      "2. residential: 121,106 segments\n",
      "3. secondary: 41,187 segments\n",
      "4. service_driveway: 31,506 segments\n",
      "5. tertiary: 30,352 segments\n",
      "6. service: 28,692 segments\n",
      "7. path: 28,627 segments\n",
      "8. cycleway: 16,089 segments\n",
      "9. track: 13,789 segments\n",
      "10. primary: 12,001 segments\n",
      "11. service_parking_aisle: 10,052 segments\n",
      "12. unclassified: 6,897 segments\n",
      "13. living_street: 6,162 segments\n",
      "14. steps: 4,065 segments\n",
      "15. pedestrian: 1,303 segments\n",
      "Calculating length-weighted statistics for each street type\n",
      "footway: weighted mean = 18.76 km/h, weighted median = 18.76 km/h, effective n = 91694.3\n",
      "residential: weighted mean = 18.98 km/h, weighted median = 18.92 km/h, effective n = 58311.4\n",
      "secondary: weighted mean = 20.86 km/h, weighted median = 20.73 km/h, effective n = 19492.7\n",
      "service_driveway: weighted mean = 12.51 km/h, weighted median = 12.05 km/h, effective n = 9726.7\n",
      "tertiary: weighted mean = 20.89 km/h, weighted median = 20.75 km/h, effective n = 14177.6\n",
      "service: weighted mean = 18.23 km/h, weighted median = 18.43 km/h, effective n = 13127.1\n",
      "path: weighted mean = 21.23 km/h, weighted median = 21.03 km/h, effective n = 17382.4\n",
      "cycleway: weighted mean = 20.17 km/h, weighted median = 20.09 km/h, effective n = 5804.2\n",
      "track: weighted mean = 19.29 km/h, weighted median = 19.06 km/h, effective n = 10602.5\n",
      "primary: weighted mean = 20.70 km/h, weighted median = 20.60 km/h, effective n = 5790.5\n",
      "service_parking_aisle: weighted mean = 15.78 km/h, weighted median = 16.06 km/h, effective n = 5120.3\n",
      "unclassified: weighted mean = 21.60 km/h, weighted median = 21.46 km/h, effective n = 3849.8\n",
      "living_street: weighted mean = 17.13 km/h, weighted median = 16.77 km/h, effective n = 3038.1\n",
      "steps: weighted mean = 11.35 km/h, weighted median = 9.53 km/h, effective n = 2083.9\n",
      "pedestrian: weighted mean = 15.96 km/h, weighted median = 15.69 km/h, effective n = 682.6\n",
      "Creating visualizations based on length-weighted histograms\n",
      "Creating length-weighted histogram comparison\n",
      "Saved length-weighted histogram comparison\n",
      "Creating median-weighted histogram comparison\n",
      "Saved median-weighted histogram comparison\n",
      "Creating boxplot from length-weighted histograms\n",
      "Creating violin plot from length-weighted histograms\n",
      "Creating mean vs median comparison plot\n",
      "\n",
      "Comparison of weighted means and medians:\n",
      "unclassified: Mean = 21.60, Median = 21.46, Diff = 0.14, Effective N = 3849.8\n",
      "path: Mean = 21.23, Median = 21.03, Diff = 0.20, Effective N = 17382.4\n",
      "tertiary: Mean = 20.89, Median = 20.75, Diff = 0.14, Effective N = 14177.6\n",
      "secondary: Mean = 20.86, Median = 20.73, Diff = 0.12, Effective N = 19492.7\n",
      "primary: Mean = 20.70, Median = 20.60, Diff = 0.09, Effective N = 5790.5\n",
      "cycleway: Mean = 20.17, Median = 20.09, Diff = 0.08, Effective N = 5804.2\n",
      "track: Mean = 19.29, Median = 19.06, Diff = 0.23, Effective N = 10602.5\n",
      "residential: Mean = 18.98, Median = 18.92, Diff = 0.06, Effective N = 58311.4\n",
      "footway: Mean = 18.76, Median = 18.76, Diff = 0.00, Effective N = 91694.3\n",
      "service: Mean = 18.23, Median = 18.43, Diff = -0.20, Effective N = 13127.1\n",
      "living_street: Mean = 17.13, Median = 16.77, Diff = 0.35, Effective N = 3038.1\n",
      "pedestrian: Mean = 15.96, Median = 15.69, Diff = 0.27, Effective N = 682.6\n",
      "service_parking_aisle: Mean = 15.78, Median = 16.06, Diff = -0.28, Effective N = 5120.3\n",
      "service_driveway: Mean = 12.51, Median = 12.05, Diff = 0.46, Effective N = 9726.7\n",
      "steps: Mean = 11.35, Median = 9.53, Diff = 1.82, Effective N = 2083.9\n",
      "\n",
      "Performing Kruskal-Wallis test followed by Dunn's post-hoc test (sorting by mean)\n",
      "Kruskal-Wallis test results:\n",
      "H statistic: 24183.4679\n",
      "p-value: 0.000000\n",
      "\n",
      "Significant differences found between street types (p < 0.05)\n",
      "Performing Dunn's test for post-hoc analysis\n",
      "\n",
      "Significantly different street type pairs with length-weighted effect sizes:\n",
      "unclassified vs living_street: p=0.000000, weighted mean diff=4.47 km/h, Cohen's d=0.755 (large)\n",
      "unclassified vs pedestrian: p=0.000000, weighted mean diff=5.64 km/h, Cohen's d=0.897 (large)\n",
      "unclassified vs service_parking_aisle: p=0.000000, weighted mean diff=5.82 km/h, Cohen's d=0.826 (large)\n",
      "unclassified vs service_driveway: p=0.000000, weighted mean diff=9.09 km/h, Cohen's d=1.285 (large)\n",
      "unclassified vs steps: p=0.000000, weighted mean diff=10.25 km/h, Cohen's d=1.485 (large)\n",
      "path vs living_street: p=0.000000, weighted mean diff=4.10 km/h, Cohen's d=0.706 (medium)\n",
      "path vs pedestrian: p=0.000000, weighted mean diff=5.27 km/h, Cohen's d=0.853 (large)\n",
      "path vs service_parking_aisle: p=0.000000, weighted mean diff=5.44 km/h, Cohen's d=0.784 (large)\n",
      "path vs service_driveway: p=0.000000, weighted mean diff=8.72 km/h, Cohen's d=1.250 (large)\n",
      "path vs steps: p=0.000000, weighted mean diff=9.88 km/h, Cohen's d=1.452 (large)\n",
      "tertiary vs living_street: p=0.000000, weighted mean diff=3.76 km/h, Cohen's d=0.626 (medium)\n",
      "tertiary vs pedestrian: p=0.000000, weighted mean diff=4.93 km/h, Cohen's d=0.774 (large)\n",
      "tertiary vs service_parking_aisle: p=0.000000, weighted mean diff=5.10 km/h, Cohen's d=0.718 (medium)\n",
      "tertiary vs service_driveway: p=0.000000, weighted mean diff=8.38 km/h, Cohen's d=1.173 (large)\n",
      "tertiary vs steps: p=0.000000, weighted mean diff=9.54 km/h, Cohen's d=1.368 (large)\n",
      "secondary vs living_street: p=0.000000, weighted mean diff=3.73 km/h, Cohen's d=0.619 (medium)\n",
      "secondary vs pedestrian: p=0.000000, weighted mean diff=4.89 km/h, Cohen's d=0.767 (large)\n",
      "secondary vs service_parking_aisle: p=0.000000, weighted mean diff=5.07 km/h, Cohen's d=0.711 (medium)\n",
      "secondary vs service_driveway: p=0.000000, weighted mean diff=8.35 km/h, Cohen's d=1.165 (large)\n",
      "secondary vs steps: p=0.000000, weighted mean diff=9.51 km/h, Cohen's d=1.360 (large)\n",
      "primary vs living_street: p=0.000000, weighted mean diff=3.57 km/h, Cohen's d=0.589 (medium)\n",
      "primary vs pedestrian: p=0.000000, weighted mean diff=4.74 km/h, Cohen's d=0.738 (medium)\n",
      "primary vs service_parking_aisle: p=0.000000, weighted mean diff=4.91 km/h, Cohen's d=0.686 (medium)\n",
      "primary vs service_driveway: p=0.000000, weighted mean diff=8.19 km/h, Cohen's d=1.139 (large)\n",
      "primary vs steps: p=0.000000, weighted mean diff=9.35 km/h, Cohen's d=1.331 (large)\n",
      "cycleway vs pedestrian: p=0.000000, weighted mean diff=4.20 km/h, Cohen's d=0.672 (medium)\n",
      "cycleway vs service_parking_aisle: p=0.000000, weighted mean diff=4.38 km/h, Cohen's d=0.624 (medium)\n",
      "cycleway vs service_driveway: p=0.000000, weighted mean diff=7.66 km/h, Cohen's d=1.086 (large)\n",
      "cycleway vs steps: p=0.000000, weighted mean diff=8.82 km/h, Cohen's d=1.282 (large)\n",
      "track vs service_driveway: p=0.000000, weighted mean diff=6.78 km/h, Cohen's d=0.963 (large)\n",
      "track vs steps: p=0.000000, weighted mean diff=7.94 km/h, Cohen's d=1.156 (large)\n",
      "residential vs service_driveway: p=0.000000, weighted mean diff=6.47 km/h, Cohen's d=0.910 (large)\n",
      "residential vs steps: p=0.000000, weighted mean diff=7.63 km/h, Cohen's d=1.100 (large)\n",
      "footway vs service_driveway: p=0.000000, weighted mean diff=6.25 km/h, Cohen's d=0.859 (large)\n",
      "footway vs steps: p=0.000000, weighted mean diff=7.41 km/h, Cohen's d=1.042 (large)\n",
      "service vs service_driveway: p=0.000000, weighted mean diff=5.72 km/h, Cohen's d=0.764 (large)\n",
      "service vs steps: p=0.000000, weighted mean diff=6.88 km/h, Cohen's d=0.939 (large)\n",
      "living_street vs steps: p=0.000000, weighted mean diff=5.78 km/h, Cohen's d=0.826 (large)\n",
      "service_parking_aisle vs steps: p=0.000000, weighted mean diff=4.44 km/h, Cohen's d=0.557 (medium)\n",
      "pedestrian vs steps: p=0.000000, weighted mean diff=4.61 km/h, Cohen's d=0.632 (medium)\n",
      "living_street vs service_driveway: p=0.000000, weighted mean diff=4.62 km/h, Cohen's d=0.645 (medium)\n",
      "unclassified vs service: p=0.000000, weighted mean diff=3.37 km/h, Cohen's d=0.535 (medium)\n",
      "cycleway vs living_street: p=0.000000, weighted mean diff=3.04 km/h, Cohen's d=0.515 (medium)\n",
      "track vs pedestrian: p=0.000000, weighted mean diff=3.33 km/h, Cohen's d=0.533 (medium)\n",
      "path vs service: p=0.000000, weighted mean diff=3.00 km/h, Cohen's d=0.484 (medium)\n",
      "unclassified vs footway: p=0.000000, weighted mean diff=2.84 km/h, Cohen's d=0.468 (medium)\n",
      "residential vs pedestrian: p=0.000000, weighted mean diff=3.02 km/h, Cohen's d=0.477 (medium)\n",
      "track vs service_parking_aisle: p=0.000000, weighted mean diff=3.51 km/h, Cohen's d=0.500 (medium)\n",
      "service_parking_aisle vs service_driveway: p=0.000000, weighted mean diff=3.27 km/h, Cohen's d=0.404 (medium)\n",
      "unclassified vs residential: p=0.000000, weighted mean diff=2.62 km/h, Cohen's d=0.448 (medium)\n",
      "footway vs pedestrian: p=0.000000, weighted mean diff=2.80 km/h, Cohen's d=0.430 (medium)\n",
      "tertiary vs service: p=0.000000, weighted mean diff=2.66 km/h, Cohen's d=0.417 (medium)\n",
      "secondary vs service: p=0.000000, weighted mean diff=2.63 km/h, Cohen's d=0.410 (medium)\n",
      "path vs footway: p=0.000000, weighted mean diff=2.47 km/h, Cohen's d=0.414 (medium)\n",
      "residential vs service_parking_aisle: p=0.000000, weighted mean diff=3.19 km/h, Cohen's d=0.451 (medium)\n",
      "unclassified vs track: p=0.000000, weighted mean diff=2.31 km/h, Cohen's d=0.399 (medium)\n",
      "pedestrian vs service_driveway: p=0.000000, weighted mean diff=3.45 km/h, Cohen's d=0.463 (medium)\n",
      "path vs residential: p=0.000000, weighted mean diff=2.25 km/h, Cohen's d=0.392 (medium)\n",
      "primary vs service: p=0.000000, weighted mean diff=2.47 km/h, Cohen's d=0.384 (medium)\n",
      "footway vs service_parking_aisle: p=0.000000, weighted mean diff=2.98 km/h, Cohen's d=0.411 (medium)\n",
      "service vs pedestrian: p=0.000000, weighted mean diff=2.26 km/h, Cohen's d=0.336 (medium)\n",
      "track vs living_street: p=0.000000, weighted mean diff=2.17 km/h, Cohen's d=0.368 (medium)\n",
      "tertiary vs footway: p=0.000000, weighted mean diff=2.12 km/h, Cohen's d=0.346 (medium)\n",
      "secondary vs footway: p=0.000000, weighted mean diff=2.09 km/h, Cohen's d=0.339 (medium)\n",
      "tertiary vs residential: p=0.000000, weighted mean diff=1.91 km/h, Cohen's d=0.322 (medium)\n",
      "path vs track: p=0.000000, weighted mean diff=1.94 km/h, Cohen's d=0.342 (medium)\n",
      "secondary vs residential: p=0.000000, weighted mean diff=1.88 km/h, Cohen's d=0.315 (medium)\n",
      "primary vs footway: p=0.000000, weighted mean diff=1.93 km/h, Cohen's d=0.312 (medium)\n",
      "residential vs living_street: p=0.000000, weighted mean diff=1.85 km/h, Cohen's d=0.310 (medium)\n",
      "service vs service_parking_aisle: p=0.000000, weighted mean diff=2.44 km/h, Cohen's d=0.328 (medium)\n",
      "cycleway vs service: p=0.000000, weighted mean diff=1.94 km/h, Cohen's d=0.309 (medium)\n",
      "primary vs residential: p=0.000000, weighted mean diff=1.72 km/h, Cohen's d=0.287 (medium)\n",
      "footway vs living_street: p=0.000000, weighted mean diff=1.64 km/h, Cohen's d=0.265 (medium)\n",
      "tertiary vs track: p=0.000000, weighted mean diff=1.59 km/h, Cohen's d=0.272 (medium)\n",
      "secondary vs track: p=0.000000, weighted mean diff=1.56 km/h, Cohen's d=0.266 (medium)\n",
      "primary vs track: p=0.000000, weighted mean diff=1.40 km/h, Cohen's d=0.237 (medium)\n",
      "unclassified vs cycleway: p=0.000000, weighted mean diff=1.43 km/h, Cohen's d=0.248 (medium)\n",
      "cycleway vs footway: p=0.000000, weighted mean diff=1.40 km/h, Cohen's d=0.232 (medium)\n",
      "service vs living_street: p=0.000000, weighted mean diff=1.10 km/h, Cohen's d=0.171 (small)\n",
      "cycleway vs residential: p=0.000000, weighted mean diff=1.19 km/h, Cohen's d=0.204 (medium)\n",
      "path vs cycleway: p=0.000000, weighted mean diff=1.06 km/h, Cohen's d=0.188 (small)\n",
      "service_driveway vs steps: p=0.000000, weighted mean diff=1.16 km/h, Cohen's d=0.145 (small)\n",
      "cycleway vs track: p=0.000000, weighted mean diff=0.87 km/h, Cohen's d=0.152 (small)\n",
      "unclassified vs primary: p=0.000000, weighted mean diff=0.90 km/h, Cohen's d=0.151 (small)\n",
      "living_street vs pedestrian: p=0.000000, weighted mean diff=1.17 km/h, Cohen's d=0.183 (small)\n",
      "track vs service: p=0.000000, weighted mean diff=1.07 km/h, Cohen's d=0.170 (small)\n",
      "tertiary vs cycleway: p=0.000000, weighted mean diff=0.72 km/h, Cohen's d=0.123 (small)\n",
      "secondary vs cycleway: p=0.000000, weighted mean diff=0.69 km/h, Cohen's d=0.117 (small)\n",
      "unclassified vs secondary: p=0.000000, weighted mean diff=0.74 km/h, Cohen's d=0.126 (small)\n",
      "unclassified vs tertiary: p=0.000000, weighted mean diff=0.71 km/h, Cohen's d=0.121 (small)\n",
      "path vs primary: p=0.000000, weighted mean diff=0.53 km/h, Cohen's d=0.091 (small)\n",
      "residential vs service: p=0.000000, weighted mean diff=0.75 km/h, Cohen's d=0.118 (small)\n",
      "living_street vs service_parking_aisle: p=0.000000, weighted mean diff=1.34 km/h, Cohen's d=0.188 (small)\n",
      "primary vs cycleway: p=0.000000, weighted mean diff=0.53 km/h, Cohen's d=0.090 (small)\n",
      "track vs footway: p=0.000001, weighted mean diff=0.53 km/h, Cohen's d=0.088 (small)\n",
      "footway vs service: p=0.000025, weighted mean diff=0.54 km/h, Cohen's d=0.082 (small)\n",
      "path vs secondary: p=0.000107, weighted mean diff=0.37 km/h, Cohen's d=0.064 (small)\n",
      "pedestrian vs service_parking_aisle: p=0.000192, weighted mean diff=0.18 km/h, Cohen's d=0.024 (small)\n",
      "unclassified vs path: p=0.000257, weighted mean diff=0.37 km/h, Cohen's d=0.065 (small)\n",
      "track vs residential: p=0.000983, weighted mean diff=0.31 km/h, Cohen's d=0.054 (small)\n",
      "path vs tertiary: p=0.001188, weighted mean diff=0.34 km/h, Cohen's d=0.059 (small)\n",
      "tertiary vs primary: p=0.003079, weighted mean diff=0.19 km/h, Cohen's d=0.031 (small)\n",
      "secondary vs primary: p=0.020277, weighted mean diff=0.16 km/h, Cohen's d=0.026 (small)\n",
      "\n",
      "Performing Kruskal-Wallis test followed by Dunn's post-hoc test (sorting by median)\n",
      "Kruskal-Wallis test results:\n",
      "H statistic: 23978.5104\n",
      "p-value: 0.000000\n",
      "\n",
      "Significant differences found between street types (p < 0.05)\n",
      "Performing Dunn's test for post-hoc analysis\n",
      "\n",
      "Significantly different street type pairs with length-weighted effect sizes:\n",
      "unclassified vs living_street: p=0.000000, weighted mean diff=4.47 km/h, Cohen's d=0.755 (large)\n",
      "unclassified vs service_parking_aisle: p=0.000000, weighted mean diff=5.82 km/h, Cohen's d=0.826 (large)\n",
      "unclassified vs pedestrian: p=0.000000, weighted mean diff=5.64 km/h, Cohen's d=0.897 (large)\n",
      "unclassified vs service_driveway: p=0.000000, weighted mean diff=9.09 km/h, Cohen's d=1.285 (large)\n",
      "unclassified vs steps: p=0.000000, weighted mean diff=10.25 km/h, Cohen's d=1.485 (large)\n",
      "path vs living_street: p=0.000000, weighted mean diff=4.10 km/h, Cohen's d=0.706 (medium)\n",
      "path vs service_parking_aisle: p=0.000000, weighted mean diff=5.44 km/h, Cohen's d=0.784 (large)\n",
      "path vs pedestrian: p=0.000000, weighted mean diff=5.27 km/h, Cohen's d=0.853 (large)\n",
      "path vs service_driveway: p=0.000000, weighted mean diff=8.72 km/h, Cohen's d=1.250 (large)\n",
      "path vs steps: p=0.000000, weighted mean diff=9.88 km/h, Cohen's d=1.452 (large)\n",
      "tertiary vs living_street: p=0.000000, weighted mean diff=3.76 km/h, Cohen's d=0.626 (medium)\n",
      "tertiary vs service_parking_aisle: p=0.000000, weighted mean diff=5.10 km/h, Cohen's d=0.718 (medium)\n",
      "tertiary vs pedestrian: p=0.000000, weighted mean diff=4.93 km/h, Cohen's d=0.774 (large)\n",
      "tertiary vs service_driveway: p=0.000000, weighted mean diff=8.38 km/h, Cohen's d=1.173 (large)\n",
      "tertiary vs steps: p=0.000000, weighted mean diff=9.54 km/h, Cohen's d=1.368 (large)\n",
      "secondary vs living_street: p=0.000000, weighted mean diff=3.73 km/h, Cohen's d=0.619 (medium)\n",
      "secondary vs service_parking_aisle: p=0.000000, weighted mean diff=5.07 km/h, Cohen's d=0.711 (medium)\n",
      "secondary vs pedestrian: p=0.000000, weighted mean diff=4.89 km/h, Cohen's d=0.767 (large)\n",
      "secondary vs service_driveway: p=0.000000, weighted mean diff=8.35 km/h, Cohen's d=1.165 (large)\n",
      "secondary vs steps: p=0.000000, weighted mean diff=9.51 km/h, Cohen's d=1.360 (large)\n",
      "primary vs living_street: p=0.000000, weighted mean diff=3.57 km/h, Cohen's d=0.589 (medium)\n",
      "primary vs service_parking_aisle: p=0.000000, weighted mean diff=4.91 km/h, Cohen's d=0.686 (medium)\n",
      "primary vs pedestrian: p=0.000000, weighted mean diff=4.74 km/h, Cohen's d=0.738 (medium)\n",
      "primary vs service_driveway: p=0.000000, weighted mean diff=8.19 km/h, Cohen's d=1.139 (large)\n",
      "primary vs steps: p=0.000000, weighted mean diff=9.35 km/h, Cohen's d=1.331 (large)\n",
      "cycleway vs service_parking_aisle: p=0.000000, weighted mean diff=4.38 km/h, Cohen's d=0.624 (medium)\n",
      "cycleway vs pedestrian: p=0.000000, weighted mean diff=4.20 km/h, Cohen's d=0.672 (medium)\n",
      "cycleway vs service_driveway: p=0.000000, weighted mean diff=7.66 km/h, Cohen's d=1.086 (large)\n",
      "cycleway vs steps: p=0.000000, weighted mean diff=8.82 km/h, Cohen's d=1.282 (large)\n",
      "track vs service_driveway: p=0.000000, weighted mean diff=6.78 km/h, Cohen's d=0.963 (large)\n",
      "track vs steps: p=0.000000, weighted mean diff=7.94 km/h, Cohen's d=1.156 (large)\n",
      "residential vs service_driveway: p=0.000000, weighted mean diff=6.47 km/h, Cohen's d=0.910 (large)\n",
      "residential vs steps: p=0.000000, weighted mean diff=7.63 km/h, Cohen's d=1.100 (large)\n",
      "footway vs service_driveway: p=0.000000, weighted mean diff=6.25 km/h, Cohen's d=0.859 (large)\n",
      "footway vs steps: p=0.000000, weighted mean diff=7.41 km/h, Cohen's d=1.042 (large)\n",
      "service vs service_driveway: p=0.000000, weighted mean diff=5.72 km/h, Cohen's d=0.764 (large)\n",
      "service vs steps: p=0.000000, weighted mean diff=6.88 km/h, Cohen's d=0.939 (large)\n",
      "living_street vs steps: p=0.000000, weighted mean diff=5.78 km/h, Cohen's d=0.826 (large)\n",
      "service_parking_aisle vs steps: p=0.000000, weighted mean diff=4.44 km/h, Cohen's d=0.557 (medium)\n",
      "unclassified vs service: p=0.000000, weighted mean diff=3.37 km/h, Cohen's d=0.535 (medium)\n",
      "pedestrian vs steps: p=0.000000, weighted mean diff=4.61 km/h, Cohen's d=0.632 (medium)\n",
      "living_street vs service_driveway: p=0.000000, weighted mean diff=4.62 km/h, Cohen's d=0.645 (medium)\n",
      "cycleway vs living_street: p=0.000000, weighted mean diff=3.04 km/h, Cohen's d=0.515 (medium)\n",
      "track vs pedestrian: p=0.000000, weighted mean diff=3.33 km/h, Cohen's d=0.533 (medium)\n",
      "residential vs pedestrian: p=0.000000, weighted mean diff=3.02 km/h, Cohen's d=0.477 (medium)\n",
      "path vs service: p=0.000000, weighted mean diff=3.00 km/h, Cohen's d=0.484 (medium)\n",
      "unclassified vs footway: p=0.000000, weighted mean diff=2.84 km/h, Cohen's d=0.468 (medium)\n",
      "footway vs pedestrian: p=0.000000, weighted mean diff=2.80 km/h, Cohen's d=0.430 (medium)\n",
      "service_parking_aisle vs service_driveway: p=0.000000, weighted mean diff=3.27 km/h, Cohen's d=0.404 (medium)\n",
      "unclassified vs residential: p=0.000000, weighted mean diff=2.62 km/h, Cohen's d=0.448 (medium)\n",
      "track vs service_parking_aisle: p=0.000000, weighted mean diff=3.51 km/h, Cohen's d=0.500 (medium)\n",
      "tertiary vs service: p=0.000000, weighted mean diff=2.66 km/h, Cohen's d=0.417 (medium)\n",
      "secondary vs service: p=0.000000, weighted mean diff=2.63 km/h, Cohen's d=0.410 (medium)\n",
      "unclassified vs track: p=0.000000, weighted mean diff=2.31 km/h, Cohen's d=0.399 (medium)\n",
      "residential vs service_parking_aisle: p=0.000000, weighted mean diff=3.19 km/h, Cohen's d=0.451 (medium)\n",
      "primary vs service: p=0.000000, weighted mean diff=2.47 km/h, Cohen's d=0.384 (medium)\n",
      "footway vs service_parking_aisle: p=0.000000, weighted mean diff=2.98 km/h, Cohen's d=0.411 (medium)\n",
      "path vs footway: p=0.000000, weighted mean diff=2.47 km/h, Cohen's d=0.414 (medium)\n",
      "pedestrian vs service_driveway: p=0.000000, weighted mean diff=3.45 km/h, Cohen's d=0.463 (medium)\n",
      "service vs pedestrian: p=0.000000, weighted mean diff=2.26 km/h, Cohen's d=0.336 (medium)\n",
      "path vs residential: p=0.000000, weighted mean diff=2.25 km/h, Cohen's d=0.392 (medium)\n",
      "track vs living_street: p=0.000000, weighted mean diff=2.17 km/h, Cohen's d=0.368 (medium)\n",
      "tertiary vs footway: p=0.000000, weighted mean diff=2.12 km/h, Cohen's d=0.346 (medium)\n",
      "path vs track: p=0.000000, weighted mean diff=1.94 km/h, Cohen's d=0.342 (medium)\n",
      "secondary vs footway: p=0.000000, weighted mean diff=2.09 km/h, Cohen's d=0.339 (medium)\n",
      "tertiary vs residential: p=0.000000, weighted mean diff=1.91 km/h, Cohen's d=0.322 (medium)\n",
      "secondary vs residential: p=0.000000, weighted mean diff=1.88 km/h, Cohen's d=0.315 (medium)\n",
      "residential vs living_street: p=0.000000, weighted mean diff=1.85 km/h, Cohen's d=0.310 (medium)\n",
      "primary vs footway: p=0.000000, weighted mean diff=1.93 km/h, Cohen's d=0.312 (medium)\n",
      "service vs service_parking_aisle: p=0.000000, weighted mean diff=2.44 km/h, Cohen's d=0.328 (medium)\n",
      "cycleway vs service: p=0.000000, weighted mean diff=1.94 km/h, Cohen's d=0.309 (medium)\n",
      "footway vs living_street: p=0.000000, weighted mean diff=1.64 km/h, Cohen's d=0.265 (medium)\n",
      "tertiary vs track: p=0.000000, weighted mean diff=1.59 km/h, Cohen's d=0.272 (medium)\n",
      "secondary vs track: p=0.000000, weighted mean diff=1.56 km/h, Cohen's d=0.266 (medium)\n",
      "primary vs residential: p=0.000000, weighted mean diff=1.72 km/h, Cohen's d=0.287 (medium)\n",
      "primary vs track: p=0.000000, weighted mean diff=1.40 km/h, Cohen's d=0.237 (medium)\n",
      "unclassified vs cycleway: p=0.000000, weighted mean diff=1.43 km/h, Cohen's d=0.248 (medium)\n",
      "cycleway vs footway: p=0.000000, weighted mean diff=1.40 km/h, Cohen's d=0.232 (medium)\n",
      "service vs living_street: p=0.000000, weighted mean diff=1.10 km/h, Cohen's d=0.171 (small)\n",
      "cycleway vs residential: p=0.000000, weighted mean diff=1.19 km/h, Cohen's d=0.204 (medium)\n",
      "cycleway vs track: p=0.000000, weighted mean diff=0.87 km/h, Cohen's d=0.152 (small)\n",
      "path vs cycleway: p=0.000000, weighted mean diff=1.06 km/h, Cohen's d=0.188 (small)\n",
      "service_driveway vs steps: p=0.000000, weighted mean diff=1.16 km/h, Cohen's d=0.145 (small)\n",
      "living_street vs pedestrian: p=0.000000, weighted mean diff=1.17 km/h, Cohen's d=0.183 (small)\n",
      "unclassified vs primary: p=0.000000, weighted mean diff=0.90 km/h, Cohen's d=0.151 (small)\n",
      "track vs service: p=0.000000, weighted mean diff=1.07 km/h, Cohen's d=0.170 (small)\n",
      "tertiary vs cycleway: p=0.000000, weighted mean diff=0.72 km/h, Cohen's d=0.123 (small)\n",
      "secondary vs cycleway: p=0.000000, weighted mean diff=0.69 km/h, Cohen's d=0.117 (small)\n",
      "unclassified vs secondary: p=0.000000, weighted mean diff=0.74 km/h, Cohen's d=0.126 (small)\n",
      "unclassified vs tertiary: p=0.000000, weighted mean diff=0.71 km/h, Cohen's d=0.121 (small)\n",
      "residential vs service: p=0.000000, weighted mean diff=0.75 km/h, Cohen's d=0.118 (small)\n",
      "primary vs cycleway: p=0.000000, weighted mean diff=0.53 km/h, Cohen's d=0.090 (small)\n",
      "living_street vs service_parking_aisle: p=0.000000, weighted mean diff=1.34 km/h, Cohen's d=0.188 (small)\n",
      "footway vs service: p=0.000000, weighted mean diff=0.54 km/h, Cohen's d=0.082 (small)\n",
      "path vs primary: p=0.000000, weighted mean diff=0.53 km/h, Cohen's d=0.091 (small)\n",
      "unclassified vs path: p=0.000001, weighted mean diff=0.37 km/h, Cohen's d=0.065 (small)\n",
      "service_parking_aisle vs pedestrian: p=0.000006, weighted mean diff=-0.18 km/h, Cohen's d=0.024 (small)\n",
      "path vs secondary: p=0.003208, weighted mean diff=0.37 km/h, Cohen's d=0.064 (small)\n",
      "track vs footway: p=0.005106, weighted mean diff=0.53 km/h, Cohen's d=0.088 (small)\n",
      "path vs tertiary: p=0.006828, weighted mean diff=0.34 km/h, Cohen's d=0.059 (small)\n",
      "tertiary vs primary: p=0.015651, weighted mean diff=0.19 km/h, Cohen's d=0.031 (small)\n",
      "secondary vs primary: p=0.029813, weighted mean diff=0.16 km/h, Cohen's d=0.026 (small)\n",
      "\n",
      "Comparing mean-based and median-based significant pair findings:\n",
      "Number of pairs significant in both tests: 100\n",
      "Number of pairs significant only in mean-based test: 3\n",
      "Number of pairs significant only in median-based test: 2\n",
      "\n",
      "Pairs significant in both tests:\n",
      "track vs pedestrian\n",
      "cycleway vs footway\n",
      "secondary vs cycleway\n",
      "service vs pedestrian\n",
      "tertiary vs secondary\n",
      "path vs tertiary\n",
      "primary vs steps\n",
      "service_parking_aisle vs steps\n",
      "tertiary vs residential\n",
      "residential vs service_driveway\n",
      "living_street vs pedestrian\n",
      "secondary vs service\n",
      "secondary vs living_street\n",
      "footway vs service\n",
      "footway vs living_street\n",
      "cycleway vs pedestrian\n",
      "track vs residential\n",
      "path vs cycleway\n",
      "unclassified vs footway\n",
      "unclassified vs path\n",
      "primary vs pedestrian\n",
      "path vs service\n",
      "path vs living_street\n",
      "tertiary vs service_driveway\n",
      "cycleway vs steps\n",
      "cycleway vs residential\n",
      "unclassified vs pedestrian\n",
      "tertiary vs service_parking_aisle\n",
      "service vs service_driveway\n",
      "primary vs residential\n",
      "track vs service_parking_aisle\n",
      "service vs service_parking_aisle\n",
      "tertiary vs track\n",
      "footway vs steps\n",
      "residential vs service\n",
      "unclassified vs steps\n",
      "unclassified vs residential\n",
      "residential vs living_street\n",
      "secondary vs footway\n",
      "pedestrian vs steps\n",
      "living_street vs service_driveway\n",
      "residential vs footway\n",
      "cycleway vs service_driveway\n",
      "primary vs cycleway\n",
      "tertiary vs cycleway\n",
      "living_street vs service_parking_aisle\n",
      "cycleway vs service_parking_aisle\n",
      "primary vs service_driveway\n",
      "service_parking_aisle vs service_driveway\n",
      "path vs steps\n",
      "unclassified vs primary\n",
      "secondary vs pedestrian\n",
      "footway vs pedestrian\n",
      "path vs footway\n",
      "tertiary vs service\n",
      "tertiary vs living_street\n",
      "primary vs service_parking_aisle\n",
      "cycleway vs track\n",
      "unclassified vs service_driveway\n",
      "pedestrian vs service_driveway\n",
      "service_driveway vs steps\n",
      "track vs service\n",
      "track vs living_street\n",
      "service vs living_street\n",
      "unclassified vs service_parking_aisle\n",
      "track vs footway\n",
      "unclassified vs secondary\n",
      "path vs primary\n",
      "primary vs track\n",
      "secondary vs steps\n",
      "secondary vs residential\n",
      "path vs pedestrian\n",
      "residential vs steps\n",
      "unclassified vs tertiary\n",
      "unclassified vs track\n",
      "cycleway vs service\n",
      "cycleway vs living_street\n",
      "unclassified vs cycleway\n",
      "path vs secondary\n",
      "secondary vs primary\n",
      "path vs residential\n",
      "primary vs service\n",
      "primary vs living_street\n",
      "tertiary vs steps\n",
      "secondary vs service_driveway\n",
      "tertiary vs footway\n",
      "primary vs footway\n",
      "footway vs service_driveway\n",
      "unclassified vs service\n",
      "unclassified vs living_street\n",
      "secondary vs service_parking_aisle\n",
      "track vs steps\n",
      "footway vs service_parking_aisle\n",
      "service vs steps\n",
      "tertiary vs primary\n",
      "tertiary vs pedestrian\n",
      "path vs service_driveway\n",
      "secondary vs track\n",
      "living_street vs steps\n",
      "path vs service_parking_aisle\n",
      "\n",
      "Pairs significant only in mean-based test:\n",
      "residential vs pedestrian\n",
      "track vs service_driveway\n",
      "pedestrian vs service_parking_aisle\n",
      "\n",
      "Pairs significant only in median-based test:\n",
      "residential vs service_parking_aisle\n",
      "service_parking_aisle vs pedestrian\n",
      "Consistent length-weighted analysis complete. All results are based on the same length-weighted histograms.\n"
     ]
    }
   ],
   "source": [
    "# Vergleich zwischen den OSM \"type\" Kategorien: Signifikanztest mit Kruskal-Wallis Test, Dunn's post-hoc Test, Cohen's d Effektmatrix (Segmente gewichtet nach Länge)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "from scikit_posthocs import posthoc_dunn\n",
    "\n",
    "# Configuration - modify to match your setup\n",
    "PARQUET_FILE = \"data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\"\n",
    "OUTPUT_DIR = \"analysis_results/005_Infra_OSM_type\"\n",
    "COLUMN_TYPE = 'type'  # Column for street type\n",
    "COLUMN_HIST = '2304-2412_speeds'  # Column for speed histogram data\n",
    "COLUMN_LENGTH = 'length_m'  # Column for length in meters\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# Plot configurations\n",
    "PLOT_CONFIG = {\n",
    "    # General\n",
    "    'TEXT_FONT_SIZE': 10,\n",
    "    'TITLE_FONT_SIZE': 16,\n",
    "    'AXIS_LABEL_FONT_SIZE': 12,\n",
    "    'LEGEND_FONT_SIZE': 9,\n",
    "    'DPI': 300,\n",
    "    \n",
    "    # Colors\n",
    "    'BOX_COLOR_1': '#a0ddff',\n",
    "    'BOX_COLOR_2': '#c8e9a0',\n",
    "    'MEDIAN_COLOR': 'darkblue',\n",
    "    'MEAN_COLOR': 'red',\n",
    "    \n",
    "    # Labels\n",
    "    'HISTOGRAM_TITLE': 'Geschwindigkeitshistogramm nach OSM \"type\", 04/23 - 12/24',\n",
    "    'BOXPLOT_TITLE': 'Boxplot Geschwindigkeitsverteilung nach OSM \"type\", 04/23 - 12/24',\n",
    "    'VIOLINPLOT_TITLE': 'Geigenplot Geschwindigkeitsverteilung nach OSM \"type\", 04/23 - 12/24',\n",
    "    'X_LABEL': 'Geschwindigkeit (km/h)',\n",
    "    'Y_LABEL': 'Prozentsatz (%)',\n",
    "    \n",
    "    # Box annotations\n",
    "    'ANNOTATION_BBOX_STYLE': dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "}\n",
    "\n",
    "# Specific street types to analyze\n",
    "STREET_TYPES_TO_ANALYZE = [\n",
    "    'footway',\n",
    "    'residential',\n",
    "    'secondary',\n",
    "    'service_driveway',\n",
    "    'tertiary',\n",
    "    'service',\n",
    "    'path',\n",
    "    'cycleway',\n",
    "    'track',\n",
    "    'primary',\n",
    "    'service_parking_aisle',\n",
    "    'unclassified',\n",
    "    'living_street',\n",
    "    'steps',\n",
    "    'pedestrian'\n",
    "]\n",
    "def log(message):\n",
    "    \"\"\"Print a log message\"\"\"\n",
    "    print(message)\n",
    "\n",
    "def force_gc():\n",
    "    \"\"\"Force garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "def parse_histogram(hist_str):\n",
    "    \"\"\"Parse histogram string to numpy array\"\"\"\n",
    "    try:\n",
    "        if isinstance(hist_str, str):\n",
    "            hist_str = hist_str.strip('[]')\n",
    "            # Fast NumPy parsing\n",
    "            try:\n",
    "                values = np.fromstring(hist_str, sep=',')\n",
    "                return values\n",
    "            except:\n",
    "                # Fallback to manual parsing if NumPy method fails\n",
    "                values = [float(x) for x in hist_str.split(',')]\n",
    "                return np.array(values)\n",
    "        else:\n",
    "            return np.zeros(32)  # Return zeros for missing histograms\n",
    "    except Exception as e:\n",
    "        log(f\"Error parsing histogram: {e}\")\n",
    "        return np.zeros(32)\n",
    "\n",
    "def calculate_all_weighted_statistics(histogram):\n",
    "    \"\"\"\n",
    "    Calculate all statistics from a histogram in a single efficient pass.\n",
    "    Returns a dictionary with all statistics.\n",
    "    \"\"\"\n",
    "    if np.sum(histogram) == 0:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'median': np.nan,\n",
    "            'std': np.nan,\n",
    "            'percentile_5': np.nan,\n",
    "            'percentile_25': np.nan,\n",
    "            'percentile_75': np.nan,\n",
    "            'percentile_95': np.nan,\n",
    "            'min': np.nan,\n",
    "            'max': np.nan\n",
    "        }\n",
    "    \n",
    "    speed_bins = np.arange(32)  # 0-31 km/h\n",
    "    total_count = np.sum(histogram)\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    norm_hist = histogram / total_count\n",
    "    \n",
    "    # Calculate weighted mean in one step\n",
    "    mean = np.sum(speed_bins * norm_hist)\n",
    "    \n",
    "    # Calculate variance and std in one step\n",
    "    variance = np.sum(((speed_bins - mean) ** 2) * norm_hist)\n",
    "    std = np.sqrt(variance)\n",
    "    \n",
    "    # Calculate cumulative distribution once\n",
    "    cum_dist = np.cumsum(norm_hist)\n",
    "    \n",
    "    # Find min/max with data present\n",
    "    min_idx = np.nonzero(histogram)[0][0]\n",
    "    max_idx = np.nonzero(histogram)[0][-1]\n",
    "    \n",
    "    # Calculate percentiles efficiently\n",
    "    percentile_5 = np.interp(0.05, cum_dist, speed_bins)\n",
    "    percentile_25 = np.interp(0.25, cum_dist, speed_bins)\n",
    "    median = np.interp(0.5, cum_dist, speed_bins)\n",
    "    percentile_75 = np.interp(0.75, cum_dist, speed_bins)\n",
    "    percentile_95 = np.interp(0.95, cum_dist, speed_bins)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'std': std,\n",
    "        'percentile_5': percentile_5,\n",
    "        'percentile_25': percentile_25,\n",
    "        'percentile_75': percentile_75,\n",
    "        'percentile_95': percentile_95,\n",
    "        'min': speed_bins[min_idx],\n",
    "        'max': speed_bins[max_idx]\n",
    "    }\n",
    "\n",
    "def calculate_cohens_d_from_histograms(hist1, hist2):\n",
    "    \"\"\"Calculate Cohen's d effect size between two histograms\"\"\"\n",
    "    speed_bins = np.arange(32)  # 0-31 km/h\n",
    "    \n",
    "    # Normalize histograms\n",
    "    norm_hist1 = hist1 / np.sum(hist1) if np.sum(hist1) > 0 else np.zeros_like(hist1)\n",
    "    norm_hist2 = hist2 / np.sum(hist2) if np.sum(hist2) > 0 else np.zeros_like(hist2)\n",
    "    \n",
    "    # Calculate means\n",
    "    mean1 = np.sum(speed_bins * norm_hist1)\n",
    "    mean2 = np.sum(speed_bins * norm_hist2)\n",
    "    \n",
    "    # Calculate variances\n",
    "    var1 = np.sum(((speed_bins - mean1) ** 2) * norm_hist1)\n",
    "    var2 = np.sum(((speed_bins - mean2) ** 2) * norm_hist2)\n",
    "    \n",
    "    # Calculate pooled standard deviation\n",
    "    pooled_std = np.sqrt((var1 + var2) / 2)\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = abs(mean1 - mean2) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return d, mean1, mean2\n",
    "\n",
    "def calculate_effective_n(weights):\n",
    "    \"\"\"Calculate effective sample size using relative weight approach\"\"\"\n",
    "    if len(weights) == 0 or np.sum(weights) == 0:\n",
    "        return 0\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    effective_n = np.sum(weights)**2 / np.sum(weights**2)\n",
    "    return effective_n\n",
    "def create_length_weighted_histogram_comparison(histograms, street_types, stats_info):\n",
    "    \"\"\"Create a length-weighted histogram comparison of multiple street types\"\"\"\n",
    "    log(\"Creating length-weighted histogram comparison\")\n",
    "    \n",
    "    # Create speed bins\n",
    "    speed_bins = np.arange(32)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot histograms\n",
    "    for i, (hist, street_type) in enumerate(zip(histograms, street_types)):\n",
    "        # Normalize histogram\n",
    "        hist_norm = hist / np.sum(hist) if np.sum(hist) > 0 else np.zeros_like(hist)\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.bar(speed_bins + i*0.1, hist_norm * 100, alpha=0.7, \n",
    "                label=f\"{street_type} ({stats_info[i]['total_length_km']:.1f} km, n={stats_info[i]['effective_n']:.1f})\", \n",
    "                width=0.8/len(histograms))\n",
    "        \n",
    "        # Add vertical line for mean\n",
    "        plt.axvline(x=stats_info[i]['stats']['mean'], color=f\"C{i}\", linestyle='-', \n",
    "                    label=f\"{street_type} Mean: {stats_info[i]['stats']['mean']:.2f} km/h\")\n",
    "    \n",
    "    # Add legends\n",
    "    plt.legend(loc='upper left', fontsize=PLOT_CONFIG['LEGEND_FONT_SIZE'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.ylabel(PLOT_CONFIG['Y_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.title(PLOT_CONFIG['HISTOGRAM_TITLE'], fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(range(0, 32, 2))\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"length_weighted_histograms.png\"), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "    log(\"Saved length-weighted histogram comparison\")\n",
    "    plt.close()\n",
    "\n",
    "def create_median_weighted_histogram_comparison(histograms, street_types, stats_info):\n",
    "    \"\"\"Create a median-based histogram comparison of multiple street types\"\"\"\n",
    "    log(\"Creating median-weighted histogram comparison\")\n",
    "    \n",
    "    # Create speed bins\n",
    "    speed_bins = np.arange(32)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot histograms\n",
    "    for i, (hist, street_type) in enumerate(zip(histograms, street_types)):\n",
    "        # Normalize histogram\n",
    "        hist_norm = hist / np.sum(hist) if np.sum(hist) > 0 else np.zeros_like(hist)\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.bar(speed_bins + i*0.1, hist_norm * 100, alpha=0.7, \n",
    "                label=f\"{street_type} ({stats_info[i]['total_length_km']:.1f} km, n={stats_info[i]['effective_n']:.1f})\", \n",
    "                width=0.8/len(histograms))\n",
    "        \n",
    "        # Add vertical line for median\n",
    "        plt.axvline(x=stats_info[i]['stats']['median'], color=f\"C{i}\", linestyle='--', \n",
    "                    label=f\"{street_type} Median: {stats_info[i]['stats']['median']:.2f} km/h\")\n",
    "    \n",
    "    # Add legends\n",
    "    plt.legend(loc='upper left', fontsize=PLOT_CONFIG['LEGEND_FONT_SIZE'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.ylabel(PLOT_CONFIG['Y_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.title(\"Geschwindigkeitshistogramm nach OSM \\\"type\\\" (sortiert nach Median), 04/23 - 12/24\", \n",
    "              fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(range(0, 32, 2))\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"median_weighted_histograms.png\"), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "    log(\"Saved median-weighted histogram comparison\")\n",
    "    plt.close()\n",
    "def create_boxplot_from_histograms(street_type_data, valid_types_sorted):\n",
    "    \"\"\"Create boxplot using statistics derived from length-weighted histograms\"\"\"\n",
    "    log(\"Creating boxplot from length-weighted histograms\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    # Prepare statistics\n",
    "    all_stats = []\n",
    "    for street_type in valid_types_sorted:\n",
    "        # Get statistics from histogram\n",
    "        stats = street_type_data[street_type]['stats']\n",
    "        \n",
    "        boxplot_stats = {\n",
    "            'mean': stats['mean'],\n",
    "            'median': stats['median'],\n",
    "            'q1': stats['percentile_25'],\n",
    "            'q3': stats['percentile_75'],\n",
    "            'whislo': stats['percentile_5'],\n",
    "            'whishi': stats['percentile_95'],\n",
    "            'label': (f\"{street_type}\\n\"\n",
    "                     f\"(effective_n={street_type_data[street_type]['effective_n']:.1f}\\n\"\n",
    "                     f\"{street_type_data[street_type]['count']:,} Segmente\\n\"\n",
    "                     f\"{street_type_data[street_type]['total_length_km']:.1f} km\")\n",
    "        }\n",
    "        all_stats.append(boxplot_stats)\n",
    "    \n",
    "    # Create boxplot using Axes.bxp()\n",
    "    ax.bxp(\n",
    "        [{\n",
    "            'med': stats['median'],\n",
    "            'q1': stats['q1'],\n",
    "            'q3': stats['q3'],\n",
    "            'whislo': stats['whislo'],\n",
    "            'whishi': stats['whishi'],\n",
    "            'fliers': [],\n",
    "            'label': stats['label']\n",
    "        } for stats in all_stats],\n",
    "        positions=range(len(all_stats)),\n",
    "        showfliers=False,\n",
    "        patch_artist=True,\n",
    "        boxprops={'facecolor': PLOT_CONFIG['BOX_COLOR_1'], 'alpha': 0.7},\n",
    "        medianprops={'color': PLOT_CONFIG['MEDIAN_COLOR'], 'linewidth': 2},\n",
    "        whiskerprops={'color': 'black', 'linestyle': '-', 'linewidth': 1},\n",
    "        capprops={'color': 'black', 'linewidth': 1}\n",
    "    )\n",
    "    \n",
    "    # Add mean markers and annotations\n",
    "    for i, stats in enumerate(all_stats):\n",
    "        # Mean star\n",
    "        ax.scatter(\n",
    "            i, stats['mean'],\n",
    "            marker='*',\n",
    "            s=150,\n",
    "            color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "            zorder=3\n",
    "        )\n",
    "        \n",
    "        # Blue median text\n",
    "        ax.text(\n",
    "            i, stats['median'] + 0.7,\n",
    "            f\"{stats['median']:.1f}\",\n",
    "            ha='center', va='bottom',\n",
    "            color=PLOT_CONFIG['MEDIAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "        \n",
    "        # Red mean text\n",
    "        ax.text(\n",
    "            i, stats['mean'] - 0.7,\n",
    "            f\"{stats['mean']:.1f}\",\n",
    "            ha='center', va='top',\n",
    "            color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xticks(range(len(all_stats)))\n",
    "    ax.set_xticklabels([stats['label'] for stats in all_stats], rotation=45, ha='right')\n",
    "    ax.set_ylabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    ax.set_title(\n",
    "        \"Boxplot Geschwindigkeitsverteilungen DB Rad+ nach OSM 'type'\\n\"\n",
    "        \"Zeitraum 04/23 - 12/24\",\n",
    "        fontsize=PLOT_CONFIG['TITLE_FONT_SIZE']\n",
    "    )\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=PLOT_CONFIG['BOX_COLOR_1'], alpha=0.7, label='IQR'),\n",
    "        plt.Line2D([0], [0], color=PLOT_CONFIG['MEDIAN_COLOR'], lw=2, label='Median'),\n",
    "        plt.Line2D([0], [0], marker='*', color=PLOT_CONFIG['MEAN_COLOR'], markersize=10, \n",
    "                   linestyle='None', label='Mittelwert')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, 'street_type_boxplot.png'),\n",
    "        dpi=PLOT_CONFIG['DPI'],\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "def create_violin_plot_from_histograms(street_type_data, valid_types_sorted):\n",
    "    \"\"\"Create violin plot using distributions from length-weighted histograms\"\"\"\n",
    "    log(\"Creating violin plot from length-weighted histograms\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    # Draw violins\n",
    "    for i, street_type in enumerate(valid_types_sorted):\n",
    "        hist = street_type_data[street_type]['length_weighted_agg_hist']\n",
    "        speeds = np.arange(32)\n",
    "        \n",
    "        # Create weighted KDE\n",
    "        kde = gaussian_kde(\n",
    "            speeds, \n",
    "            weights=hist,\n",
    "            bw_method=0.25  # Optimal smoothing for speed data\n",
    "        )\n",
    "        x = np.linspace(0, 31, 100)\n",
    "        density = kde(x)\n",
    "        \n",
    "        # Normalize density width\n",
    "        density_norm = 0.4 * density / density.max()  \n",
    "        \n",
    "        # Draw violin body\n",
    "        ax.fill_betweenx(\n",
    "            x, \n",
    "            i - density_norm, \n",
    "            i + density_norm,\n",
    "            color=PLOT_CONFIG['BOX_COLOR_2'],\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Add median line\n",
    "        median = street_type_data[street_type]['stats']['median']\n",
    "        ax.hlines(\n",
    "            median,\n",
    "            i - density_norm.max(),\n",
    "            i + density_norm.max(),\n",
    "            colors=PLOT_CONFIG['MEDIAN_COLOR'],\n",
    "            linewidths=1.5\n",
    "        )\n",
    "        \n",
    "        # Add blue median value above line\n",
    "        ax.text(\n",
    "            i, median + 0.35,\n",
    "            f\"{median:.1f}\",\n",
    "            ha='center', va='bottom',\n",
    "            color=PLOT_CONFIG['MEDIAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "    \n",
    "    # Add weighted means as stars\n",
    "    mean_markers = ax.scatter(\n",
    "        range(len(valid_types_sorted)),\n",
    "        [street_type_data[t]['stats']['mean'] for t in valid_types_sorted],\n",
    "        marker='*',\n",
    "        s=150,\n",
    "        color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "        zorder=3\n",
    "    )\n",
    "    \n",
    "    # Add red mean values below stars\n",
    "    for i, street_type in enumerate(valid_types_sorted):\n",
    "        mean = street_type_data[street_type]['stats']['mean']\n",
    "        ax.text(\n",
    "            i, mean - 0.35,\n",
    "            f\"{mean:.1f}\",\n",
    "            ha='center', va='top',\n",
    "            color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "    \n",
    "    # X-axis labels with metadata\n",
    "    labels = [\n",
    "        f\"{t}\\n\"\n",
    "        f\"n={street_type_data[t]['effective_n']:.1f}\\n\"\n",
    "        f\"{street_type_data[t]['count']:,} segments\\n\"\n",
    "        f\"{street_type_data[t]['total_length_km']:.1f} km\"\n",
    "        for t in valid_types_sorted\n",
    "    ]\n",
    "    \n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(\n",
    "        labels,\n",
    "        rotation=45,\n",
    "        ha='right',\n",
    "        fontsize=PLOT_CONFIG['TEXT_FONT_SIZE']\n",
    "    )\n",
    "\n",
    "    # Titles and styling\n",
    "    ax.set_ylabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    ax.set_title(\n",
    "        \"Geschwindigkeitsverteilungen DB Rad+ nach OSM 'type'\\n\"\n",
    "        \"Geigendiagramm, Zeitraum 04/23 - 12/24\",\n",
    "        fontsize=PLOT_CONFIG['TITLE_FONT_SIZE']\n",
    "    )\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Custom legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=PLOT_CONFIG['BOX_COLOR_2'], alpha=0.7, label='Density'),\n",
    "        plt.Line2D([0], [0], color=PLOT_CONFIG['MEDIAN_COLOR'], lw=1.5, label='Median'),\n",
    "        plt.Line2D([0], [0], marker='*', color=PLOT_CONFIG['MEAN_COLOR'], markersize=10, \n",
    "                   linestyle='None', label='Mean')\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='upper right',\n",
    "        fontsize=PLOT_CONFIG['LEGEND_FONT_SIZE']\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, 'street_type_weighted_violin.png'),\n",
    "        dpi=PLOT_CONFIG['DPI'],\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "def create_mean_vs_median_comparison(street_type_data, valid_types):\n",
    "    \"\"\"Create scatter plot comparing weighted means and medians\"\"\"\n",
    "    log(\"Creating mean vs median comparison plot\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Extract means and medians\n",
    "    means = [street_type_data[t]['stats']['mean'] for t in valid_types]\n",
    "    medians = [street_type_data[t]['stats']['median'] for t in valid_types]\n",
    "    \n",
    "    # Calculate skewness using the difference between mean and median\n",
    "    skewness = [mean - median for mean, median in zip(means, medians)]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(means, medians, \n",
    "                          c=skewness, cmap='coolwarm', alpha=0.9, \n",
    "                          s=100, edgecolors='black')\n",
    "    \n",
    "    # Add diagonal line (y=x)\n",
    "    min_val = min(min(means), min(medians)) - 1\n",
    "    max_val = max(max(means), max(medians)) + 1\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, txt in enumerate(valid_types):\n",
    "        plt.annotate(txt, (means[i], medians[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Add colorbar to show skewness\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Skewness (Mean - Median)')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Weighted Mean Speed (km/h)', fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.ylabel('Weighted Median Speed (km/h)', fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.title('Comparison of Weighted Mean vs Median Speeds by Street Type', fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "    \n",
    "    # Equal aspect ratio\n",
    "    plt.axis('equal')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'mean_vs_median_comparison.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Also create a summary table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Street Type': valid_types,\n",
    "        'Weighted Mean': means,\n",
    "        'Weighted Median': medians,\n",
    "        'Difference (Mean-Median)': skewness,\n",
    "        'Effective N': [street_type_data[t]['effective_n'] for t in valid_types],\n",
    "        'Total Length (km)': [street_type_data[t]['total_length_km'] for t in valid_types]\n",
    "    })\n",
    "    \n",
    "    comparison_df.to_csv(os.path.join(OUTPUT_DIR, 'mean_vs_median_comparison.csv'), index=False)\n",
    "    \n",
    "    # Display the comparison\n",
    "    log(\"\\nComparison of weighted means and medians:\")\n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        log(f\"{row['Street Type']}: Mean = {row['Weighted Mean']:.2f}, Median = {row['Weighted Median']:.2f}, \" +\n",
    "            f\"Diff = {row['Difference (Mean-Median)']:.2f}, Effective N = {row['Effective N']:.1f}\")\n",
    "\n",
    "def perform_kruskal_wallis_and_dunn_tests(street_type_data, valid_types, test_type=\"mean\"):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis test followed by Dunn's post-hoc test directly using \n",
    "    the length-weighted histograms. Test type can be 'mean' or 'median' to determine \n",
    "    sorting of results.\n",
    "    \"\"\"\n",
    "    log(f\"\\nPerforming Kruskal-Wallis test followed by Dunn's post-hoc test (sorting by {test_type})\")\n",
    "    \n",
    "    # Generate samples from histograms for the Kruskal-Wallis test\n",
    "    # We'll limit this to a reasonable number of samples per type to avoid memory issues\n",
    "    max_samples_per_type = 10000\n",
    "    samples_by_type = []\n",
    "    \n",
    "    for street_type in valid_types:\n",
    "        hist = street_type_data[street_type]['length_weighted_agg_hist']\n",
    "        norm_hist = hist / np.sum(hist)\n",
    "        \n",
    "        # Generate samples efficiently using numpy's random choice\n",
    "        bins = np.arange(32)\n",
    "        samples = np.random.choice(bins, size=max_samples_per_type, p=norm_hist)\n",
    "        samples_by_type.append(samples)\n",
    "    \n",
    "    # 1. Perform Kruskal-Wallis test\n",
    "    h_stat, p_value = stats.kruskal(*samples_by_type)\n",
    "    \n",
    "    log(f\"Kruskal-Wallis test results:\")\n",
    "    log(f\"H statistic: {h_stat:.4f}\")\n",
    "    log(f\"p-value: {p_value:.6f}\")\n",
    "    \n",
    "    # Create DataFrame for Dunn's test\n",
    "    all_samples = []\n",
    "    all_types = []\n",
    "    \n",
    "    for i, samples in enumerate(samples_by_type):\n",
    "        all_samples.extend(samples)\n",
    "        all_types.extend([valid_types[i]] * len(samples))\n",
    "    \n",
    "    df_samples = pd.DataFrame({\n",
    "        'speed': all_samples,\n",
    "        'type': all_types\n",
    "    })\n",
    "    \n",
    "    # Calculate Cohen's d effect sizes between all pairs\n",
    "    # Store in a matrix for fast lookups\n",
    "    d_matrix = np.zeros((len(valid_types), len(valid_types)))\n",
    "    mean_diffs = np.zeros((len(valid_types), len(valid_types)))\n",
    "    \n",
    "    for i, type1 in enumerate(valid_types):\n",
    "        for j, type2 in enumerate(valid_types):\n",
    "            if i < j:  # Only calculate once per pair\n",
    "                hist1 = street_type_data[type1]['length_weighted_agg_hist']\n",
    "                hist2 = street_type_data[type2]['length_weighted_agg_hist']\n",
    "                \n",
    "                # Calculate Cohen's d directly from histograms\n",
    "                d, mean1, mean2 = calculate_cohens_d_from_histograms(hist1, hist2)\n",
    "                d_matrix[i, j] = d\n",
    "                d_matrix[j, i] = d  # Mirror for convenience\n",
    "                \n",
    "                diff = mean1 - mean2\n",
    "                mean_diffs[i, j] = diff\n",
    "                mean_diffs[j, i] = -diff  # Mirror with sign change\n",
    "    \n",
    "    # If the Kruskal-Wallis test is significant, perform Dunn's post-hoc test\n",
    "    if p_value < 0.05:\n",
    "        log(\"\\nSignificant differences found between street types (p < 0.05)\")\n",
    "        \n",
    "        # Perform Dunn's test\n",
    "        log(\"Performing Dunn's test for post-hoc analysis\")\n",
    "        dunn_results = posthoc_dunn(df_samples, val_col='speed', group_col='type', p_adjust='fdr_bh')\n",
    "        \n",
    "        # Save Dunn's test results\n",
    "        dunn_results.to_csv(os.path.join(OUTPUT_DIR, f'dunn_test_{test_type}_results.csv'))\n",
    "        \n",
    "        # Create a heatmap of p-values\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        mask = np.triu(np.ones_like(dunn_results, dtype=bool))\n",
    "        \n",
    "        sns.heatmap(dunn_results, mask=mask, annot=True, cmap='coolwarm_r', \n",
    "                  vmin=0, vmax=0.05, center=0.025, \n",
    "                  annot_kws={\"size\": 10}, fmt='.3f')\n",
    "        \n",
    "        plt.title(f\"Vergleich nach OSM 'type': Dunn's post-hoc p-Werte ({test_type}-basiert)\\nSignifikanz: p < 0.05 (rot)\", \n",
    "                fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'dunn_test_{test_type}_heatmap.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Find significantly different pairs and organize into a list\n",
    "        sig_pairs = []\n",
    "        for i in range(len(valid_types)):\n",
    "            for j in range(i+1, len(valid_types)):\n",
    "                type1 = valid_types[i]\n",
    "                type2 = valid_types[j]\n",
    "                \n",
    "                # Get corresponding indices in dunn_results\n",
    "                try:\n",
    "                    dunn_i = dunn_results.index.get_loc(type1)\n",
    "                    dunn_j = dunn_results.columns.get_loc(type2)\n",
    "                    p_val = dunn_results.iloc[dunn_i, dunn_j]\n",
    "                    \n",
    "                    if p_val < 0.05:  # Only include significant pairs\n",
    "                        # Get effect size from pre-computed matrices\n",
    "                        d = d_matrix[i, j]\n",
    "                        diff = mean_diffs[i, j]\n",
    "                        \n",
    "                        sig_pairs.append((type1, type2, p_val, diff, d))\n",
    "                except (KeyError, IndexError):\n",
    "                    log(f\"Warning: Could not find {type1} vs {type2} in Dunn results\")\n",
    "        \n",
    "        # Sort by significance\n",
    "        sig_pairs.sort(key=lambda x: x[2])\n",
    "        \n",
    "        log(\"\\nSignificantly different street type pairs with length-weighted effect sizes:\")\n",
    "        for type1, type2, p_val, diff, d in sig_pairs:\n",
    "            effect_size_interp = \"small\" if d < 0.2 else \"medium\" if d < 0.75 else \"large\"\n",
    "            log(f\"{type1} vs {type2}: p={p_val:.6f}, weighted mean diff={diff:.2f} km/h, Cohen's d={d:.3f} ({effect_size_interp})\")\n",
    "        \n",
    "        # Save effect size results to CSV\n",
    "        effect_size_df = pd.DataFrame(sig_pairs, \n",
    "                                     columns=['Type1', 'Type2', 'p_value', 'weighted_mean_diff', 'cohens_d'])\n",
    "        effect_size_df['effect_size_interpretation'] = effect_size_df['cohens_d'].apply(\n",
    "            lambda d: \"small\" if d < 0.2 else \"medium\" if d < 0.75 else \"large\")\n",
    "        effect_size_df.to_csv(os.path.join(OUTPUT_DIR, f'effect_size_{test_type}_results.csv'), index=False)\n",
    "\n",
    "        # Create a matrix of Cohen's d values with the same structure as dunn_results\n",
    "        # Important: We'll use the same index/column order as dunn_results\n",
    "        cohens_d_df = pd.DataFrame(\n",
    "            np.zeros((len(dunn_results.index), len(dunn_results.columns))),\n",
    "            index=dunn_results.index,\n",
    "            columns=dunn_results.columns\n",
    "        )\n",
    "        \n",
    "        # First, initialize all values to NaN\n",
    "        cohens_d_df.iloc[:, :] = np.nan\n",
    "        \n",
    "        # Then fill in Cohen's d values for ALL pairs (mirror the upper triangle to match Dunn's)\n",
    "        for i in range(len(dunn_results.index)):\n",
    "            for j in range(len(dunn_results.columns)):\n",
    "                if i < j:  # Upper triangle (to match standard scientific reporting)\n",
    "                    type_i = dunn_results.index[i]\n",
    "                    type_j = dunn_results.columns[j]\n",
    "                    \n",
    "                    # Check if this pair is statistically significant\n",
    "                    if dunn_results.iloc[i, j] < 0.05:\n",
    "                        # Get indices in valid_types list\n",
    "                        try:\n",
    "                            idx_i = valid_types.index(type_i)\n",
    "                            idx_j = valid_types.index(type_j)\n",
    "                            \n",
    "                            # Get Cohen's d value from our precomputed matrix\n",
    "                            if idx_i < idx_j:\n",
    "                                cohens_d_df.iloc[i, j] = d_matrix[idx_i, idx_j]\n",
    "                            else:\n",
    "                                cohens_d_df.iloc[i, j] = d_matrix[idx_j, idx_i]\n",
    "                        except ValueError:\n",
    "                            log(f\"Warning: Could not find {type_i} or {type_j} in valid_types\")\n",
    "        \n",
    "        # Save the Cohen's d matrix (only significant differences in upper triangle)\n",
    "        cohens_d_df.to_csv(os.path.join(OUTPUT_DIR, f'cohens_d_matrix_{test_type}.csv'))\n",
    "        \n",
    "        # Create a heatmap visualization of Cohen's d values\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Use a diverging colormap to highlight differences\n",
    "        cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "        \n",
    "        # Create mask for lower triangle (opposite of Dunn's test)\n",
    "        mask = np.tril(np.ones_like(cohens_d_df, dtype=bool))\n",
    "        \n",
    "        # Create heatmap for Cohen's d values\n",
    "        sns.heatmap(cohens_d_df, annot=True, cmap=cmap, \n",
    "                  vmin=0, vmax=1.5, center=0.75,  # 0.75 is the threshold for \"large\" effect\n",
    "                  annot_kws={\"size\": 10}, fmt='.3f',\n",
    "                  mask=mask)  # Apply mask to hide lower triangle\n",
    "        \n",
    "        # Add colorbar labels for effect size interpretation\n",
    "        colorbar = plt.gcf().axes[-1]\n",
    "        colorbar.text(3.5, 0.1, 'Kleiner Effekt (<0.2)', ha='left', va='center')\n",
    "        colorbar.text(3.5, 0.4, 'Mittlerer Effekt (0.2-0.75)', ha='left', va='center')\n",
    "        colorbar.text(3.5, 0.9, 'Großer Effekt (>0.75)', ha='left', va='center')\n",
    "        \n",
    "        plt.title(f\"Vergleich nach OSM 'type': Effektstärken nach Cohen's d ({test_type}-basiert)\\nDifferenzen zwischen OSM 'type' (04/23 - 12/24)\", \n",
    "                 fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'cohens_d_heatmap_{test_type}.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return dunn_results, (d_matrix, mean_diffs)\n",
    "    else:\n",
    "        log(\"No significant differences found between street types (p >= 0.05)\")\n",
    "        return None, (d_matrix, mean_diffs)\n",
    "\n",
    "def analyze_street_types():\n",
    "    \"\"\"\n",
    "    Main analysis function that consistently uses length-weighted histograms for all calculations.\n",
    "    \"\"\"\n",
    "    log(f\"Starting consistent length-weighted analysis of parquet file: {PARQUET_FILE}\")\n",
    "    \n",
    "    # Verify the parquet file exists\n",
    "    if not os.path.exists(PARQUET_FILE):\n",
    "        log(f\"Error: File {PARQUET_FILE} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Get file info\n",
    "    parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "    num_row_groups = parquet_file.metadata.num_row_groups\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    log(f\"Parquet file has {num_row_groups} row groups and approximately {total_rows:,} rows\")\n",
    "    \n",
    "    # Define the specific columns we need\n",
    "    columns = [COLUMN_TYPE, COLUMN_HIST, COLUMN_LENGTH]\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    street_type_data = {}\n",
    "    \n",
    "    # Process each row group with efficient chunking\n",
    "    log(f\"Processing {num_row_groups} row groups to extract speed data by street type\")\n",
    "    \n",
    "    # Define chunk size for efficient processing within each row group\n",
    "    CHUNK_SIZE = 50000\n",
    "    \n",
    "    for rg in tqdm(range(num_row_groups), desc=\"Processing row groups\"):\n",
    "        try:\n",
    "            # Get the row group size to determine number of chunks needed\n",
    "            row_group_metadata = parquet_file.metadata.row_group(rg)\n",
    "            row_group_size = row_group_metadata.num_rows\n",
    "            num_chunks = (row_group_size + CHUNK_SIZE - 1) // CHUNK_SIZE  # Ceiling division\n",
    "            \n",
    "            # Read the entire row group\n",
    "            table = parquet_file.read_row_group(rg, columns=columns)\n",
    "            \n",
    "            # Process each chunk of the row group\n",
    "            for chunk_idx in tqdm(range(num_chunks), desc=f\"Chunks in row group {rg}\", leave=False):\n",
    "                # Calculate offsets\n",
    "                offset = chunk_idx * CHUNK_SIZE\n",
    "                length = min(CHUNK_SIZE, row_group_size - offset)\n",
    "                \n",
    "                # Extract just this chunk from the row group\n",
    "                chunk_table = table.slice(offset, length)\n",
    "                df_chunk = chunk_table.to_pandas()\n",
    "                \n",
    "                # Filter out NaN values\n",
    "                df_chunk = df_chunk.dropna(subset=[COLUMN_TYPE, COLUMN_HIST, COLUMN_LENGTH])\n",
    "                \n",
    "                # Filter to only include the specified street types\n",
    "                df_chunk = df_chunk[df_chunk[COLUMN_TYPE].isin(STREET_TYPES_TO_ANALYZE)]\n",
    "                \n",
    "                # Process each row in this chunk\n",
    "                for _, row in df_chunk.iterrows():\n",
    "                    street_type = row[COLUMN_TYPE]\n",
    "                    hist = parse_histogram(row[COLUMN_HIST])\n",
    "                    segment_length = row[COLUMN_LENGTH]\n",
    "                    \n",
    "                    if len(hist) > 0 and np.sum(hist) > 0:\n",
    "                        # Initialize entry for this street type if it doesn't exist\n",
    "                        if street_type not in street_type_data:\n",
    "                            street_type_data[street_type] = {\n",
    "                                'lengths': [],  # Store segment lengths\n",
    "                                'length_weighted_agg_hist': np.zeros(32),  # Initialize aggregated histogram\n",
    "                                'count': 0\n",
    "                            }\n",
    "                        \n",
    "                        # Add data for this street segment\n",
    "                        street_type_data[street_type]['lengths'].append(segment_length)\n",
    "                        \n",
    "                        # Add to the aggregated histogram with length weighting\n",
    "                        # Multiply histogram by segment length for length-weighting\n",
    "                        street_type_data[street_type]['length_weighted_agg_hist'] += hist * segment_length\n",
    "                        \n",
    "                        street_type_data[street_type]['count'] += 1\n",
    "                \n",
    "                # Clean up to free memory after each chunk\n",
    "                del df_chunk, chunk_table\n",
    "                force_gc()\n",
    "            \n",
    "            # Clean up the entire row group data\n",
    "            del table\n",
    "            force_gc()\n",
    "                \n",
    "        except Exception as e:\n",
    "            log(f\"Error processing row group {rg}: {e}\")\n",
    "    \n",
    "    # Count total processed segments\n",
    "    total_segments = sum(data['count'] for data in street_type_data.values())\n",
    "    log(f\"Processed {total_segments:,} total street segments across {len(street_type_data)} different street types\")\n",
    "    \n",
    "    # Check if we have data for all the requested street types\n",
    "    for street_type in STREET_TYPES_TO_ANALYZE:\n",
    "        if street_type not in street_type_data:\n",
    "            log(f\"Warning: No data found for street type '{street_type}'\")\n",
    "        elif street_type_data[street_type]['count'] < 30:\n",
    "            log(f\"Warning: Insufficient data for street type '{street_type}' (only {street_type_data[street_type]['count']} segments)\")\n",
    "    \n",
    "    # Get the street types that have sufficient data (at least 30 segments)\n",
    "    valid_types = [st for st in STREET_TYPES_TO_ANALYZE \n",
    "                  if st in street_type_data and street_type_data[st]['count'] >= 30]\n",
    "    \n",
    "    log(f\"Analysis will include {len(valid_types)} street types with sufficient data\")\n",
    "    for i, type_name in enumerate(valid_types, 1):\n",
    "        count = street_type_data[type_name]['count']\n",
    "        log(f\"{i}. {type_name}: {count:,} segments\")\n",
    "    \n",
    "    # Calculate statistics for each street type using only the length-weighted histogram\n",
    "    # We'll do this ONCE for each street type and store the results for reuse\n",
    "    log(\"Calculating length-weighted statistics for each street type\")\n",
    "    for street_type in valid_types:\n",
    "        # Calculate total length in km\n",
    "        total_length_km = sum(street_type_data[street_type]['lengths']) / 1000\n",
    "        \n",
    "        # Calculate effective sample size\n",
    "        effective_n = calculate_effective_n(street_type_data[street_type]['lengths'])\n",
    "        \n",
    "        # Calculate all statistics efficiently in a single pass\n",
    "        agg_hist = street_type_data[street_type]['length_weighted_agg_hist']\n",
    "        stats = calculate_all_weighted_statistics(agg_hist)\n",
    "        \n",
    "        # Store all calculated statistics\n",
    "        street_type_data[street_type]['stats'] = stats\n",
    "        street_type_data[street_type]['effective_n'] = effective_n\n",
    "        street_type_data[street_type]['total_length_km'] = total_length_km\n",
    "        \n",
    "        log(f\"{street_type}: weighted mean = {stats['mean']:.2f} km/h, weighted median = {stats['median']:.2f} km/h, effective n = {effective_n:.1f}\")\n",
    "    \n",
    "    # Sort valid types by weighted mean for visualizations\n",
    "    valid_types_sorted_by_mean = sorted(valid_types, \n",
    "                                        key=lambda x: street_type_data[x]['stats']['mean'] if x in street_type_data else 0,\n",
    "                                        reverse=True)\n",
    "    \n",
    "    # Sort valid types by weighted median for comparison\n",
    "    valid_types_sorted_by_median = sorted(valid_types, \n",
    "                                         key=lambda x: street_type_data[x]['stats']['median'] if x in street_type_data else 0,\n",
    "                                         reverse=True)\n",
    "    \n",
    "    # Create visualizations\n",
    "    log(\"Creating visualizations based on length-weighted histograms\")\n",
    "    \n",
    "    # 1. Create length-weighted histogram comparison (sorted by mean)\n",
    "    histograms = []\n",
    "    hist_labels = []\n",
    "    hist_stats = []\n",
    "    \n",
    "    for street_type in valid_types_sorted_by_mean:\n",
    "        histograms.append(street_type_data[street_type]['length_weighted_agg_hist'])\n",
    "        hist_labels.append(street_type)\n",
    "        hist_stats.append({\n",
    "            'stats': street_type_data[street_type]['stats'],\n",
    "            'total_length_km': street_type_data[street_type]['total_length_km'],\n",
    "            'effective_n': street_type_data[street_type]['effective_n']\n",
    "        })\n",
    "    \n",
    "    create_length_weighted_histogram_comparison(histograms, hist_labels, hist_stats)\n",
    "    \n",
    "    # 2. Create median-sorted histogram comparison\n",
    "    histograms_by_median = []\n",
    "    hist_labels_by_median = []\n",
    "    hist_stats_by_median = []\n",
    "    \n",
    "    for street_type in valid_types_sorted_by_median:\n",
    "        histograms_by_median.append(street_type_data[street_type]['length_weighted_agg_hist'])\n",
    "        hist_labels_by_median.append(street_type)\n",
    "        hist_stats_by_median.append({\n",
    "            'stats': street_type_data[street_type]['stats'],\n",
    "            'total_length_km': street_type_data[street_type]['total_length_km'],\n",
    "            'effective_n': street_type_data[street_type]['effective_n']\n",
    "        })\n",
    "    \n",
    "    create_median_weighted_histogram_comparison(histograms_by_median, hist_labels_by_median, hist_stats_by_median)\n",
    "    \n",
    "    # 3. Create boxplot visualization\n",
    "    create_boxplot_from_histograms(street_type_data, valid_types_sorted_by_mean)\n",
    "    \n",
    "    # 4. Create violin plot\n",
    "    create_violin_plot_from_histograms(street_type_data, valid_types_sorted_by_mean)\n",
    "    \n",
    "    # 5. Create mean vs median comparison\n",
    "    create_mean_vs_median_comparison(street_type_data, valid_types_sorted_by_mean)\n",
    "    \n",
    "    # 6. Perform Kruskal-Wallis and Dunn's tests based on mean-sorted order\n",
    "    dunn_results_mean, effect_sizes_mean = perform_kruskal_wallis_and_dunn_tests(\n",
    "        street_type_data, valid_types_sorted_by_mean, \"mean\")\n",
    "    \n",
    "    # 7. Perform Kruskal-Wallis and Dunn's tests based on median-sorted order\n",
    "    dunn_results_median, effect_sizes_median = perform_kruskal_wallis_and_dunn_tests(\n",
    "        street_type_data, valid_types_sorted_by_median, \"median\")\n",
    "    \n",
    "    # 8. Compare the two approaches if both tests were significant\n",
    "    if dunn_results_mean is not None and dunn_results_median is not None:\n",
    "        log(\"\\nComparing mean-based and median-based significant pair findings:\")\n",
    "        \n",
    "        # Get sets of significant pairs from both tests\n",
    "        mean_pairs = set()\n",
    "        median_pairs = set()\n",
    "        \n",
    "        # Extract significant pairs from Dunn's test results\n",
    "        for i in range(len(valid_types_sorted_by_mean)):\n",
    "            for j in range(i+1, len(valid_types_sorted_by_mean)):\n",
    "                if dunn_results_mean.iloc[i, j] < 0.05:\n",
    "                    mean_pairs.add((valid_types_sorted_by_mean[i], valid_types_sorted_by_mean[j]))\n",
    "        \n",
    "        for i in range(len(valid_types_sorted_by_median)):\n",
    "            for j in range(i+1, len(valid_types_sorted_by_median)):\n",
    "                if dunn_results_median.iloc[i, j] < 0.05:\n",
    "                    median_pairs.add((valid_types_sorted_by_median[i], valid_types_sorted_by_median[j]))\n",
    "        \n",
    "        # Find pairs that were significant in both tests\n",
    "        common_pairs = mean_pairs.intersection(median_pairs)\n",
    "        only_mean_pairs = mean_pairs - median_pairs\n",
    "        only_median_pairs = median_pairs - mean_pairs\n",
    "        \n",
    "        log(f\"Number of pairs significant in both tests: {len(common_pairs)}\")\n",
    "        log(f\"Number of pairs significant only in mean-based test: {len(only_mean_pairs)}\")\n",
    "        log(f\"Number of pairs significant only in median-based test: {len(only_median_pairs)}\")\n",
    "        \n",
    "        # List the common pairs\n",
    "        if common_pairs:\n",
    "            log(\"\\nPairs significant in both tests:\")\n",
    "            for pair in common_pairs:\n",
    "                log(f\"{pair[0]} vs {pair[1]}\")\n",
    "                \n",
    "        # List pairs only significant in mean test\n",
    "        if only_mean_pairs:\n",
    "            log(\"\\nPairs significant only in mean-based test:\")\n",
    "            for pair in only_mean_pairs:\n",
    "                log(f\"{pair[0]} vs {pair[1]}\")\n",
    "        \n",
    "        # List pairs only significant in median test\n",
    "        if only_median_pairs:\n",
    "            log(\"\\nPairs significant only in median-based test:\")\n",
    "            for pair in only_median_pairs:\n",
    "                log(f\"{pair[0]} vs {pair[1]}\")\n",
    "        \n",
    "        # Create Venn diagram of significant pairs if matplotlib_venn is available\n",
    "        try:\n",
    "            from matplotlib_venn import venn2\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            venn2(subsets=(len(only_mean_pairs), len(only_median_pairs), len(common_pairs)),\n",
    "                set_labels=('Mean-based', 'Median-based'))\n",
    "            plt.title(\"Significant Differences Found: Mean vs Median Approach\", fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, 'mean_vs_median_venn.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except ImportError:\n",
    "            log(\"matplotlib_venn not available for creating Venn diagram. Install with 'pip install matplotlib-venn'\")\n",
    "    \n",
    "    # 9. Create summary statistics table\n",
    "    summary_data = []\n",
    "    for street_type in valid_types_sorted_by_mean:\n",
    "        stats = street_type_data[street_type]['stats']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'type': street_type,\n",
    "            'count': street_type_data[street_type]['count'],\n",
    "            'total_length_km': street_type_data[street_type]['total_length_km'],\n",
    "            'effective_n': street_type_data[street_type]['effective_n'],\n",
    "            'mean': stats['mean'],\n",
    "            'median': stats['median'],\n",
    "            'std': stats['std'],\n",
    "            'min': stats['min'],\n",
    "            'max': stats['max'],\n",
    "            'percentile_5': stats['percentile_5'],\n",
    "            'percentile_25': stats['percentile_25'],\n",
    "            'percentile_75': stats['percentile_75'],\n",
    "            'percentile_95': stats['percentile_95'],\n",
    "            'skewness': stats['mean'] - stats['median']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(OUTPUT_DIR, 'street_type_summary_stats.csv'), index=False)\n",
    "    \n",
    "    log(\"Consistent length-weighted analysis complete. All results are based on the same length-weighted histograms.\")\n",
    "    return street_type_data\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    street_data = analyze_street_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec0e2cc-d310-481c-bdd1-c2267729a655",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting consistent length-weighted analysis of parquet file: data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\n",
      "Parquet file has 12 row groups and approximately 466,957 rows\n",
      "Processing 12 row groups to extract speed data by street type\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c6ae7e64c44daaa025e4e10d630d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing row groups:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2abe7d35cb4b008bd21539bfcab283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413189e3c56d4d8aa6e6126e4b811fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e586b32565c4d4d9003779169374431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4782a3aba574403582530f75ceb902e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e73204d524a488788294b95f3e0149c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dca1e54f334f4c85aa1b5cd8332f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8f17964e09483ca77122cdeced30ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 6:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f96fcb9c35a4ce2877a7553a61c2dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 7:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row group 8: Corrupt snappy compressed data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdca93f2d83b4ff480b3fe4a6a541902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 9:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d4297f49d84511bfe065da6c64fc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6dbafd886b40e18fa169bc7580af23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks in row group 11:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 428,595 total street segments across 15 different street types\n",
      "Analysis will include 15 street types with sufficient data\n",
      "1. footway: 192,850 segments\n",
      "2. residential: 81,186 segments\n",
      "3. secondary: 25,847 segments\n",
      "4. service_driveway: 27,412 segments\n",
      "5. tertiary: 17,130 segments\n",
      "6. service: 22,425 segments\n",
      "7. path: 13,612 segments\n",
      "8. cycleway: 12,586 segments\n",
      "9. track: 7,245 segments\n",
      "10. primary: 8,601 segments\n",
      "11. service_parking_aisle: 7,834 segments\n",
      "12. unclassified: 2,973 segments\n",
      "13. living_street: 4,291 segments\n",
      "14. steps: 3,594 segments\n",
      "15. pedestrian: 1,009 segments\n",
      "Calculating length-weighted statistics for each street type\n",
      "footway: weighted mean = 18.72 km/h, weighted median = 18.72 km/h, effective n = 72937.3\n",
      "residential: weighted mean = 18.97 km/h, weighted median = 18.93 km/h, effective n = 38380.4\n",
      "secondary: weighted mean = 20.68 km/h, weighted median = 20.57 km/h, effective n = 12104.6\n",
      "service_driveway: weighted mean = 12.57 km/h, weighted median = 12.11 km/h, effective n = 8460.0\n",
      "tertiary: weighted mean = 20.70 km/h, weighted median = 20.59 km/h, effective n = 7951.7\n",
      "service: weighted mean = 18.17 km/h, weighted median = 18.39 km/h, effective n = 10302.3\n",
      "path: weighted mean = 21.10 km/h, weighted median = 20.90 km/h, effective n = 8168.9\n",
      "cycleway: weighted mean = 20.13 km/h, weighted median = 20.06 km/h, effective n = 4502.5\n",
      "track: weighted mean = 18.90 km/h, weighted median = 18.72 km/h, effective n = 5341.7\n",
      "primary: weighted mean = 20.65 km/h, weighted median = 20.57 km/h, effective n = 4033.0\n",
      "service_parking_aisle: weighted mean = 16.20 km/h, weighted median = 16.61 km/h, effective n = 3915.8\n",
      "unclassified: weighted mean = 20.92 km/h, weighted median = 20.81 km/h, effective n = 1450.1\n",
      "living_street: weighted mean = 16.98 km/h, weighted median = 16.62 km/h, effective n = 2070.2\n",
      "steps: weighted mean = 11.51 km/h, weighted median = 9.72 km/h, effective n = 1832.4\n",
      "pedestrian: weighted mean = 15.92 km/h, weighted median = 15.64 km/h, effective n = 540.5\n",
      "Creating visualizations based on length-weighted histograms\n",
      "Creating length-weighted histogram comparison\n",
      "Saved length-weighted histogram comparison\n",
      "Creating median-weighted histogram comparison\n",
      "Saved median-weighted histogram comparison\n",
      "Creating boxplot from length-weighted histograms\n",
      "Creating violin plot from length-weighted histograms\n",
      "Creating mean vs median comparison plot\n",
      "\n",
      "Comparison of weighted means and medians:\n",
      "path: Mean = 21.10, Median = 20.90, Diff = 0.20, Effective N = 8168.9\n",
      "unclassified: Mean = 20.92, Median = 20.81, Diff = 0.10, Effective N = 1450.1\n",
      "tertiary: Mean = 20.70, Median = 20.59, Diff = 0.11, Effective N = 7951.7\n",
      "secondary: Mean = 20.68, Median = 20.57, Diff = 0.10, Effective N = 12104.6\n",
      "primary: Mean = 20.65, Median = 20.57, Diff = 0.08, Effective N = 4033.0\n",
      "cycleway: Mean = 20.13, Median = 20.06, Diff = 0.07, Effective N = 4502.5\n",
      "residential: Mean = 18.97, Median = 18.93, Diff = 0.05, Effective N = 38380.4\n",
      "track: Mean = 18.90, Median = 18.72, Diff = 0.18, Effective N = 5341.7\n",
      "footway: Mean = 18.72, Median = 18.72, Diff = 0.00, Effective N = 72937.3\n",
      "service: Mean = 18.17, Median = 18.39, Diff = -0.22, Effective N = 10302.3\n",
      "living_street: Mean = 16.98, Median = 16.62, Diff = 0.36, Effective N = 2070.2\n",
      "service_parking_aisle: Mean = 16.20, Median = 16.61, Diff = -0.41, Effective N = 3915.8\n",
      "pedestrian: Mean = 15.92, Median = 15.64, Diff = 0.28, Effective N = 540.5\n",
      "service_driveway: Mean = 12.57, Median = 12.11, Diff = 0.46, Effective N = 8460.0\n",
      "steps: Mean = 11.51, Median = 9.72, Diff = 1.79, Effective N = 1832.4\n",
      "\n",
      "Performing Kruskal-Wallis test followed by Dunn's post-hoc test (sorting by mean)\n",
      "Kruskal-Wallis test results:\n",
      "H statistic: 21674.6741\n",
      "p-value: 0.000000\n",
      "\n",
      "Significant differences found between street types (p < 0.05)\n",
      "Performing Dunn's test for post-hoc analysis\n",
      "\n",
      "Significantly different street type pairs with length-weighted effect sizes:\n",
      "path vs living_street: p=0.000000, weighted mean diff=4.12 km/h, Cohen's d=0.711 (medium)\n",
      "path vs service_parking_aisle: p=0.000000, weighted mean diff=4.91 km/h, Cohen's d=0.708 (medium)\n",
      "path vs pedestrian: p=0.000000, weighted mean diff=5.18 km/h, Cohen's d=0.836 (large)\n",
      "path vs service_driveway: p=0.000000, weighted mean diff=8.53 km/h, Cohen's d=1.224 (large)\n",
      "path vs steps: p=0.000000, weighted mean diff=9.60 km/h, Cohen's d=1.410 (large)\n",
      "unclassified vs living_street: p=0.000000, weighted mean diff=3.94 km/h, Cohen's d=0.672 (medium)\n",
      "unclassified vs service_parking_aisle: p=0.000000, weighted mean diff=4.72 km/h, Cohen's d=0.676 (medium)\n",
      "unclassified vs pedestrian: p=0.000000, weighted mean diff=5.00 km/h, Cohen's d=0.798 (large)\n",
      "unclassified vs service_driveway: p=0.000000, weighted mean diff=8.35 km/h, Cohen's d=1.188 (large)\n",
      "unclassified vs steps: p=0.000000, weighted mean diff=9.41 km/h, Cohen's d=1.372 (large)\n",
      "tertiary vs living_street: p=0.000000, weighted mean diff=3.72 km/h, Cohen's d=0.622 (medium)\n",
      "tertiary vs service_parking_aisle: p=0.000000, weighted mean diff=4.50 km/h, Cohen's d=0.635 (medium)\n",
      "tertiary vs pedestrian: p=0.000000, weighted mean diff=4.78 km/h, Cohen's d=0.749 (medium)\n",
      "tertiary vs service_driveway: p=0.000000, weighted mean diff=8.13 km/h, Cohen's d=1.140 (large)\n",
      "tertiary vs steps: p=0.000000, weighted mean diff=9.19 km/h, Cohen's d=1.319 (large)\n",
      "secondary vs living_street: p=0.000000, weighted mean diff=3.69 km/h, Cohen's d=0.614 (medium)\n",
      "secondary vs service_parking_aisle: p=0.000000, weighted mean diff=4.48 km/h, Cohen's d=0.630 (medium)\n",
      "secondary vs pedestrian: p=0.000000, weighted mean diff=4.75 km/h, Cohen's d=0.742 (medium)\n",
      "secondary vs service_driveway: p=0.000000, weighted mean diff=8.11 km/h, Cohen's d=1.133 (large)\n",
      "secondary vs steps: p=0.000000, weighted mean diff=9.17 km/h, Cohen's d=1.311 (large)\n",
      "primary vs living_street: p=0.000000, weighted mean diff=3.67 km/h, Cohen's d=0.605 (medium)\n",
      "primary vs service_parking_aisle: p=0.000000, weighted mean diff=4.45 km/h, Cohen's d=0.623 (medium)\n",
      "primary vs pedestrian: p=0.000000, weighted mean diff=4.73 km/h, Cohen's d=0.733 (medium)\n",
      "primary vs service_driveway: p=0.000000, weighted mean diff=8.08 km/h, Cohen's d=1.123 (large)\n",
      "primary vs steps: p=0.000000, weighted mean diff=9.14 km/h, Cohen's d=1.299 (large)\n",
      "cycleway vs pedestrian: p=0.000000, weighted mean diff=4.21 km/h, Cohen's d=0.670 (medium)\n",
      "cycleway vs service_driveway: p=0.000000, weighted mean diff=7.56 km/h, Cohen's d=1.073 (large)\n",
      "cycleway vs steps: p=0.000000, weighted mean diff=8.63 km/h, Cohen's d=1.253 (large)\n",
      "residential vs service_driveway: p=0.000000, weighted mean diff=6.40 km/h, Cohen's d=0.902 (large)\n",
      "residential vs steps: p=0.000000, weighted mean diff=7.47 km/h, Cohen's d=1.076 (large)\n",
      "track vs service_driveway: p=0.000000, weighted mean diff=6.33 km/h, Cohen's d=0.903 (large)\n",
      "track vs steps: p=0.000000, weighted mean diff=7.39 km/h, Cohen's d=1.080 (large)\n",
      "footway vs service_driveway: p=0.000000, weighted mean diff=6.15 km/h, Cohen's d=0.845 (large)\n",
      "footway vs steps: p=0.000000, weighted mean diff=7.22 km/h, Cohen's d=1.013 (large)\n",
      "service vs service_driveway: p=0.000000, weighted mean diff=5.60 km/h, Cohen's d=0.746 (medium)\n",
      "service vs steps: p=0.000000, weighted mean diff=6.67 km/h, Cohen's d=0.906 (large)\n",
      "living_street vs steps: p=0.000000, weighted mean diff=5.47 km/h, Cohen's d=0.783 (large)\n",
      "service_parking_aisle vs steps: p=0.000000, weighted mean diff=4.69 km/h, Cohen's d=0.590 (medium)\n",
      "cycleway vs living_street: p=0.000000, weighted mean diff=3.15 km/h, Cohen's d=0.535 (medium)\n",
      "cycleway vs service_parking_aisle: p=0.000000, weighted mean diff=3.94 km/h, Cohen's d=0.562 (medium)\n",
      "pedestrian vs steps: p=0.000000, weighted mean diff=4.41 km/h, Cohen's d=0.602 (medium)\n",
      "service_parking_aisle vs service_driveway: p=0.000000, weighted mean diff=3.63 km/h, Cohen's d=0.448 (medium)\n",
      "living_street vs service_driveway: p=0.000000, weighted mean diff=4.41 km/h, Cohen's d=0.616 (medium)\n",
      "residential vs pedestrian: p=0.000000, weighted mean diff=3.05 km/h, Cohen's d=0.481 (medium)\n",
      "footway vs pedestrian: p=0.000000, weighted mean diff=2.80 km/h, Cohen's d=0.428 (medium)\n",
      "path vs service: p=0.000000, weighted mean diff=2.93 km/h, Cohen's d=0.471 (medium)\n",
      "track vs pedestrian: p=0.000000, weighted mean diff=2.98 km/h, Cohen's d=0.477 (medium)\n",
      "unclassified vs service: p=0.000000, weighted mean diff=2.74 km/h, Cohen's d=0.437 (medium)\n",
      "tertiary vs service: p=0.000000, weighted mean diff=2.53 km/h, Cohen's d=0.395 (medium)\n",
      "primary vs service: p=0.000000, weighted mean diff=2.48 km/h, Cohen's d=0.383 (medium)\n",
      "path vs track: p=0.000000, weighted mean diff=2.20 km/h, Cohen's d=0.392 (medium)\n",
      "secondary vs service: p=0.000000, weighted mean diff=2.50 km/h, Cohen's d=0.389 (medium)\n",
      "service vs pedestrian: p=0.000000, weighted mean diff=2.25 km/h, Cohen's d=0.331 (medium)\n",
      "unclassified vs track: p=0.000000, weighted mean diff=2.02 km/h, Cohen's d=0.355 (medium)\n",
      "tertiary vs track: p=0.000000, weighted mean diff=1.80 km/h, Cohen's d=0.310 (medium)\n",
      "path vs footway: p=0.000000, weighted mean diff=2.38 km/h, Cohen's d=0.400 (medium)\n",
      "path vs residential: p=0.000000, weighted mean diff=2.13 km/h, Cohen's d=0.372 (medium)\n",
      "pedestrian vs service_driveway: p=0.000000, weighted mean diff=3.35 km/h, Cohen's d=0.448 (medium)\n",
      "unclassified vs footway: p=0.000000, weighted mean diff=2.19 km/h, Cohen's d=0.365 (medium)\n",
      "residential vs living_street: p=0.000000, weighted mean diff=1.99 km/h, Cohen's d=0.335 (medium)\n",
      "unclassified vs residential: p=0.000000, weighted mean diff=1.94 km/h, Cohen's d=0.336 (medium)\n",
      "residential vs service_parking_aisle: p=0.000000, weighted mean diff=2.77 km/h, Cohen's d=0.393 (medium)\n",
      "primary vs track: p=0.000000, weighted mean diff=1.75 km/h, Cohen's d=0.297 (medium)\n",
      "tertiary vs footway: p=0.000000, weighted mean diff=1.98 km/h, Cohen's d=0.323 (medium)\n",
      "footway vs living_street: p=0.000000, weighted mean diff=1.74 km/h, Cohen's d=0.283 (medium)\n",
      "footway vs service_parking_aisle: p=0.000000, weighted mean diff=2.52 km/h, Cohen's d=0.349 (medium)\n",
      "tertiary vs residential: p=0.000000, weighted mean diff=1.73 km/h, Cohen's d=0.292 (medium)\n",
      "secondary vs track: p=0.000000, weighted mean diff=1.77 km/h, Cohen's d=0.304 (medium)\n",
      "cycleway vs service: p=0.000000, weighted mean diff=1.96 km/h, Cohen's d=0.311 (medium)\n",
      "primary vs footway: p=0.000000, weighted mean diff=1.93 km/h, Cohen's d=0.311 (medium)\n",
      "track vs living_street: p=0.000000, weighted mean diff=1.92 km/h, Cohen's d=0.329 (medium)\n",
      "track vs service_parking_aisle: p=0.000000, weighted mean diff=2.70 km/h, Cohen's d=0.388 (medium)\n",
      "primary vs residential: p=0.000000, weighted mean diff=1.68 km/h, Cohen's d=0.280 (medium)\n",
      "secondary vs footway: p=0.000000, weighted mean diff=1.95 km/h, Cohen's d=0.317 (medium)\n",
      "secondary vs residential: p=0.000000, weighted mean diff=1.70 km/h, Cohen's d=0.286 (medium)\n",
      "cycleway vs track: p=0.000000, weighted mean diff=1.23 km/h, Cohen's d=0.216 (medium)\n",
      "service vs living_street: p=0.000000, weighted mean diff=1.19 km/h, Cohen's d=0.185 (small)\n",
      "service vs service_parking_aisle: p=0.000000, weighted mean diff=1.97 km/h, Cohen's d=0.264 (medium)\n",
      "cycleway vs footway: p=0.000000, weighted mean diff=1.41 km/h, Cohen's d=0.234 (medium)\n",
      "cycleway vs residential: p=0.000000, weighted mean diff=1.16 km/h, Cohen's d=0.200 (small)\n",
      "service_driveway vs steps: p=0.000000, weighted mean diff=1.06 km/h, Cohen's d=0.133 (small)\n",
      "service_parking_aisle vs pedestrian: p=0.000000, weighted mean diff=0.28 km/h, Cohen's d=0.037 (small)\n",
      "living_street vs pedestrian: p=0.000000, weighted mean diff=1.06 km/h, Cohen's d=0.165 (small)\n",
      "path vs cycleway: p=0.000000, weighted mean diff=0.97 km/h, Cohen's d=0.171 (small)\n",
      "unclassified vs cycleway: p=0.000000, weighted mean diff=0.78 km/h, Cohen's d=0.137 (small)\n",
      "tertiary vs cycleway: p=0.000000, weighted mean diff=0.57 km/h, Cohen's d=0.097 (small)\n",
      "residential vs service: p=0.000000, weighted mean diff=0.80 km/h, Cohen's d=0.125 (small)\n",
      "footway vs service: p=0.000000, weighted mean diff=0.55 km/h, Cohen's d=0.084 (small)\n",
      "primary vs cycleway: p=0.000000, weighted mean diff=0.52 km/h, Cohen's d=0.087 (small)\n",
      "secondary vs cycleway: p=0.000002, weighted mean diff=0.54 km/h, Cohen's d=0.092 (small)\n",
      "path vs secondary: p=0.000077, weighted mean diff=0.43 km/h, Cohen's d=0.074 (small)\n",
      "track vs service: p=0.000154, weighted mean diff=0.73 km/h, Cohen's d=0.116 (small)\n",
      "path vs primary: p=0.000939, weighted mean diff=0.45 km/h, Cohen's d=0.078 (small)\n",
      "unclassified vs secondary: p=0.002565, weighted mean diff=0.24 km/h, Cohen's d=0.041 (small)\n",
      "unclassified vs primary: p=0.017962, weighted mean diff=0.27 km/h, Cohen's d=0.045 (small)\n",
      "tertiary vs secondary: p=0.021271, weighted mean diff=0.02 km/h, Cohen's d=0.004 (small)\n",
      "residential vs track: p=0.028098, weighted mean diff=0.07 km/h, Cohen's d=0.012 (small)\n",
      "\n",
      "Performing Kruskal-Wallis test followed by Dunn's post-hoc test (sorting by median)\n",
      "Kruskal-Wallis test results:\n",
      "H statistic: 21542.6654\n",
      "p-value: 0.000000\n",
      "\n",
      "Significant differences found between street types (p < 0.05)\n",
      "Performing Dunn's test for post-hoc analysis\n",
      "\n",
      "Significantly different street type pairs with length-weighted effect sizes:\n",
      "path vs living_street: p=0.000000, weighted mean diff=4.12 km/h, Cohen's d=0.711 (medium)\n",
      "path vs service_parking_aisle: p=0.000000, weighted mean diff=4.91 km/h, Cohen's d=0.708 (medium)\n",
      "path vs pedestrian: p=0.000000, weighted mean diff=5.18 km/h, Cohen's d=0.836 (large)\n",
      "path vs service_driveway: p=0.000000, weighted mean diff=8.53 km/h, Cohen's d=1.224 (large)\n",
      "path vs steps: p=0.000000, weighted mean diff=9.60 km/h, Cohen's d=1.410 (large)\n",
      "unclassified vs living_street: p=0.000000, weighted mean diff=3.94 km/h, Cohen's d=0.672 (medium)\n",
      "unclassified vs service_parking_aisle: p=0.000000, weighted mean diff=4.72 km/h, Cohen's d=0.676 (medium)\n",
      "unclassified vs pedestrian: p=0.000000, weighted mean diff=5.00 km/h, Cohen's d=0.798 (large)\n",
      "unclassified vs service_driveway: p=0.000000, weighted mean diff=8.35 km/h, Cohen's d=1.188 (large)\n",
      "unclassified vs steps: p=0.000000, weighted mean diff=9.41 km/h, Cohen's d=1.372 (large)\n",
      "tertiary vs living_street: p=0.000000, weighted mean diff=3.72 km/h, Cohen's d=0.622 (medium)\n",
      "tertiary vs service_parking_aisle: p=0.000000, weighted mean diff=4.50 km/h, Cohen's d=0.635 (medium)\n",
      "tertiary vs pedestrian: p=0.000000, weighted mean diff=4.78 km/h, Cohen's d=0.749 (medium)\n",
      "tertiary vs service_driveway: p=0.000000, weighted mean diff=8.13 km/h, Cohen's d=1.140 (large)\n",
      "tertiary vs steps: p=0.000000, weighted mean diff=9.19 km/h, Cohen's d=1.319 (large)\n",
      "primary vs living_street: p=0.000000, weighted mean diff=3.67 km/h, Cohen's d=0.605 (medium)\n",
      "primary vs service_parking_aisle: p=0.000000, weighted mean diff=4.45 km/h, Cohen's d=0.623 (medium)\n",
      "primary vs pedestrian: p=0.000000, weighted mean diff=4.73 km/h, Cohen's d=0.733 (medium)\n",
      "primary vs service_driveway: p=0.000000, weighted mean diff=8.08 km/h, Cohen's d=1.123 (large)\n",
      "primary vs steps: p=0.000000, weighted mean diff=9.14 km/h, Cohen's d=1.299 (large)\n",
      "secondary vs living_street: p=0.000000, weighted mean diff=3.69 km/h, Cohen's d=0.614 (medium)\n",
      "secondary vs service_parking_aisle: p=0.000000, weighted mean diff=4.48 km/h, Cohen's d=0.630 (medium)\n",
      "secondary vs pedestrian: p=0.000000, weighted mean diff=4.75 km/h, Cohen's d=0.742 (medium)\n",
      "secondary vs service_driveway: p=0.000000, weighted mean diff=8.11 km/h, Cohen's d=1.133 (large)\n",
      "secondary vs steps: p=0.000000, weighted mean diff=9.17 km/h, Cohen's d=1.311 (large)\n",
      "cycleway vs pedestrian: p=0.000000, weighted mean diff=4.21 km/h, Cohen's d=0.670 (medium)\n",
      "cycleway vs service_driveway: p=0.000000, weighted mean diff=7.56 km/h, Cohen's d=1.073 (large)\n",
      "cycleway vs steps: p=0.000000, weighted mean diff=8.63 km/h, Cohen's d=1.253 (large)\n",
      "residential vs service_driveway: p=0.000000, weighted mean diff=6.40 km/h, Cohen's d=0.902 (large)\n",
      "residential vs steps: p=0.000000, weighted mean diff=7.47 km/h, Cohen's d=1.076 (large)\n",
      "track vs service_driveway: p=0.000000, weighted mean diff=6.33 km/h, Cohen's d=0.903 (large)\n",
      "track vs steps: p=0.000000, weighted mean diff=7.39 km/h, Cohen's d=1.080 (large)\n",
      "footway vs service_driveway: p=0.000000, weighted mean diff=6.15 km/h, Cohen's d=0.845 (large)\n",
      "footway vs steps: p=0.000000, weighted mean diff=7.22 km/h, Cohen's d=1.013 (large)\n",
      "service vs service_driveway: p=0.000000, weighted mean diff=5.60 km/h, Cohen's d=0.746 (medium)\n",
      "service vs steps: p=0.000000, weighted mean diff=6.67 km/h, Cohen's d=0.906 (large)\n",
      "living_street vs steps: p=0.000000, weighted mean diff=5.47 km/h, Cohen's d=0.783 (large)\n",
      "service_parking_aisle vs steps: p=0.000000, weighted mean diff=4.69 km/h, Cohen's d=0.590 (medium)\n",
      "cycleway vs service_parking_aisle: p=0.000000, weighted mean diff=3.94 km/h, Cohen's d=0.562 (medium)\n",
      "cycleway vs living_street: p=0.000000, weighted mean diff=3.15 km/h, Cohen's d=0.535 (medium)\n",
      "pedestrian vs steps: p=0.000000, weighted mean diff=4.41 km/h, Cohen's d=0.602 (medium)\n",
      "living_street vs service_driveway: p=0.000000, weighted mean diff=4.41 km/h, Cohen's d=0.616 (medium)\n",
      "service_parking_aisle vs service_driveway: p=0.000000, weighted mean diff=3.63 km/h, Cohen's d=0.448 (medium)\n",
      "path vs service: p=0.000000, weighted mean diff=2.93 km/h, Cohen's d=0.471 (medium)\n",
      "residential vs pedestrian: p=0.000000, weighted mean diff=3.05 km/h, Cohen's d=0.481 (medium)\n",
      "footway vs pedestrian: p=0.000000, weighted mean diff=2.80 km/h, Cohen's d=0.428 (medium)\n",
      "track vs pedestrian: p=0.000000, weighted mean diff=2.98 km/h, Cohen's d=0.477 (medium)\n",
      "unclassified vs service: p=0.000000, weighted mean diff=2.74 km/h, Cohen's d=0.437 (medium)\n",
      "path vs track: p=0.000000, weighted mean diff=2.20 km/h, Cohen's d=0.392 (medium)\n",
      "path vs footway: p=0.000000, weighted mean diff=2.38 km/h, Cohen's d=0.400 (medium)\n",
      "tertiary vs service: p=0.000000, weighted mean diff=2.53 km/h, Cohen's d=0.395 (medium)\n",
      "secondary vs service: p=0.000000, weighted mean diff=2.50 km/h, Cohen's d=0.389 (medium)\n",
      "primary vs service: p=0.000000, weighted mean diff=2.48 km/h, Cohen's d=0.383 (medium)\n",
      "path vs residential: p=0.000000, weighted mean diff=2.13 km/h, Cohen's d=0.372 (medium)\n",
      "service vs pedestrian: p=0.000000, weighted mean diff=2.25 km/h, Cohen's d=0.331 (medium)\n",
      "pedestrian vs service_driveway: p=0.000000, weighted mean diff=3.35 km/h, Cohen's d=0.448 (medium)\n",
      "unclassified vs track: p=0.000000, weighted mean diff=2.02 km/h, Cohen's d=0.355 (medium)\n",
      "unclassified vs footway: p=0.000000, weighted mean diff=2.19 km/h, Cohen's d=0.365 (medium)\n",
      "residential vs service_parking_aisle: p=0.000000, weighted mean diff=2.77 km/h, Cohen's d=0.393 (medium)\n",
      "tertiary vs track: p=0.000000, weighted mean diff=1.80 km/h, Cohen's d=0.310 (medium)\n",
      "secondary vs track: p=0.000000, weighted mean diff=1.77 km/h, Cohen's d=0.304 (medium)\n",
      "unclassified vs residential: p=0.000000, weighted mean diff=1.94 km/h, Cohen's d=0.336 (medium)\n",
      "tertiary vs footway: p=0.000000, weighted mean diff=1.98 km/h, Cohen's d=0.323 (medium)\n",
      "secondary vs footway: p=0.000000, weighted mean diff=1.95 km/h, Cohen's d=0.317 (medium)\n",
      "footway vs service_parking_aisle: p=0.000000, weighted mean diff=2.52 km/h, Cohen's d=0.349 (medium)\n",
      "residential vs living_street: p=0.000000, weighted mean diff=1.99 km/h, Cohen's d=0.335 (medium)\n",
      "primary vs track: p=0.000000, weighted mean diff=1.75 km/h, Cohen's d=0.297 (medium)\n",
      "track vs service_parking_aisle: p=0.000000, weighted mean diff=2.70 km/h, Cohen's d=0.388 (medium)\n",
      "primary vs footway: p=0.000000, weighted mean diff=1.93 km/h, Cohen's d=0.311 (medium)\n",
      "tertiary vs residential: p=0.000000, weighted mean diff=1.73 km/h, Cohen's d=0.292 (medium)\n",
      "secondary vs residential: p=0.000000, weighted mean diff=1.70 km/h, Cohen's d=0.286 (medium)\n",
      "footway vs living_street: p=0.000000, weighted mean diff=1.74 km/h, Cohen's d=0.283 (medium)\n",
      "cycleway vs service: p=0.000000, weighted mean diff=1.96 km/h, Cohen's d=0.311 (medium)\n",
      "track vs living_street: p=0.000000, weighted mean diff=1.92 km/h, Cohen's d=0.329 (medium)\n",
      "primary vs residential: p=0.000000, weighted mean diff=1.68 km/h, Cohen's d=0.280 (medium)\n",
      "service vs service_parking_aisle: p=0.000000, weighted mean diff=1.97 km/h, Cohen's d=0.264 (medium)\n",
      "cycleway vs track: p=0.000000, weighted mean diff=1.23 km/h, Cohen's d=0.216 (medium)\n",
      "service vs living_street: p=0.000000, weighted mean diff=1.19 km/h, Cohen's d=0.185 (small)\n",
      "cycleway vs footway: p=0.000000, weighted mean diff=1.41 km/h, Cohen's d=0.234 (medium)\n",
      "cycleway vs residential: p=0.000000, weighted mean diff=1.16 km/h, Cohen's d=0.200 (small)\n",
      "path vs cycleway: p=0.000000, weighted mean diff=0.97 km/h, Cohen's d=0.171 (small)\n",
      "service_driveway vs steps: p=0.000000, weighted mean diff=1.06 km/h, Cohen's d=0.133 (small)\n",
      "living_street vs pedestrian: p=0.000000, weighted mean diff=1.06 km/h, Cohen's d=0.165 (small)\n",
      "unclassified vs cycleway: p=0.000000, weighted mean diff=0.78 km/h, Cohen's d=0.137 (small)\n",
      "service_parking_aisle vs pedestrian: p=0.000000, weighted mean diff=0.28 km/h, Cohen's d=0.037 (small)\n",
      "tertiary vs cycleway: p=0.000000, weighted mean diff=0.57 km/h, Cohen's d=0.097 (small)\n",
      "secondary vs cycleway: p=0.000000, weighted mean diff=0.54 km/h, Cohen's d=0.092 (small)\n",
      "residential vs service: p=0.000000, weighted mean diff=0.80 km/h, Cohen's d=0.125 (small)\n",
      "path vs primary: p=0.000000, weighted mean diff=0.45 km/h, Cohen's d=0.078 (small)\n",
      "primary vs cycleway: p=0.000000, weighted mean diff=0.52 km/h, Cohen's d=0.087 (small)\n",
      "path vs secondary: p=0.000003, weighted mean diff=0.43 km/h, Cohen's d=0.074 (small)\n",
      "path vs tertiary: p=0.000006, weighted mean diff=0.40 km/h, Cohen's d=0.070 (small)\n",
      "footway vs service: p=0.000017, weighted mean diff=0.55 km/h, Cohen's d=0.084 (small)\n",
      "track vs service: p=0.000115, weighted mean diff=0.73 km/h, Cohen's d=0.116 (small)\n",
      "path vs unclassified: p=0.002752, weighted mean diff=0.19 km/h, Cohen's d=0.033 (small)\n",
      "unclassified vs primary: p=0.009400, weighted mean diff=0.27 km/h, Cohen's d=0.045 (small)\n",
      "\n",
      "Comparing mean-based and median-based significant pair findings:\n",
      "Number of pairs significant in both tests: 93\n",
      "Number of pairs significant only in mean-based test: 4\n",
      "Number of pairs significant only in median-based test: 3\n",
      "\n",
      "Pairs significant in both tests:\n",
      "service vs steps\n",
      "unclassified vs cycleway\n",
      "cycleway vs footway\n",
      "secondary vs service\n",
      "path vs service\n",
      "living_street vs service_driveway\n",
      "tertiary vs secondary\n",
      "unclassified vs service_parking_aisle\n",
      "residential vs living_street\n",
      "unclassified vs track\n",
      "path vs steps\n",
      "track vs service_driveway\n",
      "secondary vs service_parking_aisle\n",
      "residential vs footway\n",
      "tertiary vs residential\n",
      "tertiary vs service_driveway\n",
      "primary vs cycleway\n",
      "service_driveway vs steps\n",
      "path vs service_parking_aisle\n",
      "footway vs service_driveway\n",
      "tertiary vs primary\n",
      "unclassified vs secondary\n",
      "service_parking_aisle vs service_driveway\n",
      "cycleway vs service\n",
      "primary vs service_parking_aisle\n",
      "unclassified vs tertiary\n",
      "residential vs pedestrian\n",
      "primary vs track\n",
      "pedestrian vs service_driveway\n",
      "footway vs living_street\n",
      "unclassified vs primary\n",
      "cycleway vs service_parking_aisle\n",
      "living_street vs pedestrian\n",
      "residential vs service\n",
      "service vs service_parking_aisle\n",
      "secondary vs cycleway\n",
      "tertiary vs footway\n",
      "path vs cycleway\n",
      "residential vs steps\n",
      "unclassified vs living_street\n",
      "secondary vs track\n",
      "primary vs service_driveway\n",
      "tertiary vs pedestrian\n",
      "path vs track\n",
      "footway vs pedestrian\n",
      "service_parking_aisle vs pedestrian\n",
      "living_street vs steps\n",
      "track vs living_street\n",
      "unclassified vs footway\n",
      "cycleway vs service_driveway\n",
      "path vs secondary\n",
      "tertiary vs service\n",
      "footway vs service\n",
      "primary vs living_street\n",
      "service vs service_driveway\n",
      "living_street vs service_parking_aisle\n",
      "track vs footway\n",
      "unclassified vs pedestrian\n",
      "tertiary vs steps\n",
      "path vs tertiary\n",
      "footway vs steps\n",
      "service_parking_aisle vs steps\n",
      "secondary vs residential\n",
      "secondary vs service_driveway\n",
      "primary vs footway\n",
      "path vs residential\n",
      "path vs service_driveway\n",
      "path vs primary\n",
      "unclassified vs service\n",
      "tertiary vs service_parking_aisle\n",
      "footway vs service_parking_aisle\n",
      "primary vs residential\n",
      "unclassified vs steps\n",
      "primary vs pedestrian\n",
      "residential vs service_parking_aisle\n",
      "track vs service\n",
      "secondary vs living_street\n",
      "residential vs track\n",
      "path vs living_street\n",
      "path vs unclassified\n",
      "cycleway vs residential\n",
      "primary vs service\n",
      "secondary vs footway\n",
      "service vs pedestrian\n",
      "path vs footway\n",
      "primary vs steps\n",
      "track vs service_parking_aisle\n",
      "tertiary vs cycleway\n",
      "cycleway vs living_street\n",
      "path vs pedestrian\n",
      "service vs living_street\n",
      "tertiary vs track\n",
      "cycleway vs steps\n",
      "\n",
      "Pairs significant only in mean-based test:\n",
      "secondary vs primary\n",
      "track vs steps\n",
      "track vs pedestrian\n",
      "residential vs service_driveway\n",
      "\n",
      "Pairs significant only in median-based test:\n",
      "primary vs secondary\n",
      "secondary vs steps\n",
      "secondary vs pedestrian\n",
      "Consistent length-weighted analysis complete. All results are based on the same length-weighted histograms.\n"
     ]
    }
   ],
   "source": [
    "# copy_clipped Vergleich zwischen den OSM \"type\" Kategorien: Signifikanztest mit Kruskal-Wallis Test, Dunn's post-hoc Test, Cohen's d Effektmatrix (Segmente gewichtet nach Länge)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "from scikit_posthocs import posthoc_dunn\n",
    "\n",
    "# Configuration - modify to match your setup\n",
    "PARQUET_FILE = \"data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\"\n",
    "OUTPUT_DIR = \"analysis_results/005_Infra_OSM_type_CLIP\"\n",
    "COLUMN_TYPE = 'type'  # Column for street type\n",
    "COLUMN_HIST = '2304-2412_speeds'  # Column for speed histogram data\n",
    "COLUMN_LENGTH = 'length_m'  # Column for length in meters\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# Plot configurations\n",
    "PLOT_CONFIG = {\n",
    "    # General\n",
    "    'TEXT_FONT_SIZE': 10,\n",
    "    'TITLE_FONT_SIZE': 16,\n",
    "    'AXIS_LABEL_FONT_SIZE': 12,\n",
    "    'LEGEND_FONT_SIZE': 9,\n",
    "    'DPI': 300,\n",
    "    \n",
    "    # Colors\n",
    "    'BOX_COLOR_1': '#a0ddff',\n",
    "    'BOX_COLOR_2': '#c8e9a0',\n",
    "    'MEDIAN_COLOR': 'darkblue',\n",
    "    'MEAN_COLOR': 'red',\n",
    "    \n",
    "    # Labels\n",
    "    'HISTOGRAM_TITLE': 'Geschwindigkeitshistogramm nach OSM \"type\", 04/23 - 12/24',\n",
    "    'BOXPLOT_TITLE': 'Boxplot Geschwindigkeitsverteilung nach OSM \"type\", 04/23 - 12/24',\n",
    "    'VIOLINPLOT_TITLE': 'Geigenplot Geschwindigkeitsverteilung nach OSM \"type\", 04/23 - 12/24',\n",
    "    'X_LABEL': 'Geschwindigkeit (km/h)',\n",
    "    'Y_LABEL': 'Prozentsatz (%)',\n",
    "    \n",
    "    # Box annotations\n",
    "    'ANNOTATION_BBOX_STYLE': dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "}\n",
    "\n",
    "# Specific street types to analyze\n",
    "STREET_TYPES_TO_ANALYZE = [\n",
    "    'footway',\n",
    "    'residential',\n",
    "    'secondary',\n",
    "    'service_driveway',\n",
    "    'tertiary',\n",
    "    'service',\n",
    "    'path',\n",
    "    'cycleway',\n",
    "    'track',\n",
    "    'primary',\n",
    "    'service_parking_aisle',\n",
    "    'unclassified',\n",
    "    'living_street',\n",
    "    'steps',\n",
    "    'pedestrian'\n",
    "]\n",
    "def log(message):\n",
    "    \"\"\"Print a log message\"\"\"\n",
    "    print(message)\n",
    "\n",
    "def force_gc():\n",
    "    \"\"\"Force garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "def parse_histogram(hist_str):\n",
    "    \"\"\"Parse histogram string to numpy array\"\"\"\n",
    "    try:\n",
    "        if isinstance(hist_str, str):\n",
    "            hist_str = hist_str.strip('[]')\n",
    "            # Fast NumPy parsing\n",
    "            try:\n",
    "                values = np.fromstring(hist_str, sep=',')\n",
    "                return values\n",
    "            except:\n",
    "                # Fallback to manual parsing if NumPy method fails\n",
    "                values = [float(x) for x in hist_str.split(',')]\n",
    "                return np.array(values)\n",
    "        else:\n",
    "            return np.zeros(32)  # Return zeros for missing histograms\n",
    "    except Exception as e:\n",
    "        log(f\"Error parsing histogram: {e}\")\n",
    "        return np.zeros(32)\n",
    "\n",
    "def calculate_all_weighted_statistics(histogram):\n",
    "    \"\"\"\n",
    "    Calculate all statistics from a histogram in a single efficient pass.\n",
    "    Returns a dictionary with all statistics.\n",
    "    \"\"\"\n",
    "    if np.sum(histogram) == 0:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'median': np.nan,\n",
    "            'std': np.nan,\n",
    "            'percentile_5': np.nan,\n",
    "            'percentile_25': np.nan,\n",
    "            'percentile_75': np.nan,\n",
    "            'percentile_95': np.nan,\n",
    "            'min': np.nan,\n",
    "            'max': np.nan\n",
    "        }\n",
    "    \n",
    "    speed_bins = np.arange(32)  # 0-31 km/h\n",
    "    total_count = np.sum(histogram)\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    norm_hist = histogram / total_count\n",
    "    \n",
    "    # Calculate weighted mean in one step\n",
    "    mean = np.sum(speed_bins * norm_hist)\n",
    "    \n",
    "    # Calculate variance and std in one step\n",
    "    variance = np.sum(((speed_bins - mean) ** 2) * norm_hist)\n",
    "    std = np.sqrt(variance)\n",
    "    \n",
    "    # Calculate cumulative distribution once\n",
    "    cum_dist = np.cumsum(norm_hist)\n",
    "    \n",
    "    # Find min/max with data present\n",
    "    min_idx = np.nonzero(histogram)[0][0]\n",
    "    max_idx = np.nonzero(histogram)[0][-1]\n",
    "    \n",
    "    # Calculate percentiles efficiently\n",
    "    percentile_5 = np.interp(0.05, cum_dist, speed_bins)\n",
    "    percentile_25 = np.interp(0.25, cum_dist, speed_bins)\n",
    "    median = np.interp(0.5, cum_dist, speed_bins)\n",
    "    percentile_75 = np.interp(0.75, cum_dist, speed_bins)\n",
    "    percentile_95 = np.interp(0.95, cum_dist, speed_bins)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'std': std,\n",
    "        'percentile_5': percentile_5,\n",
    "        'percentile_25': percentile_25,\n",
    "        'percentile_75': percentile_75,\n",
    "        'percentile_95': percentile_95,\n",
    "        'min': speed_bins[min_idx],\n",
    "        'max': speed_bins[max_idx]\n",
    "    }\n",
    "\n",
    "def calculate_cohens_d_from_histograms(hist1, hist2):\n",
    "    \"\"\"Calculate Cohen's d effect size between two histograms\"\"\"\n",
    "    speed_bins = np.arange(32)  # 0-31 km/h\n",
    "    \n",
    "    # Normalize histograms\n",
    "    norm_hist1 = hist1 / np.sum(hist1) if np.sum(hist1) > 0 else np.zeros_like(hist1)\n",
    "    norm_hist2 = hist2 / np.sum(hist2) if np.sum(hist2) > 0 else np.zeros_like(hist2)\n",
    "    \n",
    "    # Calculate means\n",
    "    mean1 = np.sum(speed_bins * norm_hist1)\n",
    "    mean2 = np.sum(speed_bins * norm_hist2)\n",
    "    \n",
    "    # Calculate variances\n",
    "    var1 = np.sum(((speed_bins - mean1) ** 2) * norm_hist1)\n",
    "    var2 = np.sum(((speed_bins - mean2) ** 2) * norm_hist2)\n",
    "    \n",
    "    # Calculate pooled standard deviation\n",
    "    pooled_std = np.sqrt((var1 + var2) / 2)\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = abs(mean1 - mean2) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return d, mean1, mean2\n",
    "\n",
    "def calculate_effective_n(weights):\n",
    "    \"\"\"Calculate effective sample size using relative weight approach\"\"\"\n",
    "    if len(weights) == 0 or np.sum(weights) == 0:\n",
    "        return 0\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    effective_n = np.sum(weights)**2 / np.sum(weights**2)\n",
    "    return effective_n\n",
    "def create_length_weighted_histogram_comparison(histograms, street_types, stats_info):\n",
    "    \"\"\"Create a length-weighted histogram comparison of multiple street types\"\"\"\n",
    "    log(\"Creating length-weighted histogram comparison\")\n",
    "    \n",
    "    # Create speed bins\n",
    "    speed_bins = np.arange(32)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot histograms\n",
    "    for i, (hist, street_type) in enumerate(zip(histograms, street_types)):\n",
    "        # Normalize histogram\n",
    "        hist_norm = hist / np.sum(hist) if np.sum(hist) > 0 else np.zeros_like(hist)\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.bar(speed_bins + i*0.1, hist_norm * 100, alpha=0.7, \n",
    "                label=f\"{street_type} ({stats_info[i]['total_length_km']:.1f} km, n={stats_info[i]['effective_n']:.1f})\", \n",
    "                width=0.8/len(histograms))\n",
    "        \n",
    "        # Add vertical line for mean\n",
    "        plt.axvline(x=stats_info[i]['stats']['mean'], color=f\"C{i}\", linestyle='-', \n",
    "                    label=f\"{street_type} Mean: {stats_info[i]['stats']['mean']:.2f} km/h\")\n",
    "    \n",
    "    # Add legends\n",
    "    plt.legend(loc='upper left', fontsize=PLOT_CONFIG['LEGEND_FONT_SIZE'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.ylabel(PLOT_CONFIG['Y_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.title(PLOT_CONFIG['HISTOGRAM_TITLE'], fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(range(0, 32, 2))\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"length_weighted_histograms.png\"), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "    log(\"Saved length-weighted histogram comparison\")\n",
    "    plt.close()\n",
    "\n",
    "def create_median_weighted_histogram_comparison(histograms, street_types, stats_info):\n",
    "    \"\"\"Create a median-based histogram comparison of multiple street types\"\"\"\n",
    "    log(\"Creating median-weighted histogram comparison\")\n",
    "    \n",
    "    # Create speed bins\n",
    "    speed_bins = np.arange(32)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot histograms\n",
    "    for i, (hist, street_type) in enumerate(zip(histograms, street_types)):\n",
    "        # Normalize histogram\n",
    "        hist_norm = hist / np.sum(hist) if np.sum(hist) > 0 else np.zeros_like(hist)\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.bar(speed_bins + i*0.1, hist_norm * 100, alpha=0.7, \n",
    "                label=f\"{street_type} ({stats_info[i]['total_length_km']:.1f} km, n={stats_info[i]['effective_n']:.1f})\", \n",
    "                width=0.8/len(histograms))\n",
    "        \n",
    "        # Add vertical line for median\n",
    "        plt.axvline(x=stats_info[i]['stats']['median'], color=f\"C{i}\", linestyle='--', \n",
    "                    label=f\"{street_type} Median: {stats_info[i]['stats']['median']:.2f} km/h\")\n",
    "    \n",
    "    # Add legends\n",
    "    plt.legend(loc='upper left', fontsize=PLOT_CONFIG['LEGEND_FONT_SIZE'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.ylabel(PLOT_CONFIG['Y_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.title(\"Geschwindigkeitshistogramm nach OSM \\\"type\\\" (sortiert nach Median), 04/23 - 12/24\", \n",
    "              fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(range(0, 32, 2))\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"median_weighted_histograms.png\"), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "    log(\"Saved median-weighted histogram comparison\")\n",
    "    plt.close()\n",
    "def create_boxplot_from_histograms(street_type_data, valid_types_sorted):\n",
    "    \"\"\"Create boxplot using statistics derived from length-weighted histograms\"\"\"\n",
    "    log(\"Creating boxplot from length-weighted histograms\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    # Prepare statistics\n",
    "    all_stats = []\n",
    "    for street_type in valid_types_sorted:\n",
    "        # Get statistics from histogram\n",
    "        stats = street_type_data[street_type]['stats']\n",
    "        \n",
    "        boxplot_stats = {\n",
    "            'mean': stats['mean'],\n",
    "            'median': stats['median'],\n",
    "            'q1': stats['percentile_25'],\n",
    "            'q3': stats['percentile_75'],\n",
    "            'whislo': stats['percentile_5'],\n",
    "            'whishi': stats['percentile_95'],\n",
    "            'label': (f\"{street_type}\\n\"\n",
    "                     f\"(effective_n={street_type_data[street_type]['effective_n']:.1f}\\n\"\n",
    "                     f\"{street_type_data[street_type]['count']:,} Segmente\\n\"\n",
    "                     f\"{street_type_data[street_type]['total_length_km']:.1f} km\")\n",
    "        }\n",
    "        all_stats.append(boxplot_stats)\n",
    "    \n",
    "    # Create boxplot using Axes.bxp()\n",
    "    ax.bxp(\n",
    "        [{\n",
    "            'med': stats['median'],\n",
    "            'q1': stats['q1'],\n",
    "            'q3': stats['q3'],\n",
    "            'whislo': stats['whislo'],\n",
    "            'whishi': stats['whishi'],\n",
    "            'fliers': [],\n",
    "            'label': stats['label']\n",
    "        } for stats in all_stats],\n",
    "        positions=range(len(all_stats)),\n",
    "        showfliers=False,\n",
    "        patch_artist=True,\n",
    "        boxprops={'facecolor': PLOT_CONFIG['BOX_COLOR_1'], 'alpha': 0.7},\n",
    "        medianprops={'color': PLOT_CONFIG['MEDIAN_COLOR'], 'linewidth': 2},\n",
    "        whiskerprops={'color': 'black', 'linestyle': '-', 'linewidth': 1},\n",
    "        capprops={'color': 'black', 'linewidth': 1}\n",
    "    )\n",
    "    \n",
    "    # Add mean markers and annotations\n",
    "    for i, stats in enumerate(all_stats):\n",
    "        # Mean star\n",
    "        ax.scatter(\n",
    "            i, stats['mean'],\n",
    "            marker='*',\n",
    "            s=150,\n",
    "            color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "            zorder=3\n",
    "        )\n",
    "        \n",
    "        # Blue median text\n",
    "        ax.text(\n",
    "            i, stats['median'] + 0.7,\n",
    "            f\"{stats['median']:.1f}\",\n",
    "            ha='center', va='bottom',\n",
    "            color=PLOT_CONFIG['MEDIAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "        \n",
    "        # Red mean text\n",
    "        ax.text(\n",
    "            i, stats['mean'] - 0.7,\n",
    "            f\"{stats['mean']:.1f}\",\n",
    "            ha='center', va='top',\n",
    "            color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xticks(range(len(all_stats)))\n",
    "    ax.set_xticklabels([stats['label'] for stats in all_stats], rotation=45, ha='right')\n",
    "    ax.set_ylabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    ax.set_title(\n",
    "        \"Boxplot Geschwindigkeitsverteilungen DB Rad+ nach OSM 'type'\\n\"\n",
    "        \"Zeitraum 04/23 - 12/24\",\n",
    "        fontsize=PLOT_CONFIG['TITLE_FONT_SIZE']\n",
    "    )\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=PLOT_CONFIG['BOX_COLOR_1'], alpha=0.7, label='IQR'),\n",
    "        plt.Line2D([0], [0], color=PLOT_CONFIG['MEDIAN_COLOR'], lw=2, label='Median'),\n",
    "        plt.Line2D([0], [0], marker='*', color=PLOT_CONFIG['MEAN_COLOR'], markersize=10, \n",
    "                   linestyle='None', label='Mittelwert')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, 'street_type_boxplot.png'),\n",
    "        dpi=PLOT_CONFIG['DPI'],\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "def create_violin_plot_from_histograms(street_type_data, valid_types_sorted):\n",
    "    \"\"\"Create violin plot using distributions from length-weighted histograms\"\"\"\n",
    "    log(\"Creating violin plot from length-weighted histograms\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    # Draw violins\n",
    "    for i, street_type in enumerate(valid_types_sorted):\n",
    "        hist = street_type_data[street_type]['length_weighted_agg_hist']\n",
    "        speeds = np.arange(32)\n",
    "        \n",
    "        # Create weighted KDE\n",
    "        kde = gaussian_kde(\n",
    "            speeds, \n",
    "            weights=hist,\n",
    "            bw_method=0.25  # Optimal smoothing for speed data\n",
    "        )\n",
    "        x = np.linspace(0, 31, 100)\n",
    "        density = kde(x)\n",
    "        \n",
    "        # Normalize density width\n",
    "        density_norm = 0.4 * density / density.max()  \n",
    "        \n",
    "        # Draw violin body\n",
    "        ax.fill_betweenx(\n",
    "            x, \n",
    "            i - density_norm, \n",
    "            i + density_norm,\n",
    "            color=PLOT_CONFIG['BOX_COLOR_2'],\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Add median line\n",
    "        median = street_type_data[street_type]['stats']['median']\n",
    "        ax.hlines(\n",
    "            median,\n",
    "            i - density_norm.max(),\n",
    "            i + density_norm.max(),\n",
    "            colors=PLOT_CONFIG['MEDIAN_COLOR'],\n",
    "            linewidths=1.5\n",
    "        )\n",
    "        \n",
    "        # Add blue median value above line\n",
    "        ax.text(\n",
    "            i, median + 0.35,\n",
    "            f\"{median:.1f}\",\n",
    "            ha='center', va='bottom',\n",
    "            color=PLOT_CONFIG['MEDIAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "    \n",
    "    # Add weighted means as stars\n",
    "    mean_markers = ax.scatter(\n",
    "        range(len(valid_types_sorted)),\n",
    "        [street_type_data[t]['stats']['mean'] for t in valid_types_sorted],\n",
    "        marker='*',\n",
    "        s=150,\n",
    "        color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "        zorder=3\n",
    "    )\n",
    "    \n",
    "    # Add red mean values below stars\n",
    "    for i, street_type in enumerate(valid_types_sorted):\n",
    "        mean = street_type_data[street_type]['stats']['mean']\n",
    "        ax.text(\n",
    "            i, mean - 0.35,\n",
    "            f\"{mean:.1f}\",\n",
    "            ha='center', va='top',\n",
    "            color=PLOT_CONFIG['MEAN_COLOR'],\n",
    "            fontsize=PLOT_CONFIG['TEXT_FONT_SIZE'],\n",
    "            weight='bold'\n",
    "        )\n",
    "    \n",
    "    # X-axis labels with metadata\n",
    "    labels = [\n",
    "        f\"{t}\\n\"\n",
    "        f\"n={street_type_data[t]['effective_n']:.1f}\\n\"\n",
    "        f\"{street_type_data[t]['count']:,} segments\\n\"\n",
    "        f\"{street_type_data[t]['total_length_km']:.1f} km\"\n",
    "        for t in valid_types_sorted\n",
    "    ]\n",
    "    \n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(\n",
    "        labels,\n",
    "        rotation=45,\n",
    "        ha='right',\n",
    "        fontsize=PLOT_CONFIG['TEXT_FONT_SIZE']\n",
    "    )\n",
    "\n",
    "    # Titles and styling\n",
    "    ax.set_ylabel(PLOT_CONFIG['X_LABEL'], fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    ax.set_title(\n",
    "        \"Geschwindigkeitsverteilungen DB Rad+ nach OSM 'type'\\n\"\n",
    "        \"Geigendiagramm, Zeitraum 04/23 - 12/24\",\n",
    "        fontsize=PLOT_CONFIG['TITLE_FONT_SIZE']\n",
    "    )\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Custom legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=PLOT_CONFIG['BOX_COLOR_2'], alpha=0.7, label='Density'),\n",
    "        plt.Line2D([0], [0], color=PLOT_CONFIG['MEDIAN_COLOR'], lw=1.5, label='Median'),\n",
    "        plt.Line2D([0], [0], marker='*', color=PLOT_CONFIG['MEAN_COLOR'], markersize=10, \n",
    "                   linestyle='None', label='Mean')\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='upper right',\n",
    "        fontsize=PLOT_CONFIG['LEGEND_FONT_SIZE']\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, 'street_type_weighted_violin.png'),\n",
    "        dpi=PLOT_CONFIG['DPI'],\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "def create_mean_vs_median_comparison(street_type_data, valid_types):\n",
    "    \"\"\"Create scatter plot comparing weighted means and medians\"\"\"\n",
    "    log(\"Creating mean vs median comparison plot\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Extract means and medians\n",
    "    means = [street_type_data[t]['stats']['mean'] for t in valid_types]\n",
    "    medians = [street_type_data[t]['stats']['median'] for t in valid_types]\n",
    "    \n",
    "    # Calculate skewness using the difference between mean and median\n",
    "    skewness = [mean - median for mean, median in zip(means, medians)]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(means, medians, \n",
    "                          c=skewness, cmap='coolwarm', alpha=0.9, \n",
    "                          s=100, edgecolors='black')\n",
    "    \n",
    "    # Add diagonal line (y=x)\n",
    "    min_val = min(min(means), min(medians)) - 1\n",
    "    max_val = max(max(means), max(medians)) + 1\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, txt in enumerate(valid_types):\n",
    "        plt.annotate(txt, (means[i], medians[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Add colorbar to show skewness\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Skewness (Mean - Median)')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Weighted Mean Speed (km/h)', fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.ylabel('Weighted Median Speed (km/h)', fontsize=PLOT_CONFIG['AXIS_LABEL_FONT_SIZE'])\n",
    "    plt.title('Comparison of Weighted Mean vs Median Speeds by Street Type', fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "    \n",
    "    # Equal aspect ratio\n",
    "    plt.axis('equal')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'mean_vs_median_comparison.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Also create a summary table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Street Type': valid_types,\n",
    "        'Weighted Mean': means,\n",
    "        'Weighted Median': medians,\n",
    "        'Difference (Mean-Median)': skewness,\n",
    "        'Effective N': [street_type_data[t]['effective_n'] for t in valid_types],\n",
    "        'Total Length (km)': [street_type_data[t]['total_length_km'] for t in valid_types]\n",
    "    })\n",
    "    \n",
    "    comparison_df.to_csv(os.path.join(OUTPUT_DIR, 'mean_vs_median_comparison.csv'), index=False)\n",
    "    \n",
    "    # Display the comparison\n",
    "    log(\"\\nComparison of weighted means and medians:\")\n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        log(f\"{row['Street Type']}: Mean = {row['Weighted Mean']:.2f}, Median = {row['Weighted Median']:.2f}, \" +\n",
    "            f\"Diff = {row['Difference (Mean-Median)']:.2f}, Effective N = {row['Effective N']:.1f}\")\n",
    "\n",
    "def perform_kruskal_wallis_and_dunn_tests(street_type_data, valid_types, test_type=\"mean\"):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis test followed by Dunn's post-hoc test directly using \n",
    "    the length-weighted histograms. Test type can be 'mean' or 'median' to determine \n",
    "    sorting of results.\n",
    "    \"\"\"\n",
    "    log(f\"\\nPerforming Kruskal-Wallis test followed by Dunn's post-hoc test (sorting by {test_type})\")\n",
    "    \n",
    "    # Generate samples from histograms for the Kruskal-Wallis test\n",
    "    # We'll limit this to a reasonable number of samples per type to avoid memory issues\n",
    "    max_samples_per_type = 10000\n",
    "    samples_by_type = []\n",
    "    \n",
    "    for street_type in valid_types:\n",
    "        hist = street_type_data[street_type]['length_weighted_agg_hist']\n",
    "        norm_hist = hist / np.sum(hist)\n",
    "        \n",
    "        # Generate samples efficiently using numpy's random choice\n",
    "        bins = np.arange(32)\n",
    "        samples = np.random.choice(bins, size=max_samples_per_type, p=norm_hist)\n",
    "        samples_by_type.append(samples)\n",
    "    \n",
    "    # 1. Perform Kruskal-Wallis test\n",
    "    h_stat, p_value = stats.kruskal(*samples_by_type)\n",
    "    \n",
    "    log(f\"Kruskal-Wallis test results:\")\n",
    "    log(f\"H statistic: {h_stat:.4f}\")\n",
    "    log(f\"p-value: {p_value:.6f}\")\n",
    "    \n",
    "    # Create DataFrame for Dunn's test\n",
    "    all_samples = []\n",
    "    all_types = []\n",
    "    \n",
    "    for i, samples in enumerate(samples_by_type):\n",
    "        all_samples.extend(samples)\n",
    "        all_types.extend([valid_types[i]] * len(samples))\n",
    "    \n",
    "    df_samples = pd.DataFrame({\n",
    "        'speed': all_samples,\n",
    "        'type': all_types\n",
    "    })\n",
    "    \n",
    "    # Calculate Cohen's d effect sizes between all pairs\n",
    "    # Store in a matrix for fast lookups\n",
    "    d_matrix = np.zeros((len(valid_types), len(valid_types)))\n",
    "    mean_diffs = np.zeros((len(valid_types), len(valid_types)))\n",
    "    \n",
    "    for i, type1 in enumerate(valid_types):\n",
    "        for j, type2 in enumerate(valid_types):\n",
    "            if i < j:  # Only calculate once per pair\n",
    "                hist1 = street_type_data[type1]['length_weighted_agg_hist']\n",
    "                hist2 = street_type_data[type2]['length_weighted_agg_hist']\n",
    "                \n",
    "                # Calculate Cohen's d directly from histograms\n",
    "                d, mean1, mean2 = calculate_cohens_d_from_histograms(hist1, hist2)\n",
    "                d_matrix[i, j] = d\n",
    "                d_matrix[j, i] = d  # Mirror for convenience\n",
    "                \n",
    "                diff = mean1 - mean2\n",
    "                mean_diffs[i, j] = diff\n",
    "                mean_diffs[j, i] = -diff  # Mirror with sign change\n",
    "    \n",
    "    # If the Kruskal-Wallis test is significant, perform Dunn's post-hoc test\n",
    "    if p_value < 0.05:\n",
    "        log(\"\\nSignificant differences found between street types (p < 0.05)\")\n",
    "        \n",
    "        # Perform Dunn's test\n",
    "        log(\"Performing Dunn's test for post-hoc analysis\")\n",
    "        dunn_results = posthoc_dunn(df_samples, val_col='speed', group_col='type', p_adjust='fdr_bh')\n",
    "        \n",
    "        # Save Dunn's test results\n",
    "        dunn_results.to_csv(os.path.join(OUTPUT_DIR, f'dunn_test_{test_type}_results.csv'))\n",
    "        \n",
    "        # Create a heatmap of p-values\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        mask = np.triu(np.ones_like(dunn_results, dtype=bool))\n",
    "        \n",
    "        sns.heatmap(dunn_results, mask=mask, annot=True, cmap='coolwarm_r', \n",
    "                  vmin=0, vmax=0.05, center=0.025, \n",
    "                  annot_kws={\"size\": 10}, fmt='.3f')\n",
    "        \n",
    "        plt.title(f\"Vergleich nach OSM 'type': Dunn's post-hoc p-Werte ({test_type}-basiert)\\nSignifikanz: p < 0.05 (rot)\", \n",
    "                fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'dunn_test_{test_type}_heatmap.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Find significantly different pairs and organize into a list\n",
    "        sig_pairs = []\n",
    "        for i in range(len(valid_types)):\n",
    "            for j in range(i+1, len(valid_types)):\n",
    "                type1 = valid_types[i]\n",
    "                type2 = valid_types[j]\n",
    "                \n",
    "                # Get corresponding indices in dunn_results\n",
    "                try:\n",
    "                    dunn_i = dunn_results.index.get_loc(type1)\n",
    "                    dunn_j = dunn_results.columns.get_loc(type2)\n",
    "                    p_val = dunn_results.iloc[dunn_i, dunn_j]\n",
    "                    \n",
    "                    if p_val < 0.05:  # Only include significant pairs\n",
    "                        # Get effect size from pre-computed matrices\n",
    "                        d = d_matrix[i, j]\n",
    "                        diff = mean_diffs[i, j]\n",
    "                        \n",
    "                        sig_pairs.append((type1, type2, p_val, diff, d))\n",
    "                except (KeyError, IndexError):\n",
    "                    log(f\"Warning: Could not find {type1} vs {type2} in Dunn results\")\n",
    "        \n",
    "        # Sort by significance\n",
    "        sig_pairs.sort(key=lambda x: x[2])\n",
    "        \n",
    "        log(\"\\nSignificantly different street type pairs with length-weighted effect sizes:\")\n",
    "        for type1, type2, p_val, diff, d in sig_pairs:\n",
    "            effect_size_interp = \"small\" if d < 0.2 else \"medium\" if d < 0.75 else \"large\"\n",
    "            log(f\"{type1} vs {type2}: p={p_val:.6f}, weighted mean diff={diff:.2f} km/h, Cohen's d={d:.3f} ({effect_size_interp})\")\n",
    "        \n",
    "        # Save effect size results to CSV\n",
    "        effect_size_df = pd.DataFrame(sig_pairs, \n",
    "                                     columns=['Type1', 'Type2', 'p_value', 'weighted_mean_diff', 'cohens_d'])\n",
    "        effect_size_df['effect_size_interpretation'] = effect_size_df['cohens_d'].apply(\n",
    "            lambda d: \"small\" if d < 0.2 else \"medium\" if d < 0.75 else \"large\")\n",
    "        effect_size_df.to_csv(os.path.join(OUTPUT_DIR, f'effect_size_{test_type}_results.csv'), index=False)\n",
    "\n",
    "        # Create a matrix of Cohen's d values with the same structure as dunn_results\n",
    "        # Important: We'll use the same index/column order as dunn_results\n",
    "        cohens_d_df = pd.DataFrame(\n",
    "            np.zeros((len(dunn_results.index), len(dunn_results.columns))),\n",
    "            index=dunn_results.index,\n",
    "            columns=dunn_results.columns\n",
    "        )\n",
    "        \n",
    "        # First, initialize all values to NaN\n",
    "        cohens_d_df.iloc[:, :] = np.nan\n",
    "        \n",
    "        # Then fill in Cohen's d values for ALL pairs (mirror the upper triangle to match Dunn's)\n",
    "        for i in range(len(dunn_results.index)):\n",
    "            for j in range(len(dunn_results.columns)):\n",
    "                if i < j:  # Upper triangle (to match standard scientific reporting)\n",
    "                    type_i = dunn_results.index[i]\n",
    "                    type_j = dunn_results.columns[j]\n",
    "                    \n",
    "                    # Check if this pair is statistically significant\n",
    "                    if dunn_results.iloc[i, j] < 0.05:\n",
    "                        # Get indices in valid_types list\n",
    "                        try:\n",
    "                            idx_i = valid_types.index(type_i)\n",
    "                            idx_j = valid_types.index(type_j)\n",
    "                            \n",
    "                            # Get Cohen's d value from our precomputed matrix\n",
    "                            if idx_i < idx_j:\n",
    "                                cohens_d_df.iloc[i, j] = d_matrix[idx_i, idx_j]\n",
    "                            else:\n",
    "                                cohens_d_df.iloc[i, j] = d_matrix[idx_j, idx_i]\n",
    "                        except ValueError:\n",
    "                            log(f\"Warning: Could not find {type_i} or {type_j} in valid_types\")\n",
    "        \n",
    "        # Save the Cohen's d matrix (only significant differences in upper triangle)\n",
    "        cohens_d_df.to_csv(os.path.join(OUTPUT_DIR, f'cohens_d_matrix_{test_type}.csv'))\n",
    "        \n",
    "        # Create a heatmap visualization of Cohen's d values\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Use a diverging colormap to highlight differences\n",
    "        cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "        \n",
    "        # Create mask for lower triangle (opposite of Dunn's test)\n",
    "        mask = np.tril(np.ones_like(cohens_d_df, dtype=bool))\n",
    "        \n",
    "        # Create heatmap for Cohen's d values\n",
    "        sns.heatmap(cohens_d_df, annot=True, cmap=cmap, \n",
    "                  vmin=0, vmax=1.5, center=0.75,  # 0.75 is the threshold for \"large\" effect\n",
    "                  annot_kws={\"size\": 10}, fmt='.3f',\n",
    "                  mask=mask)  # Apply mask to hide lower triangle\n",
    "        \n",
    "        # Add colorbar labels for effect size interpretation\n",
    "        colorbar = plt.gcf().axes[-1]\n",
    "        colorbar.text(3.5, 0.1, 'Kleiner Effekt (<0.2)', ha='left', va='center')\n",
    "        colorbar.text(3.5, 0.4, 'Mittlerer Effekt (0.2-0.75)', ha='left', va='center')\n",
    "        colorbar.text(3.5, 0.9, 'Großer Effekt (>0.75)', ha='left', va='center')\n",
    "        \n",
    "        plt.title(f\"Vergleich nach OSM 'type': Effektstärken nach Cohen's d ({test_type}-basiert)\\nDifferenzen zwischen OSM 'type' (04/23 - 12/24)\", \n",
    "                 fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'cohens_d_heatmap_{test_type}.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return dunn_results, (d_matrix, mean_diffs)\n",
    "    else:\n",
    "        log(\"No significant differences found between street types (p >= 0.05)\")\n",
    "        return None, (d_matrix, mean_diffs)\n",
    "\n",
    "def analyze_street_types():\n",
    "    \"\"\"\n",
    "    Main analysis function that consistently uses length-weighted histograms for all calculations.\n",
    "    \"\"\"\n",
    "    log(f\"Starting consistent length-weighted analysis of parquet file: {PARQUET_FILE}\")\n",
    "    \n",
    "    # Verify the parquet file exists\n",
    "    if not os.path.exists(PARQUET_FILE):\n",
    "        log(f\"Error: File {PARQUET_FILE} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Get file info\n",
    "    parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "    num_row_groups = parquet_file.metadata.num_row_groups\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    log(f\"Parquet file has {num_row_groups} row groups and approximately {total_rows:,} rows\")\n",
    "    \n",
    "    # Define the specific columns we need\n",
    "    columns = [COLUMN_TYPE, COLUMN_HIST, COLUMN_LENGTH]\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    street_type_data = {}\n",
    "    \n",
    "    # Process each row group with efficient chunking\n",
    "    log(f\"Processing {num_row_groups} row groups to extract speed data by street type\")\n",
    "    \n",
    "    # Define chunk size for efficient processing within each row group\n",
    "    CHUNK_SIZE = 50000\n",
    "    \n",
    "    for rg in tqdm(range(num_row_groups), desc=\"Processing row groups\"):\n",
    "        try:\n",
    "            # Get the row group size to determine number of chunks needed\n",
    "            row_group_metadata = parquet_file.metadata.row_group(rg)\n",
    "            row_group_size = row_group_metadata.num_rows\n",
    "            num_chunks = (row_group_size + CHUNK_SIZE - 1) // CHUNK_SIZE  # Ceiling division\n",
    "            \n",
    "            # Read the entire row group\n",
    "            table = parquet_file.read_row_group(rg, columns=columns)\n",
    "            \n",
    "            # Process each chunk of the row group\n",
    "            for chunk_idx in tqdm(range(num_chunks), desc=f\"Chunks in row group {rg}\", leave=False):\n",
    "                # Calculate offsets\n",
    "                offset = chunk_idx * CHUNK_SIZE\n",
    "                length = min(CHUNK_SIZE, row_group_size - offset)\n",
    "                \n",
    "                # Extract just this chunk from the row group\n",
    "                chunk_table = table.slice(offset, length)\n",
    "                df_chunk = chunk_table.to_pandas()\n",
    "                \n",
    "                # Filter out NaN values\n",
    "                df_chunk = df_chunk.dropna(subset=[COLUMN_TYPE, COLUMN_HIST, COLUMN_LENGTH])\n",
    "                \n",
    "                # Filter to only include the specified street types\n",
    "                df_chunk = df_chunk[df_chunk[COLUMN_TYPE].isin(STREET_TYPES_TO_ANALYZE)]\n",
    "                \n",
    "                # Process each row in this chunk\n",
    "                for _, row in df_chunk.iterrows():\n",
    "                    street_type = row[COLUMN_TYPE]\n",
    "                    hist = parse_histogram(row[COLUMN_HIST])\n",
    "                    segment_length = row[COLUMN_LENGTH]\n",
    "                    \n",
    "                    if len(hist) > 0 and np.sum(hist) > 0:\n",
    "                        # Initialize entry for this street type if it doesn't exist\n",
    "                        if street_type not in street_type_data:\n",
    "                            street_type_data[street_type] = {\n",
    "                                'lengths': [],  # Store segment lengths\n",
    "                                'length_weighted_agg_hist': np.zeros(32),  # Initialize aggregated histogram\n",
    "                                'count': 0\n",
    "                            }\n",
    "                        \n",
    "                        # Add data for this street segment\n",
    "                        street_type_data[street_type]['lengths'].append(segment_length)\n",
    "                        \n",
    "                        # Add to the aggregated histogram with length weighting\n",
    "                        # Multiply histogram by segment length for length-weighting\n",
    "                        street_type_data[street_type]['length_weighted_agg_hist'] += hist * segment_length\n",
    "                        \n",
    "                        street_type_data[street_type]['count'] += 1\n",
    "                \n",
    "                # Clean up to free memory after each chunk\n",
    "                del df_chunk, chunk_table\n",
    "                force_gc()\n",
    "            \n",
    "            # Clean up the entire row group data\n",
    "            del table\n",
    "            force_gc()\n",
    "                \n",
    "        except Exception as e:\n",
    "            log(f\"Error processing row group {rg}: {e}\")\n",
    "    \n",
    "    # Count total processed segments\n",
    "    total_segments = sum(data['count'] for data in street_type_data.values())\n",
    "    log(f\"Processed {total_segments:,} total street segments across {len(street_type_data)} different street types\")\n",
    "    \n",
    "    # Check if we have data for all the requested street types\n",
    "    for street_type in STREET_TYPES_TO_ANALYZE:\n",
    "        if street_type not in street_type_data:\n",
    "            log(f\"Warning: No data found for street type '{street_type}'\")\n",
    "        elif street_type_data[street_type]['count'] < 30:\n",
    "            log(f\"Warning: Insufficient data for street type '{street_type}' (only {street_type_data[street_type]['count']} segments)\")\n",
    "    \n",
    "    # Get the street types that have sufficient data (at least 30 segments)\n",
    "    valid_types = [st for st in STREET_TYPES_TO_ANALYZE \n",
    "                  if st in street_type_data and street_type_data[st]['count'] >= 30]\n",
    "    \n",
    "    log(f\"Analysis will include {len(valid_types)} street types with sufficient data\")\n",
    "    for i, type_name in enumerate(valid_types, 1):\n",
    "        count = street_type_data[type_name]['count']\n",
    "        log(f\"{i}. {type_name}: {count:,} segments\")\n",
    "    \n",
    "    # Calculate statistics for each street type using only the length-weighted histogram\n",
    "    # We'll do this ONCE for each street type and store the results for reuse\n",
    "    log(\"Calculating length-weighted statistics for each street type\")\n",
    "    for street_type in valid_types:\n",
    "        # Calculate total length in km\n",
    "        total_length_km = sum(street_type_data[street_type]['lengths']) / 1000\n",
    "        \n",
    "        # Calculate effective sample size\n",
    "        effective_n = calculate_effective_n(street_type_data[street_type]['lengths'])\n",
    "        \n",
    "        # Calculate all statistics efficiently in a single pass\n",
    "        agg_hist = street_type_data[street_type]['length_weighted_agg_hist']\n",
    "        stats = calculate_all_weighted_statistics(agg_hist)\n",
    "        \n",
    "        # Store all calculated statistics\n",
    "        street_type_data[street_type]['stats'] = stats\n",
    "        street_type_data[street_type]['effective_n'] = effective_n\n",
    "        street_type_data[street_type]['total_length_km'] = total_length_km\n",
    "        \n",
    "        log(f\"{street_type}: weighted mean = {stats['mean']:.2f} km/h, weighted median = {stats['median']:.2f} km/h, effective n = {effective_n:.1f}\")\n",
    "    \n",
    "    # Sort valid types by weighted mean for visualizations\n",
    "    valid_types_sorted_by_mean = sorted(valid_types, \n",
    "                                        key=lambda x: street_type_data[x]['stats']['mean'] if x in street_type_data else 0,\n",
    "                                        reverse=True)\n",
    "    \n",
    "    # Sort valid types by weighted median for comparison\n",
    "    valid_types_sorted_by_median = sorted(valid_types, \n",
    "                                         key=lambda x: street_type_data[x]['stats']['median'] if x in street_type_data else 0,\n",
    "                                         reverse=True)\n",
    "    \n",
    "    # Create visualizations\n",
    "    log(\"Creating visualizations based on length-weighted histograms\")\n",
    "    \n",
    "    # 1. Create length-weighted histogram comparison (sorted by mean)\n",
    "    histograms = []\n",
    "    hist_labels = []\n",
    "    hist_stats = []\n",
    "    \n",
    "    for street_type in valid_types_sorted_by_mean:\n",
    "        histograms.append(street_type_data[street_type]['length_weighted_agg_hist'])\n",
    "        hist_labels.append(street_type)\n",
    "        hist_stats.append({\n",
    "            'stats': street_type_data[street_type]['stats'],\n",
    "            'total_length_km': street_type_data[street_type]['total_length_km'],\n",
    "            'effective_n': street_type_data[street_type]['effective_n']\n",
    "        })\n",
    "    \n",
    "    create_length_weighted_histogram_comparison(histograms, hist_labels, hist_stats)\n",
    "    \n",
    "    # 2. Create median-sorted histogram comparison\n",
    "    histograms_by_median = []\n",
    "    hist_labels_by_median = []\n",
    "    hist_stats_by_median = []\n",
    "    \n",
    "    for street_type in valid_types_sorted_by_median:\n",
    "        histograms_by_median.append(street_type_data[street_type]['length_weighted_agg_hist'])\n",
    "        hist_labels_by_median.append(street_type)\n",
    "        hist_stats_by_median.append({\n",
    "            'stats': street_type_data[street_type]['stats'],\n",
    "            'total_length_km': street_type_data[street_type]['total_length_km'],\n",
    "            'effective_n': street_type_data[street_type]['effective_n']\n",
    "        })\n",
    "    \n",
    "    create_median_weighted_histogram_comparison(histograms_by_median, hist_labels_by_median, hist_stats_by_median)\n",
    "    \n",
    "    # 3. Create boxplot visualization\n",
    "    create_boxplot_from_histograms(street_type_data, valid_types_sorted_by_mean)\n",
    "    \n",
    "    # 4. Create violin plot\n",
    "    create_violin_plot_from_histograms(street_type_data, valid_types_sorted_by_mean)\n",
    "    \n",
    "    # 5. Create mean vs median comparison\n",
    "    create_mean_vs_median_comparison(street_type_data, valid_types_sorted_by_mean)\n",
    "    \n",
    "    # 6. Perform Kruskal-Wallis and Dunn's tests based on mean-sorted order\n",
    "    dunn_results_mean, effect_sizes_mean = perform_kruskal_wallis_and_dunn_tests(\n",
    "        street_type_data, valid_types_sorted_by_mean, \"mean\")\n",
    "    \n",
    "    # 7. Perform Kruskal-Wallis and Dunn's tests based on median-sorted order\n",
    "    dunn_results_median, effect_sizes_median = perform_kruskal_wallis_and_dunn_tests(\n",
    "        street_type_data, valid_types_sorted_by_median, \"median\")\n",
    "    \n",
    "    # 8. Compare the two approaches if both tests were significant\n",
    "    if dunn_results_mean is not None and dunn_results_median is not None:\n",
    "        log(\"\\nComparing mean-based and median-based significant pair findings:\")\n",
    "        \n",
    "        # Get sets of significant pairs from both tests\n",
    "        mean_pairs = set()\n",
    "        median_pairs = set()\n",
    "        \n",
    "        # Extract significant pairs from Dunn's test results\n",
    "        for i in range(len(valid_types_sorted_by_mean)):\n",
    "            for j in range(i+1, len(valid_types_sorted_by_mean)):\n",
    "                if dunn_results_mean.iloc[i, j] < 0.05:\n",
    "                    mean_pairs.add((valid_types_sorted_by_mean[i], valid_types_sorted_by_mean[j]))\n",
    "        \n",
    "        for i in range(len(valid_types_sorted_by_median)):\n",
    "            for j in range(i+1, len(valid_types_sorted_by_median)):\n",
    "                if dunn_results_median.iloc[i, j] < 0.05:\n",
    "                    median_pairs.add((valid_types_sorted_by_median[i], valid_types_sorted_by_median[j]))\n",
    "        \n",
    "        # Find pairs that were significant in both tests\n",
    "        common_pairs = mean_pairs.intersection(median_pairs)\n",
    "        only_mean_pairs = mean_pairs - median_pairs\n",
    "        only_median_pairs = median_pairs - mean_pairs\n",
    "        \n",
    "        log(f\"Number of pairs significant in both tests: {len(common_pairs)}\")\n",
    "        log(f\"Number of pairs significant only in mean-based test: {len(only_mean_pairs)}\")\n",
    "        log(f\"Number of pairs significant only in median-based test: {len(only_median_pairs)}\")\n",
    "        \n",
    "        # List the common pairs\n",
    "        if common_pairs:\n",
    "            log(\"\\nPairs significant in both tests:\")\n",
    "            for pair in common_pairs:\n",
    "                log(f\"{pair[0]} vs {pair[1]}\")\n",
    "                \n",
    "        # List pairs only significant in mean test\n",
    "        if only_mean_pairs:\n",
    "            log(\"\\nPairs significant only in mean-based test:\")\n",
    "            for pair in only_mean_pairs:\n",
    "                log(f\"{pair[0]} vs {pair[1]}\")\n",
    "        \n",
    "        # List pairs only significant in median test\n",
    "        if only_median_pairs:\n",
    "            log(\"\\nPairs significant only in median-based test:\")\n",
    "            for pair in only_median_pairs:\n",
    "                log(f\"{pair[0]} vs {pair[1]}\")\n",
    "        \n",
    "        # Create Venn diagram of significant pairs if matplotlib_venn is available\n",
    "        try:\n",
    "            from matplotlib_venn import venn2\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            venn2(subsets=(len(only_mean_pairs), len(only_median_pairs), len(common_pairs)),\n",
    "                set_labels=('Mean-based', 'Median-based'))\n",
    "            plt.title(\"Significant Differences Found: Mean vs Median Approach\", fontsize=PLOT_CONFIG['TITLE_FONT_SIZE'])\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, 'mean_vs_median_venn.png'), dpi=PLOT_CONFIG['DPI'], bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except ImportError:\n",
    "            log(\"matplotlib_venn not available for creating Venn diagram. Install with 'pip install matplotlib-venn'\")\n",
    "    \n",
    "    # 9. Create summary statistics table\n",
    "    summary_data = []\n",
    "    for street_type in valid_types_sorted_by_mean:\n",
    "        stats = street_type_data[street_type]['stats']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'type': street_type,\n",
    "            'count': street_type_data[street_type]['count'],\n",
    "            'total_length_km': street_type_data[street_type]['total_length_km'],\n",
    "            'effective_n': street_type_data[street_type]['effective_n'],\n",
    "            'mean': stats['mean'],\n",
    "            'median': stats['median'],\n",
    "            'std': stats['std'],\n",
    "            'min': stats['min'],\n",
    "            'max': stats['max'],\n",
    "            'percentile_5': stats['percentile_5'],\n",
    "            'percentile_25': stats['percentile_25'],\n",
    "            'percentile_75': stats['percentile_75'],\n",
    "            'percentile_95': stats['percentile_95'],\n",
    "            'skewness': stats['mean'] - stats['median']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(OUTPUT_DIR, 'street_type_summary_stats.csv'), index=False)\n",
    "    \n",
    "    log(\"Consistent length-weighted analysis complete. All results are based on the same length-weighted histograms.\")\n",
    "    return street_type_data\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    street_data = analyze_street_types()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
