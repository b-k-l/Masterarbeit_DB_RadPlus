{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3485aa87-3627-444e-a6ee-b1ffac600f0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting district metrics calculation\n",
      "Loading districts from data/bezirke_berlin.geojson\n",
      "Opening parquet file: data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\n",
      "Parquet file has 12 row groups and approximately 466,957 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing row groups:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 0: 100%|██████████| 1/1 [00:29<00:00, 29.79s/it]\n",
      "Processing row groups:   8%|▊         | 1/12 [00:29<05:29, 29.91s/it]\n",
      "\u001b[Anks in group 1:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 1: 100%|██████████| 1/1 [00:23<00:00, 23.35s/it]\n",
      "Processing row groups:  17%|█▋        | 2/12 [00:53<04:21, 26.12s/it]\n",
      "\u001b[Anks in group 2:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 2: 100%|██████████| 1/1 [00:25<00:00, 25.70s/it]\n",
      "Processing row groups:  25%|██▌       | 3/12 [01:19<03:53, 25.98s/it]\n",
      "\u001b[Anks in group 3:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 3: 100%|██████████| 1/1 [00:29<00:00, 29.58s/it]\n",
      "Processing row groups:  33%|███▎      | 4/12 [01:48<03:39, 27.46s/it]\n",
      "\u001b[Anks in group 4:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 4: 100%|██████████| 1/1 [00:28<00:00, 28.74s/it]\n",
      "Processing row groups:  42%|████▏     | 5/12 [02:17<03:15, 27.96s/it]\n",
      "\u001b[Anks in group 5:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 5: 100%|██████████| 1/1 [00:30<00:00, 30.67s/it]\n",
      "Processing row groups:  50%|█████     | 6/12 [02:48<02:53, 28.93s/it]\n",
      "\u001b[Anks in group 6:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 6: 100%|██████████| 1/1 [00:29<00:00, 29.34s/it]\n",
      "Processing row groups:  58%|█████▊    | 7/12 [03:18<02:25, 29.11s/it]\n",
      "\u001b[Anks in group 7:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 7: 100%|██████████| 1/1 [00:26<00:00, 26.26s/it]\n",
      "Processing row groups:  67%|██████▋   | 8/12 [03:44<01:52, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row group 8: Corrupt snappy compressed data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Anks in group 9:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 9: 100%|██████████| 1/1 [00:36<00:00, 36.03s/it]\n",
      "Processing row groups:  83%|████████▎ | 10/12 [04:20<00:46, 23.41s/it]\n",
      "\u001b[Anks in group 10:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 10: 100%|██████████| 1/1 [00:34<00:00, 34.75s/it]\n",
      "Processing row groups:  92%|█████████▏| 11/12 [04:55<00:26, 26.33s/it]\n",
      "\u001b[Anks in group 11:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Anks in group 11: 100%|██████████| 1/1 [00:30<00:00, 30.53s/it]\n",
      "Processing row groups: 100%|██████████| 12/12 [05:26<00:00, 27.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final statistics for each district\n",
      "Saving output to data/bezirke_berlin_metrics.geojson\n",
      "Saved CSV to analysis_results/Bezirke/bezirke_berlin_metrics.csv\n",
      "\n",
      "District Metrics Summary:\n",
      "                        Bezirk  mean_speed  median_speed  total_length_km  \\\n",
      "8             Treptow-Köpenick   20.720511     20.753022         1769.911   \n",
      "6                      Spandau   20.218258     20.111272          916.086   \n",
      "9          Marzahn-Hellersdorf   20.137714     20.051772         1137.002   \n",
      "10                 Lichtenberg   19.966903     20.057429         1019.599   \n",
      "11               Reinickendorf   19.872396     19.818555         1015.620   \n",
      "7          Steglitz-Zehlendorf   19.869438     19.734697         1502.659   \n",
      "4                     Neukölln   19.857536     19.824575          782.853   \n",
      "5   Charlottenburg-Wilmersdorf   19.424771     19.328688         1465.697   \n",
      "2                       Pankow   19.364373     19.347080         1598.759   \n",
      "3         Tempelhof-Schöneberg   19.349305     19.327201         1283.433   \n",
      "0                        Mitte   19.348169     19.369594         1389.345   \n",
      "1     Friedrichshain-Kreuzberg   19.249734     19.272786          807.825   \n",
      "\n",
      "    segment_count  \n",
      "8           42941  \n",
      "6           19505  \n",
      "9           36502  \n",
      "10          31726  \n",
      "11          22389  \n",
      "7           35546  \n",
      "4           28490  \n",
      "5           41825  \n",
      "2           46817  \n",
      "3           41986  \n",
      "0           50777  \n",
      "1           35412  \n",
      "District metrics calculation complete\n"
     ]
    }
   ],
   "source": [
    "# Auswertung der DB Rad+ Daten auf Bezirksebene\n",
    "# Zur Erstellung des Codes wurde die generative Künstliche Intelligenz (KI) „Claude AI“ des Anbieters Anthropic in Version 3.7 genutzt\n",
    "\n",
    "# Calculate metrics for each Berlin district from network data\n",
    "# This script creates a new GeoJSON with metrics derived from network segments in each district\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "INPUT_PARQUET = \"data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\"\n",
    "DISTRICTS_GEOJSON = \"data/bezirke_berlin.geojson\"\n",
    "OUTPUT_GEOJSON = \"data/bezirke_berlin_metrics.geojson\"\n",
    "OUTPUT_DIR = \"analysis_results/Bezirke\"\n",
    "COLUMN_HIST = '2304-2412_speeds'  # Column for speed histogram data\n",
    "COLUMN_LENGTH = 'length_m'  # Column for length in meters\n",
    "COLUMN_ROUTE_COUNT = '2304-2412_route_count'  # Column for route count\n",
    "CHUNK_SIZE = 50000  # Process this many rows at a time (adjust based on available memory)\n",
    "N_WORKERS = 2  # Number of parallel workers\n",
    "\n",
    "# Fields to read from parquet - keeping minimal to reduce memory usage\n",
    "PARQUET_FIELDS = ['id', 'geometry_wkt', COLUMN_HIST, COLUMN_LENGTH, COLUMN_ROUTE_COUNT]\n",
    "\n",
    "# Ensure output directory exists\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print a log message\"\"\"\n",
    "    print(message)\n",
    "\n",
    "def force_gc():\n",
    "    \"\"\"Force garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "def parse_histogram(hist_str):\n",
    "    \"\"\"Parse histogram string to numpy array\"\"\"\n",
    "    try:\n",
    "        if isinstance(hist_str, str):\n",
    "            hist_str = hist_str.strip('[]')\n",
    "            # Fast NumPy parsing\n",
    "            try:\n",
    "                values = np.fromstring(hist_str, sep=',')\n",
    "                return values\n",
    "            except:\n",
    "                # Fallback to manual parsing if NumPy method fails\n",
    "                values = [float(x) for x in hist_str.split(',')]\n",
    "                return np.array(values)\n",
    "        else:\n",
    "            return np.zeros(32)  # Return zeros for missing histograms\n",
    "    except Exception as e:\n",
    "        log(f\"Error parsing histogram: {e}\")\n",
    "        return np.zeros(32)\n",
    "\n",
    "def calculate_histogram_statistics(histogram):\n",
    "    \"\"\"\n",
    "    Calculate statistics from a histogram\n",
    "    Returns a dictionary with mean, median, and std\n",
    "    \"\"\"\n",
    "    if np.sum(histogram) == 0:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'median': np.nan,\n",
    "            'std': np.nan\n",
    "        }\n",
    "    \n",
    "    speed_bins = np.arange(32)  # 0-31 km/h\n",
    "    total_count = np.sum(histogram)\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    norm_hist = histogram / total_count\n",
    "    \n",
    "    # Calculate weighted mean\n",
    "    mean = np.sum(speed_bins * norm_hist)\n",
    "    \n",
    "    # Calculate variance and std\n",
    "    variance = np.sum(((speed_bins - mean) ** 2) * norm_hist)\n",
    "    std = np.sqrt(variance)\n",
    "    \n",
    "    # Calculate median using cumulative distribution\n",
    "    cum_dist = np.cumsum(norm_hist)\n",
    "    median = np.interp(0.5, cum_dist, speed_bins)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'std': std\n",
    "    }\n",
    "\n",
    "def process_district_chunk(district_name, district_geom, chunk_df):\n",
    "    \"\"\"\n",
    "    Process a chunk of network data for a specific district.\n",
    "    Returns metrics derived from segments intersecting with the district.\n",
    "    \"\"\"\n",
    "    # Filter to segments that intersect with this district\n",
    "    geometries = chunk_df['geometry_wkt'].apply(wkt.loads)\n",
    "    intersects = geometries.apply(lambda g: g.intersects(district_geom) if g else False)\n",
    "    district_segments = chunk_df.loc[intersects].copy()\n",
    "    \n",
    "    if district_segments.empty:\n",
    "        # Return empty results if no segments in this district from this chunk\n",
    "        return {\n",
    "            'segment_count': 0,\n",
    "            'total_length_km': 0,\n",
    "            'total_route_count': 0,\n",
    "            'aggregated_histogram': np.zeros(32),\n",
    "            'segments': [] \n",
    "        }\n",
    "    \n",
    "    # Parse histograms\n",
    "    district_segments['parsed_histogram'] = district_segments[COLUMN_HIST].apply(parse_histogram)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    segment_count = len(district_segments)\n",
    "    total_length_km = district_segments[COLUMN_LENGTH].sum() / 1000\n",
    "    total_route_count = district_segments[COLUMN_ROUTE_COUNT].sum()\n",
    "    \n",
    "    # Aggregate histograms weighted by length\n",
    "    aggregated_histogram = np.zeros(32)\n",
    "    for idx, row in district_segments.iterrows():\n",
    "        hist = row['parsed_histogram']\n",
    "        length = row[COLUMN_LENGTH]\n",
    "        if np.sum(hist) > 0:\n",
    "            aggregated_histogram += hist * length\n",
    "    \n",
    "    # Save segment IDs for potential future reference\n",
    "    segment_ids = district_segments['id'].tolist()\n",
    "    \n",
    "    return {\n",
    "        'segment_count': segment_count,\n",
    "        'total_length_km': total_length_km,\n",
    "        'total_route_count': total_route_count,\n",
    "        'aggregated_histogram': aggregated_histogram,\n",
    "        'segments': segment_ids\n",
    "    }\n",
    "\n",
    "def calculate_district_metrics():\n",
    "    \"\"\"\n",
    "    Calculate metrics for each district by processing the network data\n",
    "    and identifying segments that intersect with each district.\n",
    "    \"\"\"\n",
    "    log(f\"Loading districts from {DISTRICTS_GEOJSON}\")\n",
    "    # Load districts\n",
    "    districts_gdf = gpd.read_file(DISTRICTS_GEOJSON)\n",
    "    \n",
    "    # Ensure CRS is EPSG:25833\n",
    "    if districts_gdf.crs != \"EPSG:25833\":\n",
    "        districts_gdf = districts_gdf.to_crs(\"EPSG:25833\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    district_results = {}\n",
    "    for idx, row in districts_gdf.iterrows():\n",
    "        district_name = row['bbez_name']\n",
    "        district_results[district_name] = {\n",
    "            'segment_count': 0,\n",
    "            'total_length_km': 0,\n",
    "            'total_route_count': 0,\n",
    "            'aggregated_histogram': np.zeros(32),\n",
    "            'segments': []\n",
    "        }\n",
    "    \n",
    "    # Open parquet file\n",
    "    log(f\"Opening parquet file: {INPUT_PARQUET}\")\n",
    "    parquet_file = pq.ParquetFile(INPUT_PARQUET)\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    num_row_groups = parquet_file.metadata.num_row_groups\n",
    "    log(f\"Parquet file has {num_row_groups} row groups and approximately {total_rows:,} rows\")\n",
    "    \n",
    "    # Process each row group\n",
    "    for rg in tqdm(range(num_row_groups), desc=\"Processing row groups\"):\n",
    "        try:\n",
    "            # Get row group size\n",
    "            row_group_metadata = parquet_file.metadata.row_group(rg)\n",
    "            row_group_size = row_group_metadata.num_rows\n",
    "            \n",
    "            # Read only necessary columns from this row group\n",
    "            table = parquet_file.read_row_group(rg, columns=PARQUET_FIELDS)\n",
    "            df = table.to_pandas()\n",
    "            \n",
    "            # Handle NaN values\n",
    "            df = df.dropna(subset=['geometry_wkt', COLUMN_HIST, COLUMN_LENGTH, COLUMN_ROUTE_COUNT])\n",
    "            \n",
    "            # Process in smaller chunks to manage memory\n",
    "            num_chunks = (len(df) + CHUNK_SIZE - 1) // CHUNK_SIZE  # Ceiling division\n",
    "            \n",
    "            for chunk_idx in tqdm(range(num_chunks), desc=f\"Chunks in group {rg}\", leave=False):\n",
    "                # Get chunk\n",
    "                start_idx = chunk_idx * CHUNK_SIZE\n",
    "                end_idx = min(start_idx + CHUNK_SIZE, len(df))\n",
    "                chunk_df = df.iloc[start_idx:end_idx]\n",
    "                \n",
    "                # Process this chunk for each district\n",
    "                for idx, row in districts_gdf.iterrows():\n",
    "                    district_name = row['bbez_name']\n",
    "                    district_geom = row['geometry']\n",
    "                    \n",
    "                    # Process chunk for this district\n",
    "                    chunk_results = process_district_chunk(district_name, district_geom, chunk_df)\n",
    "                    \n",
    "                    # Aggregate results\n",
    "                    district_results[district_name]['segment_count'] += chunk_results['segment_count']\n",
    "                    district_results[district_name]['total_length_km'] += chunk_results['total_length_km']\n",
    "                    district_results[district_name]['total_route_count'] += chunk_results['total_route_count']\n",
    "                    district_results[district_name]['aggregated_histogram'] += chunk_results['aggregated_histogram']\n",
    "                    district_results[district_name]['segments'].extend(chunk_results['segments'])\n",
    "                \n",
    "                # Clean up the chunk data\n",
    "                del chunk_df\n",
    "                force_gc()\n",
    "            \n",
    "            # Clean up the row group data\n",
    "            del df, table\n",
    "            force_gc()\n",
    "            \n",
    "        except Exception as e:\n",
    "            log(f\"Error processing row group {rg}: {e}\")\n",
    "    \n",
    "    # Calculate final statistics for each district\n",
    "    log(\"Calculating final statistics for each district\")\n",
    "    for district_name, metrics in district_results.items():\n",
    "        # Calculate statistics from aggregated histogram\n",
    "        histogram = metrics['aggregated_histogram']\n",
    "        if np.sum(histogram) > 0:\n",
    "            stats = calculate_histogram_statistics(histogram)\n",
    "            metrics['mean_speed'] = stats['mean']\n",
    "            metrics['median_speed'] = stats['median']\n",
    "            metrics['speed_std'] = stats['std']\n",
    "            \n",
    "            # Normalize the aggregated histogram\n",
    "            metrics['normalized_histogram'] = histogram / np.sum(histogram)\n",
    "        else:\n",
    "            metrics['mean_speed'] = np.nan\n",
    "            metrics['median_speed'] = np.nan\n",
    "            metrics['speed_std'] = np.nan\n",
    "            metrics['normalized_histogram'] = np.zeros(32)\n",
    "    \n",
    "    return districts_gdf, district_results\n",
    "\n",
    "def create_output_geojson(districts_gdf, district_results):\n",
    "    \"\"\"\n",
    "    Create a new GeoJSON file with district metrics\n",
    "    \"\"\"\n",
    "    # Copy districts GeoDataFrame\n",
    "    output_gdf = districts_gdf.copy()\n",
    "    \n",
    "    # Rename bbez_name to Bezirk\n",
    "    output_gdf = output_gdf.rename(columns={'bbez_name': 'Bezirk'})\n",
    "    \n",
    "    # Add metrics columns\n",
    "    output_gdf['mean_speed'] = output_gdf['Bezirk'].map(lambda x: district_results[x]['mean_speed'])\n",
    "    output_gdf['median_speed'] = output_gdf['Bezirk'].map(lambda x: district_results[x]['median_speed'])\n",
    "    output_gdf['total_route_count'] = output_gdf['Bezirk'].map(lambda x: district_results[x]['total_route_count'])\n",
    "    output_gdf['segment_count'] = output_gdf['Bezirk'].map(lambda x: district_results[x]['segment_count'])\n",
    "    output_gdf['total_length_km'] = output_gdf['Bezirk'].map(lambda x: district_results[x]['total_length_km'])\n",
    "    output_gdf['speed_std'] = output_gdf['Bezirk'].map(lambda x: district_results[x]['speed_std'])\n",
    "    \n",
    "    # Convert normalized histogram to string for storage in GeoJSON\n",
    "    output_gdf['normalized_histogram'] = output_gdf['Bezirk'].map(\n",
    "        lambda x: np.array2string(district_results[x]['normalized_histogram'], precision=6, separator=',')\n",
    "    )\n",
    "    \n",
    "    # Save to file\n",
    "    log(f\"Saving output to {OUTPUT_GEOJSON}\")\n",
    "    output_gdf.to_file(OUTPUT_GEOJSON, driver='GeoJSON')\n",
    "    \n",
    "    # Also save as CSV for easier analysis\n",
    "    csv_path = os.path.join(OUTPUT_DIR, 'bezirke_berlin_metrics.csv')\n",
    "    output_gdf.drop('geometry', axis=1).to_csv(csv_path, index=False)\n",
    "    log(f\"Saved CSV to {csv_path}\")\n",
    "    \n",
    "    return output_gdf\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    log(\"Starting district metrics calculation\")\n",
    "    \n",
    "    # Calculate metrics for each district\n",
    "    districts_gdf, district_results = calculate_district_metrics()\n",
    "    \n",
    "    # Create and save output GeoJSON\n",
    "    output_gdf = create_output_geojson(districts_gdf, district_results)\n",
    "    \n",
    "    # Print summary\n",
    "    log(\"\\nDistrict Metrics Summary:\")\n",
    "    summary_df = output_gdf[['Bezirk', 'mean_speed', 'median_speed', 'total_length_km', 'segment_count']].copy()\n",
    "    summary_df = summary_df.sort_values('mean_speed', ascending=False)\n",
    "    print(summary_df)\n",
    "    \n",
    "    log(\"District metrics calculation complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
