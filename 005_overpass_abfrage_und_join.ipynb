{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c6534-b14b-44ef-8aeb-a5bb2a2e385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abfrage von OSM-Daten aus overpass-API, Parallele Segmente in DB Rad+ Daten finden, optional neues Attribut in Rad+ Datensatz aufnehmen\n",
    "# Zur Erstellung des Codes wurde die generative Künstliche Intelligenz (KI) „Claude AI“ des Anbieters Anthropic in Version 3.7 genutzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361af23b-1ace-4391-8943-d66f61ec37a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame Head:\n",
      "             bicycle cycleway:left cycleway:right          foot  highway  \\\n",
      "0  optional_sidepath            no       separate  use_sidepath  primary   \n",
      "1  optional_sidepath            no       separate  use_sidepath  primary   \n",
      "2  optional_sidepath            no       separate  use_sidepath  primary   \n",
      "3  optional_sidepath            no       separate  use_sidepath  primary   \n",
      "4  optional_sidepath            no       separate  use_sidepath  primary   \n",
      "\n",
      "  lanes  lit maxspeed        name name:etymology:wikidata  ... destination  \\\n",
      "0     4  yes       50  Kaiserdamm                   Q2677  ...         NaN   \n",
      "1     4  yes       50  Kaiserdamm                   Q2677  ...         NaN   \n",
      "2     4  yes       50  Kaiserdamm                   Q2677  ...         NaN   \n",
      "3     4  yes       50  Kaiserdamm                   Q2677  ...         NaN   \n",
      "4     4  yes       50  Kaiserdamm                   Q2677  ...         NaN   \n",
      "\n",
      "  destination:colour destination:ref:to destination:symbol:to destination:to  \\\n",
      "0                NaN                NaN                   NaN            NaN   \n",
      "1                NaN                NaN                   NaN            NaN   \n",
      "2                NaN                NaN                   NaN            NaN   \n",
      "3                NaN                NaN                   NaN            NaN   \n",
      "4                NaN                NaN                   NaN            NaN   \n",
      "\n",
      "  postal_code destination:ref sidewalk:right:surface cycleway:both sidewalk  \n",
      "0         NaN             NaN                    NaN           NaN      NaN  \n",
      "1         NaN             NaN                    NaN           NaN      NaN  \n",
      "2         NaN             NaN                    NaN           NaN      NaN  \n",
      "3         NaN             NaN                    NaN           NaN      NaN  \n",
      "4         NaN             NaN                    NaN           NaN      NaN  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "\n",
      "Columns in GeoDataFrame:\n",
      "['bicycle', 'cycleway:left', 'cycleway:right', 'foot', 'highway', 'lanes', 'lit', 'maxspeed', 'name', 'name:etymology:wikidata', 'name:etymology:wikipedia', 'oneway', 'parking:left', 'parking:right', 'parking:right:orientation', 'ref', 'sidewalk:left', 'sidewalk:right', 'smoothness', 'surface', 'wikidata', 'wikipedia', 'zone:traffic', 'id', 'type', 'geometry', 'lane_markings', 'parking:both', 'bridge', 'bridge:name', 'layer', 'maxweight:signed', 'shoulder', 'turn:lanes', 'destination', 'destination:colour', 'destination:ref:to', 'destination:symbol:to', 'destination:to', 'postal_code', 'destination:ref', 'sidewalk:right:surface', 'cycleway:both', 'sidewalk']\n",
      "\n",
      "CRS Information:\n",
      "EPSG:25833\n"
     ]
    }
   ],
   "source": [
    "# Step 1: var 1 Request data from overpass API (one Street Name, TEST Kaiserdamm)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "import urllib.parse\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    response = requests.get(overpass_url, params={'data': query})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        for node_id in way['nodes']:\n",
    "            if node_id in nodes:\n",
    "                point = nodes[node_id]\n",
    "                coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Your query\n",
    "query = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "// Get Berlin using its relation ID\n",
    "area(3600062422)->.berlin;  // 62422 is Berlin's relation ID\n",
    "// Get the street only within Berlin area\n",
    "way[\"name\"=\"Kaiserdamm\"](area.berlin)->.mainStreet;\n",
    "(\n",
    "  // The main street\n",
    "  way.mainStreet;\n",
    "  // All nodes of the main street\n",
    "  node(w.mainStreet);\n",
    ");\n",
    "out body;\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"kaiserdamm.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"street\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ccbf11-a26a-4314-8d44-48d9c4f92174",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame Head:\n",
      "  cycleway:both      highway  \\\n",
      "0            no  residential   \n",
      "1            no  residential   \n",
      "2            no  residential   \n",
      "3            no  residential   \n",
      "4            no  residential   \n",
      "\n",
      "                                               image lane_markings  lit  \\\n",
      "0  https://commons.wikimedia.org/wiki/File:Mitte_...            no  yes   \n",
      "1  https://commons.wikimedia.org/wiki/File:Mitte_...            no  yes   \n",
      "2  https://commons.wikimedia.org/wiki/File:Mitte_...           NaN  yes   \n",
      "3                                                NaN           NaN  yes   \n",
      "4                                                NaN            no  yes   \n",
      "\n",
      "  maxspeed maxspeed:type              name name:etymology:wikidata  \\\n",
      "0       30     DE:zone30    Oberwallstraße                  Q91203   \n",
      "1       30           NaN    Oberwallstraße                  Q91203   \n",
      "2       30           NaN  Niederwallstraße                Q1398534   \n",
      "3       30     DE:zone30        Wallstraße                  Q91203   \n",
      "4       30           NaN        Wallstraße                  Q91203   \n",
      "\n",
      "   name:etymology:wikipedia  ... parking:both:orientation sidewalk:left  \\\n",
      "0  de:Schanze (Festungsbau)  ...                      NaN           NaN   \n",
      "1  de:Schanze (Festungsbau)  ...                      NaN           NaN   \n",
      "2           de:Fausse-Braie  ...                      NaN           NaN   \n",
      "3  de:Schanze (Festungsbau)  ...                 parallel      separate   \n",
      "4  de:Schanze (Festungsbau)  ...                 parallel           yes   \n",
      "\n",
      "  sidewalk:right parking:left:orientation parking:left:surface maxheight  \\\n",
      "0            NaN                      NaN                  NaN       NaN   \n",
      "1            NaN                      NaN                  NaN       NaN   \n",
      "2            NaN                      NaN                  NaN       NaN   \n",
      "3            yes                      NaN                  NaN       NaN   \n",
      "4       separate                      NaN                  NaN       NaN   \n",
      "\n",
      "  tunnel sidewalk:left:surface sidewalk:right:surface horse  \n",
      "0    NaN                   NaN                    NaN   NaN  \n",
      "1    NaN                   NaN                    NaN   NaN  \n",
      "2    NaN                   NaN                    NaN   NaN  \n",
      "3    NaN                   NaN                    NaN   NaN  \n",
      "4    NaN                   NaN                    NaN   NaN  \n",
      "\n",
      "[5 rows x 47 columns]\n",
      "\n",
      "Columns in GeoDataFrame:\n",
      "['cycleway:both', 'highway', 'image', 'lane_markings', 'lit', 'maxspeed', 'maxspeed:type', 'name', 'name:etymology:wikidata', 'name:etymology:wikipedia', 'oneway:bicycle', 'parking:both', 'postal_code', 'sidewalk', 'sidewalk:both:surface', 'smoothness', 'source:maxspeed', 'surface', 'traffic_sign', 'width', 'wikidata', 'wikimedia_commons', 'zone:maxspeed', 'id', 'type', 'geometry', 'bicycle', 'bicycle_road', 'oneway', 'parking:left', 'parking:right', 'parking:right:orientation', 'priority_road', 'vehicle', 'lanes', 'foot', 'motor_vehicle', 'parking:both:orientation', 'sidewalk:left', 'sidewalk:right', 'parking:left:orientation', 'parking:left:surface', 'maxheight', 'tunnel', 'sidewalk:left:surface', 'sidewalk:right:surface', 'horse']\n",
      "\n",
      "CRS Information:\n",
      "EPSG:25833\n"
     ]
    }
   ],
   "source": [
    "# Step 1: var 2 Request data from overpass API (Multiple Street names, Wallstraßen)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "import urllib.parse\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    response = requests.get(overpass_url, params={'data': query})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        for node_id in way['nodes']:\n",
    "            if node_id in nodes:\n",
    "                point = nodes[node_id]\n",
    "                coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Your query\n",
    "query = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "// Get Berlin using its relation ID\n",
    "area(3600062422)->.berlin;  // 62422 is Berlin's relation ID\n",
    "// Get multiple streets within Berlin area and their nodes\n",
    "(\n",
    "  // Remove [\"bicycle_road\"=\"yes\"] at Wallstraße to include street infront of Märkisches Museum (no bicycle road)\n",
    "  way[\"name\"=\"Wallstraße\"][\"bicycle_road\"=\"yes\"](area.berlin);\n",
    "  way[\"name\"=\"Oberwallstraße\"](area.berlin);\n",
    "  way[\"name\"=\"Niederwallstraße\"](area.berlin);\n",
    "  way[\"name\"=\"Märkisches Ufer\"][\"bicycle_road\"=\"yes\"](area.berlin);\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"kaiserdamm.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"street\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cb5fa0-8f09-4e52-98dd-b48fefb81e4b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame Head:\n",
      "      bicycle bicycle_road          foot      highway lane_markings  lit  \\\n",
      "0  designated          yes  use_sidepath  residential            no  yes   \n",
      "1  designated          yes  use_sidepath  residential            no  yes   \n",
      "2  designated          yes  use_sidepath  residential            no  yes   \n",
      "3  designated          yes  use_sidepath  residential           NaN  yes   \n",
      "4  designated          yes  use_sidepath  residential            no  yes   \n",
      "\n",
      "  maxspeed                                   name name:etymology:wikidata  \\\n",
      "0       30                         Handjerystraße                Q1986912   \n",
      "1       30                         Handjerystraße                Q1986912   \n",
      "2       30                         Handjerystraße                Q1986912   \n",
      "3       30  Handjerystraße / Renée-Sintenis-Platz                     NaN   \n",
      "4       30                         Handjerystraße                Q1986912   \n",
      "\n",
      "  parking:left  ... motor_vehicle             name:left  \\\n",
      "0           no  ...           NaN                   NaN   \n",
      "1         lane  ...           NaN                   NaN   \n",
      "2     separate  ...           NaN                   NaN   \n",
      "3           no  ...   destination  Renée-Sintenis-Platz   \n",
      "4         lane  ...           NaN                   NaN   \n",
      "\n",
      "  name:left:etymology:wikidata      name:right name:right:etymology:wikidata  \\\n",
      "0                          NaN             NaN                           NaN   \n",
      "1                          NaN             NaN                           NaN   \n",
      "2                          NaN             NaN                           NaN   \n",
      "3                       Q64341  Handjerystraße                      Q1986912   \n",
      "4                          NaN             NaN                           NaN   \n",
      "\n",
      "  parking:right:orientation sidewalk:left sidewalk:right  parking:both  \\\n",
      "0                       NaN           NaN            NaN           NaN   \n",
      "1                       NaN           NaN            NaN           NaN   \n",
      "2                       NaN           NaN            NaN           NaN   \n",
      "3                  parallel            no       separate           NaN   \n",
      "4                       NaN           NaN            NaN           NaN   \n",
      "\n",
      "  sidewalk:both  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2           NaN  \n",
      "3           NaN  \n",
      "4           NaN  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "Columns in GeoDataFrame:\n",
      "['bicycle', 'bicycle_road', 'foot', 'highway', 'lane_markings', 'lit', 'maxspeed', 'name', 'name:etymology:wikidata', 'parking:left', 'parking:right', 'sidewalk', 'source:maxspeed', 'source:width', 'surface', 'traffic_sign', 'vehicle', 'wikidata', 'id', 'type', 'geometry', 'cycleway:both', 'parking:left:orientation', 'smoothness', 'width', 'oneway', 'oneway:bicycle', 'junction', 'motor_vehicle', 'name:left', 'name:left:etymology:wikidata', 'name:right', 'name:right:etymology:wikidata', 'parking:right:orientation', 'sidewalk:left', 'sidewalk:right', 'parking:both', 'sidewalk:both']\n",
      "\n",
      "CRS Information:\n",
      "EPSG:25833\n"
     ]
    }
   ],
   "source": [
    "# Step 1: var 3 Request data from overpass API (Multiple Street names, Handjerystraße)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "import urllib.parse\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    response = requests.get(overpass_url, params={'data': query})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        for node_id in way['nodes']:\n",
    "            if node_id in nodes:\n",
    "                point = nodes[node_id]\n",
    "                coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Your query\n",
    "query = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "// Define the Berlin area\n",
    "area(3600062422)->.berlin;\n",
    "\n",
    "// Find the Tempelhof-Schöneberg administrative area\n",
    "rel[\"admin_level\"=\"9\"][\"name\"=\"Tempelhof-Schöneberg\"](area.berlin)->.tempelhof;\n",
    "\n",
    "// Convert the relation to an area\n",
    ".tempelhof map_to_area -> .tempelhof_area;\n",
    "\n",
    "// Get the streets within Tempelhof-Schöneberg\n",
    "// Remove [\"bicycle_road\"=\"yes\"] to include Kreuzung Schmiljanstraße (No Cycling Street)\n",
    "(\n",
    "  way(area.tempelhof_area)[\"name\"=\"Handjerystraße\"][\"bicycle_road\"=\"yes\"];\n",
    "  // Optional: Renée-Sintenis-Platz\n",
    "  way(area.tempelhof_area)[\"name:left\"=\"Renée-Sintenis-Platz\"][\"bicycle_road\"=\"yes\"];\n",
    "  way(area.tempelhof_area)[\"name:right\"=\"Handjerystraße\"][\"bicycle_road\"=\"yes\"];\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"kaiserdamm.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"street\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1678ab7f-981c-4c23-aaa7-e9ee9be59961",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame Head:\n",
      "      bicycle bicycle_road cycleway:both          foot      highway  \\\n",
      "0  designated          yes            no  use_sidepath  residential   \n",
      "1  designated          yes            no  use_sidepath  residential   \n",
      "2  designated          yes            no  use_sidepath  residential   \n",
      "3  designated          yes            no  use_sidepath  residential   \n",
      "4  designated          yes            no  use_sidepath  residential   \n",
      "\n",
      "                                               image lane_markings  lit  \\\n",
      "0  https://commons.wikimedia.org/wiki/File:Berlin...            no  yes   \n",
      "1  https://commons.wikimedia.org/wiki/File:Berlin...            no  yes   \n",
      "2  https://commons.wikimedia.org/wiki/File:Berlin...            no  yes   \n",
      "3  https://commons.wikimedia.org/wiki/File:Berlin...            no  yes   \n",
      "4  https://commons.wikimedia.org/wiki/File:Berlin...            no  yes   \n",
      "\n",
      "  maxspeed                  name  ... parking:both:reason sidewalk:both  \\\n",
      "0       30  Stallschreiberstraße  ...                 NaN           NaN   \n",
      "1       30  Stallschreiberstraße  ...              narrow      separate   \n",
      "2       30  Stallschreiberstraße  ...              narrow      separate   \n",
      "3       30  Stallschreiberstraße  ...                 NaN      separate   \n",
      "4       30  Stallschreiberstraße  ...              narrow      separate   \n",
      "\n",
      "  sidewalk:both:surface parking:left parking:left:fee  \\\n",
      "0                   NaN          NaN              NaN   \n",
      "1         paving_stones          NaN              NaN   \n",
      "2                   NaN          NaN              NaN   \n",
      "3                   NaN         lane               no   \n",
      "4                   NaN          NaN              NaN   \n",
      "\n",
      "  parking:left:orientation parking:right parking:right:reason  \\\n",
      "0                      NaN           NaN                  NaN   \n",
      "1                      NaN           NaN                  NaN   \n",
      "2                      NaN           NaN                  NaN   \n",
      "3                 parallel            no               narrow   \n",
      "4                      NaN           NaN                  NaN   \n",
      "\n",
      "  parking:left:reason parking:right:fee  \n",
      "0                 NaN               NaN  \n",
      "1                 NaN               NaN  \n",
      "2                 NaN               NaN  \n",
      "3                 NaN               NaN  \n",
      "4                 NaN               NaN  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "Columns in GeoDataFrame:\n",
      "['bicycle', 'bicycle_road', 'cycleway:both', 'foot', 'highway', 'image', 'lane_markings', 'lit', 'maxspeed', 'name', 'name:etymology:wikidata', 'name:etymology:wikipedia', 'oneway', 'oneway:bicycle', 'parking:both', 'parking:both:fee', 'postal_code', 'sidewalk', 'smoothness', 'source:maxspeed', 'source:width', 'surface', 'traffic_sign', 'vehicle', 'width', 'wikidata', 'wikimedia_commons', 'id', 'type', 'geometry', 'parking:both:reason', 'sidewalk:both', 'sidewalk:both:surface', 'parking:left', 'parking:left:fee', 'parking:left:orientation', 'parking:right', 'parking:right:reason', 'parking:left:reason', 'parking:right:fee']\n",
      "\n",
      "CRS Information:\n",
      "EPSG:25833\n"
     ]
    }
   ],
   "source": [
    "# Step 1: var 4 Request data from overpass API (Multiple Street names, Stallschreiberstraße)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "import urllib.parse\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    response = requests.get(overpass_url, params={'data': query})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        for node_id in way['nodes']:\n",
    "            if node_id in nodes:\n",
    "                point = nodes[node_id]\n",
    "                coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Your query\n",
    "query = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "// Get Berlin using its relation ID\n",
    "area(3600062422)->.berlin;  // 62422 is Berlin's relation ID\n",
    "// Get multiple streets within Berlin area and their nodes\n",
    "(\n",
    "  way[\"name\"=\"Stallschreiberstraße\"][\"bicycle_road\"=\"yes\"](area.berlin);\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"kaiserdamm.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"street\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5846cdd-c13a-4935-b0c6-8e790f1185c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame Head:\n",
      "      bicycle bicycle_road cycleway:both      highway lane_markings  lit  \\\n",
      "0  designated          yes            no  residential            no  yes   \n",
      "1  designated          yes           NaN      service           NaN  yes   \n",
      "2  designated          yes            no  residential            no  yes   \n",
      "3  designated          yes            no  residential            no   no   \n",
      "4  designated          yes            no  residential            no   no   \n",
      "\n",
      "  maxspeed         name name:etymology:wikidata name:etymology:wikipedia  ...  \\\n",
      "0       30  Hönower Weg                 Q176778                 de:Hönow  ...   \n",
      "1       30  Hönower Weg                     NaN                      NaN  ...   \n",
      "2       30  Hönower Weg                 Q176778                 de:Hönow  ...   \n",
      "3       30  Hönower Weg                 Q176778                 de:Hönow  ...   \n",
      "4       30  Hönower Weg                 Q176778                 de:Hönow  ...   \n",
      "\n",
      "                                            geometry foot lanes oneway  \\\n",
      "0  LINESTRING (398775.994 5816741.094, 398782.901...  NaN   NaN    NaN   \n",
      "1  LINESTRING (398302.837 5816922.042, 398372.25 ...  yes     1     no   \n",
      "2  LINESTRING (399244.866 5816832.947, 399255.617...  NaN   NaN    NaN   \n",
      "3  LINESTRING (399064.611 5816764.252, 399106.526...  NaN   NaN    NaN   \n",
      "4  LINESTRING (398961.033 5816738.112, 399012.543...  NaN   NaN    NaN   \n",
      "\n",
      "  parking:both  source:maxspeed sidewalk:right:surface parking:right:fee  \\\n",
      "0          NaN              NaN                    NaN               NaN   \n",
      "1           no  DE:bicycle_road                    NaN               NaN   \n",
      "2           no              NaN                asphalt               NaN   \n",
      "3          NaN              NaN                    NaN               NaN   \n",
      "4          NaN              NaN                    NaN               NaN   \n",
      "\n",
      "  parking:right:orientation parking:both:fee  \n",
      "0                       NaN              NaN  \n",
      "1                       NaN              NaN  \n",
      "2                       NaN              NaN  \n",
      "3                       NaN              NaN  \n",
      "4                       NaN              NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "Columns in GeoDataFrame:\n",
      "['bicycle', 'bicycle_road', 'cycleway:both', 'highway', 'lane_markings', 'lit', 'maxspeed', 'name', 'name:etymology:wikidata', 'name:etymology:wikipedia', 'parking:left', 'parking:left:fee', 'parking:left:orientation', 'parking:right', 'postal_code', 'sidewalk', 'smoothness', 'surface', 'traffic_sign', 'vehicle', 'width', 'id', 'type', 'geometry', 'foot', 'lanes', 'oneway', 'parking:both', 'source:maxspeed', 'sidewalk:right:surface', 'parking:right:fee', 'parking:right:orientation', 'parking:both:fee']\n",
      "\n",
      "CRS Information:\n",
      "EPSG:25833\n"
     ]
    }
   ],
   "source": [
    "# Step 1: var 5 Request data from overpass API (Multiple Street names, Hönower Weg)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "import urllib.parse\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    response = requests.get(overpass_url, params={'data': query})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        for node_id in way['nodes']:\n",
    "            if node_id in nodes:\n",
    "                point = nodes[node_id]\n",
    "                coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Your query\n",
    "query = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "// Get Berlin using its relation ID\n",
    "area(3600062422)->.berlin;  // 62422 is Berlin's relation ID\n",
    "// Get multiple streets within Berlin area and their nodes\n",
    "(\n",
    "  // Remove [\"bicycle_road\"=\"yes\"] to include Road infront of Parkplatz Dolgenseestraße (No bicycle road)\n",
    "  way[\"name\"=\"Hönower Weg\"][\"bicycle_road\"=\"yes\"](area.berlin);\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"kaiserdamm.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"street\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdb407a-20d9-43a6-9ef9-74e5f2b89ca3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 158 features\n",
      "\n",
      "GeoDataFrame Head:\n",
      "  cycleway:left cycleway:right cycleway:right:bicycle cycleway:right:lane  \\\n",
      "0            no           lane             designated           exclusive   \n",
      "1            no           lane                    NaN           exclusive   \n",
      "2            no           lane             designated           exclusive   \n",
      "3            no           lane             designated           exclusive   \n",
      "4            no           lane                    NaN           exclusive   \n",
      "\n",
      "  cycleway:right:separation:left cycleway:right:width          foot  \\\n",
      "0                           bump                  2.8  use_sidepath   \n",
      "1                           bump                  NaN  use_sidepath   \n",
      "2           bollard;parking_lane                    2  use_sidepath   \n",
      "3                        bollard                    2  use_sidepath   \n",
      "4            bump;vertical_panel                  2.4  use_sidepath   \n",
      "\n",
      "     highway lanes  lit  ... buffer:both lanes:bus class:bicycle  \\\n",
      "0    primary     2  yes  ...         NaN       NaN           NaN   \n",
      "1  secondary     2  yes  ...         NaN       NaN           NaN   \n",
      "2    primary     2  yes  ...         NaN       NaN           NaN   \n",
      "3  secondary     2  yes  ...         NaN       NaN           NaN   \n",
      "4  secondary     2  yes  ...         NaN       NaN           NaN   \n",
      "\n",
      "  motor_vehicle traffic_sign:backward traffic_sign:forward button_operated  \\\n",
      "0           NaN                   NaN                  NaN             NaN   \n",
      "1           NaN                   NaN                  NaN             NaN   \n",
      "2           NaN                   NaN                  NaN             NaN   \n",
      "3           NaN                   NaN                  NaN             NaN   \n",
      "4           NaN                   NaN                  NaN             NaN   \n",
      "\n",
      "  crossing crossing:markings cycleway  \n",
      "0      NaN               NaN      NaN  \n",
      "1      NaN               NaN      NaN  \n",
      "2      NaN               NaN      NaN  \n",
      "3      NaN               NaN      NaN  \n",
      "4      NaN               NaN      NaN  \n",
      "\n",
      "[5 rows x 146 columns]\n",
      "\n",
      "Columns in GeoDataFrame:\n",
      "['cycleway:left', 'cycleway:right', 'cycleway:right:bicycle', 'cycleway:right:lane', 'cycleway:right:separation:left', 'cycleway:right:width', 'foot', 'highway', 'lanes', 'lit', 'maxheight', 'maxspeed', 'name', 'name:etymology:wikidata', 'oneway', 'parking:both', 'postal_code', 'ref', 'sidewalk:left', 'sidewalk:right', 'smoothness', 'surface', 'wikidata', 'wikipedia', 'zone:traffic', 'id', 'type', 'geometry', 'dual_carriageway', 'name:etymology:wikipedia', 'cycleway:right:buffer:left', 'cycleway:right:surface:colour', 'cycleway:right:traffic_sign', 'lane_markings', 'old_name:1822-1874', 'old_name:1874-1950', 'old_name:1950-1995', 'cycleway:right:separation:right', 'note', 'parking:left:reason', 'parking:right:reason', 'destination', 'parking:right:restriction', 'destination:colour', 'maxspeed:type', 'source:maxspeed', 'bridge', 'bridge:name', 'bridge:wikipedia', 'layer', 'maxweight:signed', 'hazard', 'image', 'wikimedia_commons', 'cycleway:both:traffic_sign', 'cycleway:left:width', 'sidewalk:both', 'lanes:backward', 'lanes:forward', 'parking:left:restriction', 'sidewalk:both:surface', 'cycleway:both', 'cycleway:both:lane', 'cycleway:both:surface:colour', 'cycleway:both:width', 'cycleway:left:buffer:left', 'cycleway:right:traffic_mode:left', 'parking:left', 'parking:right', 'parking:right:fee', 'parking:right:orientation', 'parking:right:restriction:conditional', 'placement:forward', 'source:width', 'turn:lanes:backward', 'width', 'width:lanes:backward', 'width:lanes:forward', 'parking:both:reason', 'cycleway:left:buffer:right', 'cycleway:left:lane', 'cycleway:left:marking:both', 'motor_vehicle:conditional', 'turn:lanes:forward', 'sidewalk', 'cycleway:left:traffic_mode:left', 'parking:left:orientation', 'cycleway:both:buffer:right', 'cycleway:left:separation:left', 'cycleway:left:traffic_mode:right', 'parking:left:fee', 'parking:left:restriction:conditional', 'cycleway:both:buffer:left', 'parking:left:fee:conditional', 'parking:left:zone', 'placement', 'destination:arrow:lanes', 'destination:arrow:to:lanes', 'destination:colour:lanes', 'destination:lanes', 'destination:ref:lanes', 'destination:ref:to:lanes', 'destination:symbol:lanes', 'destination:symbol:to:lanes', 'destination:to:lanes', 'sidewalk:width', 'turn:lanes', 'oneway:bicycle', 'sidewalk:left:surface', 'sidewalk:right:surface', 'traffic_sign', 'zone:maxspeed', 'bicycle:lanes', 'cycleway:lanes', 'vehicle:lanes', 'description', 'incline', 'buffer:left', 'is_sidepath', 'is_sidepath:of', 'is_sidepath:of:name', 'separation:left', 'separation:right', 'start_date', 'surface:colour', 'traffic_mode:left', 'bicycle', 'marking:left', 'buffer:right', 'marking:right', 'shoulder', 'width:lanes', 'width:lanes:end', 'width:lanes:start', 'priority_road', 'check_date:smoothness', 'buffer:both', 'lanes:bus', 'class:bicycle', 'motor_vehicle', 'traffic_sign:backward', 'traffic_sign:forward', 'button_operated', 'crossing', 'crossing:markings', 'cycleway']\n",
      "\n",
      "CRS Information:\n",
      "EPSG:25833\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: var 6 Request data from overpass API (custom, geschützte Radfahrstreifen)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Use POST method with the query directly in the data parameter\n",
    "    response = requests.post(overpass_url, data=query)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}, Response: {response.text[:200]}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        # Handle ways with embedded geometry\n",
    "        if 'geometry' in way:\n",
    "            for node in way['geometry']:\n",
    "                coords.append((node['lon'], node['lat']))\n",
    "        # Handle ways with node references\n",
    "        elif 'nodes' in way:\n",
    "            for node_id in way['nodes']:\n",
    "                if node_id in nodes:\n",
    "                    point = nodes[node_id]\n",
    "                    coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Modified query using Berlin's relation ID directly\n",
    "query = \"\"\"\n",
    "[out:json][timeout:100];\n",
    "// Get Berlin area by its relation ID (62422 is Berlin's OSM relation ID)\n",
    "area(3600062422)->.searchArea;\n",
    "(\n",
    "  // Roads with cycleway:right:separation:left = bump or bollard\n",
    "  way[\"highway\"]\n",
    "      [\"cycleway:right:separation:left\"~\"bump|bollard\"]\n",
    "      (area.searchArea);\n",
    "  // Cycleways with separation:left = bollard\n",
    "  way[\"highway\"=\"cycleway\"]\n",
    "      [\"separation:left\"~\"bump|bollard\"]\n",
    "      (area.searchArea);\n",
    ");\n",
    "// Output results as individual road segments\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "# Alternative query if you prefer to use name-based area search\n",
    "alternative_query = \"\"\"\n",
    "[out:json][timeout:100];\n",
    "// Define Berlin's boundary\n",
    "rel[\"name\"=\"Berlin\"][\"admin_level\"=\"4\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.searchArea;\n",
    "(\n",
    "  // Roads with cycleway:right:separation:left = bump or bollard\n",
    "  way[\"highway\"]\n",
    "      [\"cycleway:right:separation:left\"~\"bump|bollard\"]\n",
    "      (area.searchArea);\n",
    "  // Cycleways with separation:left = bollard\n",
    "  way[\"highway\"=\"cycleway\"]\n",
    "      [\"separation:left\"~\"bump|bollard\"]\n",
    "      (area.searchArea);\n",
    ");\n",
    "// Output results as individual road segments\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Use the first query with relation ID (more reliable)\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(f\"\\nFound {len(gdf)} features\")\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"protected_bike_lanes.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"protected_bike_lanes\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # If the first query fails, try the alternative\n",
    "    print(\"\\nFirst query failed. Trying alternative query...\")\n",
    "    try:\n",
    "        osm_data = get_osm_data(alternative_query)\n",
    "        gdf = process_osm_to_gdf(osm_data)\n",
    "        \n",
    "        print(f\"\\nFound {len(gdf)} features with alternative query\")\n",
    "        \n",
    "        print(\"\\nGeoDataFrame Head:\")\n",
    "        print(gdf.head())\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Alternative query also failed: {str(e2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe94148-4aef-4a73-9b2a-a1782e4e86e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 1.1: var 7 Request data from overpass API (Fahrradstraßen)\n",
    "# Adjust Overpass API query and test in Overpass Turbo before launching\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def get_osm_data(query):\n",
    "    \"\"\"\n",
    "    Send query to Overpass API and return JSON response\n",
    "    \"\"\"\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Use POST method with the query directly in the data parameter\n",
    "    response = requests.post(overpass_url, data=query)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}, Response: {response.text[:200]}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def process_osm_to_gdf(data):\n",
    "    \"\"\"\n",
    "    Convert OSM JSON data to GeoDataFrame using efficient methods\n",
    "    \"\"\"\n",
    "    # Extract nodes and ways\n",
    "    nodes = {\n",
    "        element['id']: Point(element['lon'], element['lat'])\n",
    "        for element in data['elements']\n",
    "        if element['type'] == 'node'\n",
    "    }\n",
    "    \n",
    "    ways = [\n",
    "        element for element in data['elements']\n",
    "        if element['type'] == 'way'\n",
    "    ]\n",
    "    \n",
    "    # Create features list with proper structure\n",
    "    features = []\n",
    "    \n",
    "    # Process each way\n",
    "    for way in ways:\n",
    "        coords = []\n",
    "        # Handle ways with embedded geometry\n",
    "        if 'geometry' in way:\n",
    "            for node in way['geometry']:\n",
    "                coords.append((node['lon'], node['lat']))\n",
    "        # Handle ways with node references\n",
    "        elif 'nodes' in way:\n",
    "            for node_id in way['nodes']:\n",
    "                if node_id in nodes:\n",
    "                    point = nodes[node_id]\n",
    "                    coords.append((point.x, point.y))\n",
    "        \n",
    "        if coords:\n",
    "            line = LineString(coords)\n",
    "            # Create a dictionary with all properties and geometry\n",
    "            feature = way.get('tags', {}).copy()  # Start with tags\n",
    "            feature['id'] = way['id']\n",
    "            feature['type'] = 'way'\n",
    "            feature['geometry'] = line\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Create DataFrame first\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found in the OSM data\")\n",
    "        \n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        features,\n",
    "        geometry='geometry',\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Transform to EPSG:25833\n",
    "    gdf = gdf.to_crs(epsg=25833)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Modified query using Berlin's relation ID directly\n",
    "query = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "area[name=\"Berlin\"]->.searchArea;\n",
    "(\n",
    "  \n",
    "  // Straßen, die als Wohnstraße markiert sind und für Fahrräder ausgewiesen\n",
    "  way[\"highway\"=\"residential\"][\"bicycle\"~\"designated|official\"](area.searchArea);\n",
    "  \n",
    "  // Straßen, die möglicherweise eine Verkehrsberuhigung für Fahrräder haben\n",
    "  way[\"bicycle_road\"=\"yes\"](area.searchArea);\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out skel qt;\n",
    "\"\"\"\n",
    "\n",
    "# Alternative query if you prefer to use name-based area search\n",
    "alternative_query = \"\"\"\n",
    "[out:json][timeout:100];\n",
    "// Define Berlin's boundary\n",
    "rel[\"name\"=\"Berlin\"][\"admin_level\"=\"4\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.searchArea;\n",
    "(\n",
    "  // Roads with cycleway:right:separation:left = bump or bollard\n",
    "  way[\"highway\"]\n",
    "      [\"cycleway:right:separation:left\"~\"bump|bollard\"]\n",
    "      (area.searchArea);\n",
    "  // Cycleways with separation:left = bollard\n",
    "  way[\"highway\"=\"cycleway\"]\n",
    "      [\"separation:left\"~\"bump|bollard\"]\n",
    "      (area.searchArea);\n",
    ");\n",
    "// Output results as individual road segments\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Use the first query with relation ID (more reliable)\n",
    "    osm_data = get_osm_data(query)\n",
    "    gdf = process_osm_to_gdf(osm_data)\n",
    "    \n",
    "    print(f\"\\nFound {len(gdf)} features\")\n",
    "    \n",
    "    print(\"\\nGeoDataFrame Head:\")\n",
    "    print(gdf.head())\n",
    "    \n",
    "    print(\"\\nColumns in GeoDataFrame:\")\n",
    "    print(gdf.columns.tolist())\n",
    "    \n",
    "    print(\"\\nCRS Information:\")\n",
    "    print(gdf.crs)\n",
    "    \n",
    "    # Optional: Save to file\n",
    "    # pyogrio.write_dataframe(\n",
    "    #     gdf,\n",
    "    #     \"protected_bike_lanes.gpkg\",\n",
    "    #     driver=\"GPKG\",\n",
    "    #     layer=\"protected_bike_lanes\"\n",
    "    # )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # If the first query fails, try the alternative\n",
    "    print(\"\\nFirst query failed. Trying alternative query...\")\n",
    "    try:\n",
    "        osm_data = get_osm_data(alternative_query)\n",
    "        gdf = process_osm_to_gdf(osm_data)\n",
    "        \n",
    "        print(f\"\\nFound {len(gdf)} features with alternative query\")\n",
    "        \n",
    "        print(\"\\nGeoDataFrame Head:\")\n",
    "        print(gdf.head())\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Alternative query also failed: {str(e2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc22c39-0a2b-4cde-a98e-b0873e5ccb71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_8161f1e579d46844b8f319590b8f6f30 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "    \n",
       "                    &lt;style&gt;\n",
       "                        .foliumtooltip {\n",
       "                            \n",
       "                        }\n",
       "                       .foliumtooltip table{\n",
       "                            margin: auto;\n",
       "                        }\n",
       "                        .foliumtooltip tr{\n",
       "                            text-align: left;\n",
       "                        }\n",
       "                        .foliumtooltip th{\n",
       "                            padding: 2px; padding-right: 8px;\n",
       "                        }\n",
       "                    &lt;/style&gt;\n",
       "            \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_8161f1e579d46844b8f319590b8f6f30&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_8161f1e579d46844b8f319590b8f6f30 = L.map(\n",
       "                &quot;map_8161f1e579d46844b8f319590b8f6f30&quot;,\n",
       "                {\n",
       "                    center: [52.49235639999999, 13.50923665],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    ...{\n",
       "  &quot;zoom&quot;: 10,\n",
       "  &quot;zoomControl&quot;: true,\n",
       "  &quot;preferCanvas&quot;: false,\n",
       "}\n",
       "\n",
       "                }\n",
       "            );\n",
       "            L.control.scale().addTo(map_8161f1e579d46844b8f319590b8f6f30);\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_1a3145e0614a4e76623b1625ce834327 = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {\n",
       "  &quot;minZoom&quot;: 0,\n",
       "  &quot;maxZoom&quot;: 19,\n",
       "  &quot;maxNativeZoom&quot;: 19,\n",
       "  &quot;noWrap&quot;: false,\n",
       "  &quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;,\n",
       "  &quot;subdomains&quot;: &quot;abc&quot;,\n",
       "  &quot;detectRetina&quot;: false,\n",
       "  &quot;tms&quot;: false,\n",
       "  &quot;opacity&quot;: 1,\n",
       "}\n",
       "\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_1a3145e0614a4e76623b1625ce834327.addTo(map_8161f1e579d46844b8f319590b8f6f30);\n",
       "        \n",
       "    \n",
       "            map_8161f1e579d46844b8f319590b8f6f30.fitBounds(\n",
       "                [[52.4912712, 13.4993809], [52.49344159999998, 13.5190924]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "        function geo_json_936127907cf76aa613074b1cd0b449fa_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.5, &quot;weight&quot;: 2};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_936127907cf76aa613074b1cd0b449fa_highlighter(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.75};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_936127907cf76aa613074b1cd0b449fa_pointToLayer(feature, latlng) {\n",
       "            var opts = {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;#3388ff&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;#3388ff&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;opacity&quot;: 1.0, &quot;radius&quot;: 2, &quot;stroke&quot;: true, &quot;weight&quot;: 3};\n",
       "            \n",
       "            let style = geo_json_936127907cf76aa613074b1cd0b449fa_styler(feature)\n",
       "            Object.assign(opts, style)\n",
       "            \n",
       "            return new L.CircleMarker(latlng, opts)\n",
       "        }\n",
       "\n",
       "        function geo_json_936127907cf76aa613074b1cd0b449fa_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "                mouseout: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                            geo_json_936127907cf76aa613074b1cd0b449fa.resetStyle(e.target);\n",
       "                    }\n",
       "                },\n",
       "                mouseover: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        const highlightStyle = geo_json_936127907cf76aa613074b1cd0b449fa_highlighter(e.target.feature)\n",
       "                        e.target.setStyle(highlightStyle);\n",
       "                    }\n",
       "                },\n",
       "            });\n",
       "        };\n",
       "        var geo_json_936127907cf76aa613074b1cd0b449fa = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_936127907cf76aa613074b1cd0b449fa_onEachFeature,\n",
       "            \n",
       "                style: geo_json_936127907cf76aa613074b1cd0b449fa_styler,\n",
       "                pointToLayer: geo_json_936127907cf76aa613074b1cd0b449fa_pointToLayer,\n",
       "            ...{\n",
       "}\n",
       "        });\n",
       "\n",
       "        function geo_json_936127907cf76aa613074b1cd0b449fa_add (data) {\n",
       "            geo_json_936127907cf76aa613074b1cd0b449fa\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_936127907cf76aa613074b1cd0b449fa_add({&quot;bbox&quot;: [13.4993809, 52.4912712, 13.5190924, 52.49344159999998], &quot;features&quot;: [{&quot;bbox&quot;: [13.5090948, 52.4912712, 13.5113806, 52.4914078], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5090948, 52.4914078], [13.5091972, 52.491388199999996], [13.5094505, 52.49133949999998], [13.5094663, 52.4913365], [13.509984, 52.4912712], [13.5102079, 52.49127299999999], [13.510662100000001, 52.4912903], [13.5110865, 52.49132619999999], [13.5113806, 52.49135899999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {&quot;id&quot;: 31525840}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.5020731, 52.4920551, 13.5061047, 52.492945999999996], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5020731, 52.492945999999996], [13.5031038, 52.4927033], [13.50337, 52.492640599999994], [13.5047173, 52.49235410000001], [13.5061047, 52.4920551]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;1&quot;, &quot;properties&quot;: {&quot;id&quot;: 71282548}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.5159704, 52.4923201, 13.5162459, 52.49240739999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5159704, 52.4923201], [13.5161268, 52.4923782], [13.5162459, 52.49240739999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;2&quot;, &quot;properties&quot;: {&quot;id&quot;: 164927492}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.513337200000002, 52.49166939999999, 13.5159704, 52.4923201], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.513337200000002, 52.49166939999999], [13.51395, 52.49180559999999], [13.5144695, 52.4919262], [13.5157929, 52.4922541], [13.5159704, 52.4923201]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;3&quot;, &quot;properties&quot;: {&quot;id&quot;: 520777778}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.5118201, 52.49141530000001, 13.513337200000002, 52.49166939999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5118201, 52.49141530000001], [13.512574900000002, 52.4915317], [13.513337200000002, 52.49166939999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;4&quot;, &quot;properties&quot;: {&quot;id&quot;: 630214733}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.5162459, 52.49240739999999, 13.516702300000002, 52.492521099999976], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5162459, 52.49240739999999], [13.5164181, 52.4924496], [13.516702300000002, 52.492521099999976]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;5&quot;, &quot;properties&quot;: {&quot;id&quot;: 630214736}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.516702300000002, 52.492521099999976, 13.517820300000002, 52.4927927], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.516702300000002, 52.492521099999976], [13.517820300000002, 52.4927927]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;6&quot;, &quot;properties&quot;: {&quot;id&quot;: 630214740}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.517820300000002, 52.4927927, 13.5190924, 52.493104], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.517820300000002, 52.4927927], [13.5182184, 52.49289009999999], [13.5190924, 52.493104]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;7&quot;, &quot;properties&quot;: {&quot;id&quot;: 630214749}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.5074115, 52.4914078, 13.5090948, 52.49180059999998], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5074115, 52.49180059999998], [13.507774800000002, 52.491715799999994], [13.5079284, 52.49168000000001], [13.508183600000002, 52.4916204], [13.508441900000001, 52.491560199999995], [13.5086856, 52.49150329999999], [13.5089408, 52.49144369999999], [13.5090948, 52.4914078]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;8&quot;, &quot;properties&quot;: {&quot;id&quot;: 630217174}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.5113806, 52.49135899999999, 13.5118201, 52.49141530000001], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.5113806, 52.49135899999999], [13.5118201, 52.49141530000001]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;9&quot;, &quot;properties&quot;: {&quot;id&quot;: 630217175}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.4993809, 52.492945999999996, 13.5020731, 52.49344159999998], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.4993809, 52.493391599999995], [13.4995141, 52.49341340000001], [13.4996388, 52.49342870000001], [13.499737, 52.49343720000001], [13.499833, 52.49344159999998], [13.4999138, 52.49344049999999], [13.4999946, 52.493435799999986], [13.5000538, 52.493428699999996], [13.500133500000002, 52.4934132], [13.5020731, 52.492945999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;10&quot;, &quot;properties&quot;: {&quot;id&quot;: 1093614701}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "    geo_json_936127907cf76aa613074b1cd0b449fa.bindTooltip(\n",
       "    function(layer){\n",
       "    let div = L.DomUtil.create(&#x27;div&#x27;);\n",
       "    \n",
       "    let handleObject = feature=&gt;typeof(feature)==&#x27;object&#x27; ? JSON.stringify(feature) : feature;\n",
       "    let fields = [&quot;id&quot;];\n",
       "    let aliases = [&quot;id&quot;];\n",
       "    let table = &#x27;&lt;table&gt;&#x27; +\n",
       "        String(\n",
       "        fields.map(\n",
       "        (v,i)=&gt;\n",
       "        `&lt;tr&gt;\n",
       "            &lt;th&gt;${aliases[i]}&lt;/th&gt;\n",
       "            \n",
       "            &lt;td&gt;${handleObject(layer.feature.properties[v])}&lt;/td&gt;\n",
       "        &lt;/tr&gt;`).join(&#x27;&#x27;))\n",
       "    +&#x27;&lt;/table&gt;&#x27;;\n",
       "    div.innerHTML=table;\n",
       "    \n",
       "    return div\n",
       "    }\n",
       "    ,{\n",
       "  &quot;sticky&quot;: true,\n",
       "  &quot;className&quot;: &quot;foliumtooltip&quot;,\n",
       "});\n",
       "                     \n",
       "    \n",
       "            geo_json_936127907cf76aa613074b1cd0b449fa.addTo(map_8161f1e579d46844b8f319590b8f6f30);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f0a278985f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1: Check if necessary\n",
    "# gdf.explore()\n",
    "\n",
    "# Create a simplified GeoDataFrame with only id and geometry\n",
    "gdf_simple = gdf[['id', 'geometry']].copy()\n",
    "\n",
    "# Create the interactive map\n",
    "gdf_simple.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f453c5-1403-494b-900e-0e29dbdc1db2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet file (id, geometry_wkt fields only)...\n",
      "Successfully loaded 592136 rows\n",
      "Converting WKT to geometry objects...\n",
      "Creating spatial index...\n",
      "Finding potential intersections...\n",
      "Performing actual intersection test...\n",
      "Checking id column after spatial join...\n",
      "Renaming 'id_left' to 'id'\n",
      "Processing each reference geometry...\n",
      "Total nodes in buffer: 150try 25/25\n",
      "Parallel nodes found: 61\n",
      "\n",
      "Parallel score statistics:\n",
      "count    61.000000\n",
      "mean      0.992664\n",
      "std       0.024564\n",
      "min       0.842145\n",
      "25%       0.998488\n",
      "50%       0.999943\n",
      "75%       0.999989\n",
      "max       1.000000\n",
      "Name: parallel_score, dtype: float64\n",
      "Loading additional fields for parallel segments...\n",
      "Found 61 unique IDs for rejoining\n",
      "Reading additional fields for only 61 segments...\n",
      "Successfully loaded 61 rows with all fields\n",
      "Merging 803 additional fields: city_id, osmid, access, 2304-2412_speed, 2304-2412_route_count...\n",
      "Created gdf3 with 61 parallel segments\n"
     ]
    }
   ],
   "source": [
    "# Step 2 v2.3 final [FULL] [parquet]: Find parallel segments to the overpass data in analysis data\n",
    "# ADJUST buffer_distance and parallel_threshold at the end of the code!\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pyogrio\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely import wkt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.linear_model import LinearRegression  # For fitting a line to vertices\n",
    "from shapely.geometry import CAP_STYLE\n",
    "\n",
    "def find_parallel_segments(gdf, parquet_path, buffer_distance=15, parallel_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Find parallel segments from a parquet file with WKT geometries within a buffer of the input geometry.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame containing the reference street geometries (selection tool)\n",
    "    parquet_path : str\n",
    "        Path to the parquet file with WKT geometries (data source)\n",
    "    buffer_distance : float\n",
    "        Buffer distance in meters (default: 15)\n",
    "    parallel_threshold : float\n",
    "        Threshold for parallel score (default: 0.8)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (nodes_in_buffer, parallel_nodes)\n",
    "        Two GeoDataFrames containing all nodes in buffer and parallel nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create buffer for each reference geometry\n",
    "    gdf_buffer = gdf.copy()\n",
    "    # gdf_buffer['geometry'] = gdf.geometry.buffer(buffer_distance)\n",
    "    gdf_buffer['geometry'] = gdf.geometry.apply(lambda x: x.buffer(buffer_distance, cap_style=CAP_STYLE.flat))\n",
    "    gdf_buffer['ref_idx'] = gdf_buffer.index  # Track which reference each buffer is from\n",
    "\n",
    "    # Read the parquet file efficiently - only load id and geometry_wkt initially\n",
    "    print(\"Reading parquet file (id, geometry_wkt fields only)...\")\n",
    "    parquet_fields = ['id', 'geometry_wkt']\n",
    "    \n",
    "    # Use pyarrow to read only selected columns\n",
    "    try:\n",
    "        parquet_table = pq.read_table(parquet_path, columns=parquet_fields)\n",
    "        df_network = parquet_table.to_pandas()\n",
    "        print(f\"Successfully loaded {len(df_network)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet columns: {e}\")\n",
    "        print(\"Attempting to read all columns and then filter...\")\n",
    "        parquet_table = pq.read_table(parquet_path)\n",
    "        df_network = parquet_table.to_pandas()\n",
    "        # Keep only the columns we need\n",
    "        required_cols = ['id', 'geometry_wkt']\n",
    "        for col in required_cols:\n",
    "            if col not in df_network.columns:\n",
    "                raise ValueError(f\"Required column '{col}' not found in parquet file.\")\n",
    "        df_network = df_network[required_cols]\n",
    "        print(f\"Successfully loaded {len(df_network)} rows\")\n",
    "    \n",
    "    # Convert WKT to geometry objects\n",
    "    print(\"Converting WKT to geometry objects...\")\n",
    "    df_network['geometry'] = df_network['geometry_wkt'].apply(lambda x: wkt.loads(x) if x else None)\n",
    "    \n",
    "    # Drop rows with invalid geometries\n",
    "    df_network = df_network.dropna(subset=['geometry'])\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    gdf_network = gpd.GeoDataFrame(df_network, geometry='geometry')\n",
    "    \n",
    "    # Set CRS from the original GeoDataFrame\n",
    "    gdf_network.set_crs(gdf_buffer.crs, inplace=True)\n",
    "    \n",
    "    # Create spatial index for more efficient intersection\n",
    "    print(\"Creating spatial index...\")\n",
    "    sindex = gdf_network.sindex\n",
    "    \n",
    "    # Find potential intersections using the spatial index\n",
    "    print(\"Finding potential intersections...\")\n",
    "    potential_matches_index = []\n",
    "    \n",
    "    for geom in gdf_buffer.geometry:\n",
    "        potential_matches_index.extend(list(sindex.intersection(geom.bounds)))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    potential_matches_index = list(set(potential_matches_index))\n",
    "    \n",
    "    # Extract the candidates\n",
    "    candidates = gdf_network.iloc[potential_matches_index].copy()\n",
    "    \n",
    "    # Perform actual intersection test\n",
    "    print(\"Performing actual intersection test...\")\n",
    "    nodes_in_buffer = gpd.sjoin(\n",
    "        candidates, \n",
    "        gdf_buffer, \n",
    "        how='inner', \n",
    "        predicate='intersects'\n",
    "    )\n",
    "    \n",
    "    # Fix id column after spatial join (it becomes id_left)\n",
    "    print(\"Checking id column after spatial join...\")\n",
    "    if 'id' not in nodes_in_buffer.columns and 'id_left' in nodes_in_buffer.columns:\n",
    "        print(\"Renaming 'id_left' to 'id'\")\n",
    "        nodes_in_buffer['id'] = nodes_in_buffer['id_left']\n",
    "    \n",
    "    def calculate_direction_vector(geom):\n",
    "        \"\"\"\n",
    "        Calculate the direction vector of a line segment by fitting a line to all its vertices.\n",
    "        \"\"\"\n",
    "        if isinstance(geom, Point) or len(geom.coords) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Extract all vertices\n",
    "        x = np.array([coord[0] for coord in geom.coords])\n",
    "        y = np.array([coord[1] for coord in geom.coords])\n",
    "        \n",
    "        # Fit a line using linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(x.reshape(-1, 1), y)\n",
    "        \n",
    "        # Calculate direction vector from the slope\n",
    "        direction_vector = np.array([1, model.coef_[0]])  # [dx, dy]\n",
    "        return direction_vector / np.linalg.norm(direction_vector)  # Normalize\n",
    "\n",
    "    def calculate_parallel_score(geom, main_line):\n",
    "        \"\"\"\n",
    "        Calculate how parallel a line segment is to the main street.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert Point geometries to their closest main_line point\n",
    "            if isinstance(geom, Point):\n",
    "                return 0.0\n",
    "                \n",
    "            # Get direction vectors\n",
    "            main_vector = calculate_direction_vector(main_line)\n",
    "            segment_vector = calculate_direction_vector(geom)\n",
    "            \n",
    "            if main_vector is None or segment_vector is None:\n",
    "                return 0.0\n",
    "                \n",
    "            # Calculate dot product and magnitudes\n",
    "            dot_product = np.dot(main_vector, segment_vector)\n",
    "            main_magnitude = np.linalg.norm(main_vector)\n",
    "            segment_magnitude = np.linalg.norm(segment_vector)\n",
    "            \n",
    "            if main_magnitude == 0 or segment_magnitude == 0:\n",
    "                return 0.0\n",
    "                \n",
    "            cos_angle = abs(dot_product / (main_magnitude * segment_magnitude))\n",
    "            return float(cos_angle)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating parallel score: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Process each reference geometry separately\n",
    "    print(\"Processing each reference geometry...\")\n",
    "    all_parallel = []\n",
    "    \n",
    "    # Group by reference geometry\n",
    "    for ref_idx, group in nodes_in_buffer.groupby('ref_idx'):\n",
    "        print(f\"Processing reference geometry {ref_idx+1}/{len(gdf)}\", end='\\r')\n",
    "        \n",
    "        # Get the reference geometry\n",
    "        ref_geom = gdf.iloc[ref_idx].geometry\n",
    "        \n",
    "        # Calculate parallel scores\n",
    "        group = group.copy()\n",
    "        group['parallel_score'] = group.geometry.apply(\n",
    "            lambda x: calculate_parallel_score(x, ref_geom)\n",
    "        )\n",
    "        \n",
    "        # Filter parallel segments\n",
    "        parallel_segments = group[group['parallel_score'] > parallel_threshold].copy()\n",
    "        \n",
    "        # Add to results\n",
    "        if not parallel_segments.empty:\n",
    "            # Sort by parallel score\n",
    "            parallel_segments = parallel_segments.sort_values('parallel_score', ascending=False)\n",
    "            all_parallel.append(parallel_segments)\n",
    "    \n",
    "    # Combine all parallel segments and remove duplicates\n",
    "    if all_parallel:\n",
    "        parallel_nodes = pd.concat(all_parallel, ignore_index=True)\n",
    "        # Remove duplicates by ID (keeping the one with the highest parallel score)\n",
    "        if 'id' in parallel_nodes.columns:\n",
    "            parallel_nodes = parallel_nodes.sort_values('parallel_score', ascending=False)\n",
    "            parallel_nodes = parallel_nodes.drop_duplicates(subset=['id'])\n",
    "    else:\n",
    "        parallel_nodes = gpd.GeoDataFrame(\n",
    "            [], \n",
    "            geometry='geometry', \n",
    "            crs=gdf.crs,\n",
    "            columns=nodes_in_buffer.columns if not nodes_in_buffer.empty else ['geometry']\n",
    "        )\n",
    "\n",
    "    print(f\"Total nodes in buffer: {len(nodes_in_buffer)}\")\n",
    "    print(f\"Parallel nodes found: {len(parallel_nodes)}\")\n",
    "    \n",
    "    # Print some diagnostic information\n",
    "    if len(parallel_nodes) > 0:\n",
    "        print(\"\\nParallel score statistics:\")\n",
    "        print(parallel_nodes['parallel_score'].describe())\n",
    "    \n",
    "    # If we want additional fields from the original parquet, re-join now with the filtered dataset\n",
    "    if not parallel_nodes.empty and len(parallel_nodes) < 10000:  # Limit to reasonable size\n",
    "        print(\"Loading additional fields for parallel segments...\")\n",
    "        # Get IDs of parallel segments\n",
    "        # Verify id column exists\n",
    "        if 'id' not in parallel_nodes.columns:\n",
    "            print(\"ERROR: 'id' column not found, looking for alternatives...\")\n",
    "            if 'id_left' in parallel_nodes.columns:\n",
    "                print(\"Using 'id_left' instead\")\n",
    "                parallel_ids = parallel_nodes['id_left'].tolist()\n",
    "                id_field_to_use = 'id'  # The field in the original dataset is still called 'id'\n",
    "            else:\n",
    "                print(\"No suitable ID column found. Cannot load additional fields.\")\n",
    "                return nodes_in_buffer, parallel_nodes\n",
    "        else:\n",
    "            parallel_ids = parallel_nodes['id'].tolist()\n",
    "            id_field_to_use = 'id'\n",
    "            \n",
    "        print(f\"Found {len(parallel_ids)} unique IDs for rejoining\")\n",
    "        \n",
    "        # Read only the needed rows using a filter expression\n",
    "        # This is much more efficient than loading the entire file\n",
    "        try:\n",
    "            print(f\"Reading additional fields for only {len(parallel_ids)} segments...\")\n",
    "            \n",
    "            # For very large ID lists, we need to split the filter into chunks\n",
    "            # to avoid exceeding filter expression size limits\n",
    "            if len(parallel_ids) > 1000:\n",
    "                print(\"Large number of IDs, splitting into chunks...\")\n",
    "                chunk_size = 1000\n",
    "                chunks = [parallel_ids[i:i + chunk_size] for i in range(0, len(parallel_ids), chunk_size)]\n",
    "                \n",
    "                dfs = []\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    print(f\"Reading chunk {i+1}/{len(chunks)}...\", end='\\r')\n",
    "                    # Create filter expression for this chunk\n",
    "                    filters = [('id', 'in', chunk)]\n",
    "                    chunk_table = pq.read_table(\n",
    "                        parquet_path,\n",
    "                        filters=filters\n",
    "                    )\n",
    "                    dfs.append(chunk_table.to_pandas())\n",
    "                \n",
    "                # Combine all chunks\n",
    "                df_full_filtered = pd.concat(dfs, ignore_index=True)\n",
    "            else:\n",
    "                # For smaller ID lists, we can do it in one go\n",
    "                filters = [('id', 'in', parallel_ids)]\n",
    "                table = pq.read_table(\n",
    "                    parquet_path,\n",
    "                    filters=filters\n",
    "                )\n",
    "                df_full_filtered = table.to_pandas()\n",
    "                \n",
    "            print(f\"Successfully loaded {len(df_full_filtered)} rows with all fields\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error using filters: {e}\")\n",
    "            print(\"Falling back to reading entire file and filtering...\")\n",
    "            \n",
    "            # Fallback to reading the entire file\n",
    "            parquet_table_full = pq.read_table(parquet_path)\n",
    "            df_full = parquet_table_full.to_pandas()\n",
    "            \n",
    "            # Filter to only parallel segment IDs\n",
    "            df_full_filtered = df_full[df_full[id_field_to_use].isin(parallel_ids)]\n",
    "            print(f\"Filtered to {len(df_full_filtered)} rows\")\n",
    "        \n",
    "        # Get list of columns to merge (exclude geometry_wkt and any columns we already have)\n",
    "        merge_columns = [col for col in df_full_filtered.columns \n",
    "                         if col != 'geometry_wkt' and col not in parallel_nodes.columns]\n",
    "        \n",
    "        if merge_columns:\n",
    "            print(f\"Merging {len(merge_columns)} additional fields: {', '.join(merge_columns[:5])}...\")\n",
    "            \n",
    "            # Create a subset with only needed columns for more efficient merge\n",
    "            merge_df = df_full_filtered[[id_field_to_use] + merge_columns].copy()\n",
    "            \n",
    "            # Merge with our parallel segments\n",
    "            # Need to match the id field in parallel_nodes with id_field_to_use\n",
    "            if 'id' in parallel_nodes.columns:\n",
    "                parallel_nodes = parallel_nodes.merge(\n",
    "                    merge_df, \n",
    "                    left_on='id',\n",
    "                    right_on=id_field_to_use, \n",
    "                    how='left'\n",
    "                )\n",
    "            elif 'id_left' in parallel_nodes.columns:\n",
    "                parallel_nodes = parallel_nodes.merge(\n",
    "                    merge_df, \n",
    "                    left_on='id_left',\n",
    "                    right_on=id_field_to_use, \n",
    "                    how='left'\n",
    "                )\n",
    "        else:\n",
    "            print(\"No additional fields to merge.\")\n",
    "    else:\n",
    "        if parallel_nodes.empty:\n",
    "            print(\"No parallel segments found, skipping data re-join.\")\n",
    "        else:\n",
    "            print(f\"Too many parallel segments ({len(parallel_nodes)}) for full data re-join.\")\n",
    "\n",
    "    return nodes_in_buffer, parallel_nodes\n",
    "\n",
    "def main():\n",
    "    # Assumes gdf is already defined in the notebook\n",
    "    nodes_buffer, gdf3 = find_parallel_segments(\n",
    "        gdf, \n",
    "        'data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet',\n",
    "        # Buffer distance in meters\n",
    "        buffer_distance=4,\n",
    "        parallel_threshold=0.8\n",
    "        # Filter parallel segments using the score column, change as needed:\n",
    "        # 1.0 = completely parallel (0° angle)\n",
    "        # 0.866 = 30° angle\n",
    "        # 0.7 = approximately 45° angle\n",
    "        # 0.5 = 60° angle\n",
    "        # 0.0 = perpendicular (90° angle)\n",
    "    )\n",
    "    \n",
    "    print(f\"Created gdf3 with {len(gdf3)} parallel segments\")\n",
    "    \n",
    "    return nodes_buffer, gdf3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nodes_buffer, gdf3 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c1776d-6a1c-4524-aba5-fe5001d790ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet file (id, geometry_wkt fields only)...\n",
      "Successfully loaded 140632 rows\n",
      "Converting WKT to geometry objects...\n",
      "Creating spatial index...\n",
      "Finding potential intersections...\n",
      "Performing actual intersection test...\n",
      "Checking id column after spatial join...\n",
      "Renaming 'id_left' to 'id'\n",
      "Processing each reference geometry...\n",
      "Total nodes in buffer: 652try 148/158\n",
      "Parallel nodes found: 290\n",
      "\n",
      "Parallel score statistics:\n",
      "count    290.000000\n",
      "mean       0.987946\n",
      "std        0.029542\n",
      "min        0.837761\n",
      "25%        0.996547\n",
      "50%        0.999774\n",
      "75%        0.999987\n",
      "max        1.000000\n",
      "Name: parallel_score, dtype: float64\n",
      "Loading additional fields for parallel segments...\n",
      "Found 290 unique IDs for rejoining\n",
      "Reading additional fields for only 290 segments...\n",
      "Successfully loaded 436 rows with all fields\n",
      "Merging 814 additional fields: sobj_kz, segm_segm, segm_bez, stst_str, stor_name...\n",
      "Created gdf3 with 436 parallel segments\n"
     ]
    }
   ],
   "source": [
    "# Step 2 v2.3 final var RADWEGE [FULL] [parquet]: Find parallel segments to the overpass data in RADWEGE DATA\n",
    "# ADJUST buffer_distance and parallel_threshold at the end of the code!\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pyogrio\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely import wkt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.linear_model import LinearRegression  # For fitting a line to vertices\n",
    "from shapely.geometry import CAP_STYLE\n",
    "\n",
    "def find_parallel_segments(gdf, parquet_path, buffer_distance=15, parallel_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Find parallel segments from a parquet file with WKT geometries within a buffer of the input geometry.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame containing the reference street geometries (selection tool)\n",
    "    parquet_path : str\n",
    "        Path to the parquet file with WKT geometries (data source)\n",
    "    buffer_distance : float\n",
    "        Buffer distance in meters (default: 15)\n",
    "    parallel_threshold : float\n",
    "        Threshold for parallel score (default: 0.8)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (nodes_in_buffer, parallel_nodes)\n",
    "        Two GeoDataFrames containing all nodes in buffer and parallel nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create buffer for each reference geometry\n",
    "    gdf_buffer = gdf.copy()\n",
    "    # gdf_buffer['geometry'] = gdf.geometry.buffer(buffer_distance)\n",
    "    gdf_buffer['geometry'] = gdf.geometry.apply(lambda x: x.buffer(buffer_distance, cap_style=CAP_STYLE.flat))\n",
    "    gdf_buffer['ref_idx'] = gdf_buffer.index  # Track which reference each buffer is from\n",
    "\n",
    "    # Read the parquet file efficiently - only load id and geometry_wkt initially\n",
    "    print(\"Reading parquet file (id, geometry_wkt fields only)...\")\n",
    "    parquet_fields = ['id', 'geometry_wkt']\n",
    "    \n",
    "    # Use pyarrow to read only selected columns\n",
    "    try:\n",
    "        parquet_table = pq.read_table(parquet_path, columns=parquet_fields)\n",
    "        df_network = parquet_table.to_pandas()\n",
    "        print(f\"Successfully loaded {len(df_network)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet columns: {e}\")\n",
    "        print(\"Attempting to read all columns and then filter...\")\n",
    "        parquet_table = pq.read_table(parquet_path)\n",
    "        df_network = parquet_table.to_pandas()\n",
    "        # Keep only the columns we need\n",
    "        required_cols = ['id', 'geometry_wkt']\n",
    "        for col in required_cols:\n",
    "            if col not in df_network.columns:\n",
    "                raise ValueError(f\"Required column '{col}' not found in parquet file.\")\n",
    "        df_network = df_network[required_cols]\n",
    "        print(f\"Successfully loaded {len(df_network)} rows\")\n",
    "    \n",
    "    # Convert WKT to geometry objects\n",
    "    print(\"Converting WKT to geometry objects...\")\n",
    "    df_network['geometry'] = df_network['geometry_wkt'].apply(lambda x: wkt.loads(x) if x else None)\n",
    "    \n",
    "    # Drop rows with invalid geometries\n",
    "    df_network = df_network.dropna(subset=['geometry'])\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    gdf_network = gpd.GeoDataFrame(df_network, geometry='geometry')\n",
    "    \n",
    "    # Set CRS from the original GeoDataFrame\n",
    "    gdf_network.set_crs(gdf_buffer.crs, inplace=True)\n",
    "    \n",
    "    # Create spatial index for more efficient intersection\n",
    "    print(\"Creating spatial index...\")\n",
    "    sindex = gdf_network.sindex\n",
    "    \n",
    "    # Find potential intersections using the spatial index\n",
    "    print(\"Finding potential intersections...\")\n",
    "    potential_matches_index = []\n",
    "    \n",
    "    for geom in gdf_buffer.geometry:\n",
    "        potential_matches_index.extend(list(sindex.intersection(geom.bounds)))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    potential_matches_index = list(set(potential_matches_index))\n",
    "    \n",
    "    # Extract the candidates\n",
    "    candidates = gdf_network.iloc[potential_matches_index].copy()\n",
    "    \n",
    "    # Perform actual intersection test\n",
    "    print(\"Performing actual intersection test...\")\n",
    "    nodes_in_buffer = gpd.sjoin(\n",
    "        candidates, \n",
    "        gdf_buffer, \n",
    "        how='inner', \n",
    "        predicate='intersects'\n",
    "    )\n",
    "    \n",
    "    # Fix id column after spatial join (it becomes id_left)\n",
    "    print(\"Checking id column after spatial join...\")\n",
    "    if 'id' not in nodes_in_buffer.columns and 'id_left' in nodes_in_buffer.columns:\n",
    "        print(\"Renaming 'id_left' to 'id'\")\n",
    "        nodes_in_buffer['id'] = nodes_in_buffer['id_left']\n",
    "    \n",
    "    def calculate_direction_vector(geom):\n",
    "        \"\"\"\n",
    "        Calculate the direction vector of a line segment by fitting a line to all its vertices.\n",
    "        \"\"\"\n",
    "        if isinstance(geom, Point) or len(geom.coords) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Extract all vertices\n",
    "        x = np.array([coord[0] for coord in geom.coords])\n",
    "        y = np.array([coord[1] for coord in geom.coords])\n",
    "        \n",
    "        # Fit a line using linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(x.reshape(-1, 1), y)\n",
    "        \n",
    "        # Calculate direction vector from the slope\n",
    "        direction_vector = np.array([1, model.coef_[0]])  # [dx, dy]\n",
    "        return direction_vector / np.linalg.norm(direction_vector)  # Normalize\n",
    "\n",
    "    def calculate_parallel_score(geom, main_line):\n",
    "        \"\"\"\n",
    "        Calculate how parallel a line segment is to the main street.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert Point geometries to their closest main_line point\n",
    "            if isinstance(geom, Point):\n",
    "                return 0.0\n",
    "                \n",
    "            # Get direction vectors\n",
    "            main_vector = calculate_direction_vector(main_line)\n",
    "            segment_vector = calculate_direction_vector(geom)\n",
    "            \n",
    "            if main_vector is None or segment_vector is None:\n",
    "                return 0.0\n",
    "                \n",
    "            # Calculate dot product and magnitudes\n",
    "            dot_product = np.dot(main_vector, segment_vector)\n",
    "            main_magnitude = np.linalg.norm(main_vector)\n",
    "            segment_magnitude = np.linalg.norm(segment_vector)\n",
    "            \n",
    "            if main_magnitude == 0 or segment_magnitude == 0:\n",
    "                return 0.0\n",
    "                \n",
    "            cos_angle = abs(dot_product / (main_magnitude * segment_magnitude))\n",
    "            return float(cos_angle)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating parallel score: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Process each reference geometry separately\n",
    "    print(\"Processing each reference geometry...\")\n",
    "    all_parallel = []\n",
    "    \n",
    "    # Group by reference geometry\n",
    "    for ref_idx, group in nodes_in_buffer.groupby('ref_idx'):\n",
    "        print(f\"Processing reference geometry {ref_idx+1}/{len(gdf)}\", end='\\r')\n",
    "        \n",
    "        # Get the reference geometry\n",
    "        ref_geom = gdf.iloc[ref_idx].geometry\n",
    "        \n",
    "        # Calculate parallel scores\n",
    "        group = group.copy()\n",
    "        group['parallel_score'] = group.geometry.apply(\n",
    "            lambda x: calculate_parallel_score(x, ref_geom)\n",
    "        )\n",
    "        \n",
    "        # Filter parallel segments\n",
    "        parallel_segments = group[group['parallel_score'] > parallel_threshold].copy()\n",
    "        \n",
    "        # Add to results\n",
    "        if not parallel_segments.empty:\n",
    "            # Sort by parallel score\n",
    "            parallel_segments = parallel_segments.sort_values('parallel_score', ascending=False)\n",
    "            all_parallel.append(parallel_segments)\n",
    "    \n",
    "    # Combine all parallel segments and remove duplicates\n",
    "    if all_parallel:\n",
    "        parallel_nodes = pd.concat(all_parallel, ignore_index=True)\n",
    "        # Remove duplicates by ID (keeping the one with the highest parallel score)\n",
    "        if 'id' in parallel_nodes.columns:\n",
    "            parallel_nodes = parallel_nodes.sort_values('parallel_score', ascending=False)\n",
    "            parallel_nodes = parallel_nodes.drop_duplicates(subset=['id'])\n",
    "    else:\n",
    "        parallel_nodes = gpd.GeoDataFrame(\n",
    "            [], \n",
    "            geometry='geometry', \n",
    "            crs=gdf.crs,\n",
    "            columns=nodes_in_buffer.columns if not nodes_in_buffer.empty else ['geometry']\n",
    "        )\n",
    "\n",
    "    print(f\"Total nodes in buffer: {len(nodes_in_buffer)}\")\n",
    "    print(f\"Parallel nodes found: {len(parallel_nodes)}\")\n",
    "    \n",
    "    # Print some diagnostic information\n",
    "    if len(parallel_nodes) > 0:\n",
    "        print(\"\\nParallel score statistics:\")\n",
    "        print(parallel_nodes['parallel_score'].describe())\n",
    "    \n",
    "    # If we want additional fields from the original parquet, re-join now with the filtered dataset\n",
    "    if not parallel_nodes.empty and len(parallel_nodes) < 10000:  # Limit to reasonable size\n",
    "        print(\"Loading additional fields for parallel segments...\")\n",
    "        # Get IDs of parallel segments\n",
    "        # Verify id column exists\n",
    "        if 'id' not in parallel_nodes.columns:\n",
    "            print(\"ERROR: 'id' column not found, looking for alternatives...\")\n",
    "            if 'id_left' in parallel_nodes.columns:\n",
    "                print(\"Using 'id_left' instead\")\n",
    "                parallel_ids = parallel_nodes['id_left'].tolist()\n",
    "                id_field_to_use = 'id'  # The field in the original dataset is still called 'id'\n",
    "            else:\n",
    "                print(\"No suitable ID column found. Cannot load additional fields.\")\n",
    "                return nodes_in_buffer, parallel_nodes\n",
    "        else:\n",
    "            parallel_ids = parallel_nodes['id'].tolist()\n",
    "            id_field_to_use = 'id'\n",
    "            \n",
    "        print(f\"Found {len(parallel_ids)} unique IDs for rejoining\")\n",
    "        \n",
    "        # Read only the needed rows using a filter expression\n",
    "        # This is much more efficient than loading the entire file\n",
    "        try:\n",
    "            print(f\"Reading additional fields for only {len(parallel_ids)} segments...\")\n",
    "            \n",
    "            # For very large ID lists, we need to split the filter into chunks\n",
    "            # to avoid exceeding filter expression size limits\n",
    "            if len(parallel_ids) > 1000:\n",
    "                print(\"Large number of IDs, splitting into chunks...\")\n",
    "                chunk_size = 1000\n",
    "                chunks = [parallel_ids[i:i + chunk_size] for i in range(0, len(parallel_ids), chunk_size)]\n",
    "                \n",
    "                dfs = []\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    print(f\"Reading chunk {i+1}/{len(chunks)}...\", end='\\r')\n",
    "                    # Create filter expression for this chunk\n",
    "                    filters = [('id', 'in', chunk)]\n",
    "                    chunk_table = pq.read_table(\n",
    "                        parquet_path,\n",
    "                        filters=filters\n",
    "                    )\n",
    "                    dfs.append(chunk_table.to_pandas())\n",
    "                \n",
    "                # Combine all chunks\n",
    "                df_full_filtered = pd.concat(dfs, ignore_index=True)\n",
    "            else:\n",
    "                # For smaller ID lists, we can do it in one go\n",
    "                filters = [('id', 'in', parallel_ids)]\n",
    "                table = pq.read_table(\n",
    "                    parquet_path,\n",
    "                    filters=filters\n",
    "                )\n",
    "                df_full_filtered = table.to_pandas()\n",
    "                \n",
    "            print(f\"Successfully loaded {len(df_full_filtered)} rows with all fields\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error using filters: {e}\")\n",
    "            print(\"Falling back to reading entire file and filtering...\")\n",
    "            \n",
    "            # Fallback to reading the entire file\n",
    "            parquet_table_full = pq.read_table(parquet_path)\n",
    "            df_full = parquet_table_full.to_pandas()\n",
    "            \n",
    "            # Filter to only parallel segment IDs\n",
    "            df_full_filtered = df_full[df_full[id_field_to_use].isin(parallel_ids)]\n",
    "            print(f\"Filtered to {len(df_full_filtered)} rows\")\n",
    "        \n",
    "        # Get list of columns to merge (exclude geometry_wkt and any columns we already have)\n",
    "        merge_columns = [col for col in df_full_filtered.columns \n",
    "                         if col != 'geometry_wkt' and col not in parallel_nodes.columns]\n",
    "        \n",
    "        if merge_columns:\n",
    "            print(f\"Merging {len(merge_columns)} additional fields: {', '.join(merge_columns[:5])}...\")\n",
    "            \n",
    "            # Create a subset with only needed columns for more efficient merge\n",
    "            merge_df = df_full_filtered[[id_field_to_use] + merge_columns].copy()\n",
    "            \n",
    "            # Merge with our parallel segments\n",
    "            # Need to match the id field in parallel_nodes with id_field_to_use\n",
    "            if 'id' in parallel_nodes.columns:\n",
    "                parallel_nodes = parallel_nodes.merge(\n",
    "                    merge_df, \n",
    "                    left_on='id',\n",
    "                    right_on=id_field_to_use, \n",
    "                    how='left'\n",
    "                )\n",
    "            elif 'id_left' in parallel_nodes.columns:\n",
    "                parallel_nodes = parallel_nodes.merge(\n",
    "                    merge_df, \n",
    "                    left_on='id_left',\n",
    "                    right_on=id_field_to_use, \n",
    "                    how='left'\n",
    "                )\n",
    "        else:\n",
    "            print(\"No additional fields to merge.\")\n",
    "    else:\n",
    "        if parallel_nodes.empty:\n",
    "            print(\"No parallel segments found, skipping data re-join.\")\n",
    "        else:\n",
    "            print(f\"Too many parallel segments ({len(parallel_nodes)}) for full data re-join.\")\n",
    "\n",
    "    return nodes_in_buffer, parallel_nodes\n",
    "\n",
    "def main():\n",
    "    # Assumes gdf is already defined in the notebook\n",
    "    nodes_buffer, gdf3 = find_parallel_segments(\n",
    "        gdf, \n",
    "        'data/network_all_months_plus_25833_RVA_clipped.parquet',\n",
    "        # Buffer distance in meters\n",
    "        buffer_distance=10,\n",
    "        parallel_threshold=0.8\n",
    "        # Filter parallel segments using the score column, change as needed:\n",
    "        # 1.0 = completely parallel (0° angle)\n",
    "        # 0.866 = 30° angle\n",
    "        # 0.7 = approximately 45° angle\n",
    "        # 0.5 = 60° angle\n",
    "        # 0.0 = perpendicular (90° angle)\n",
    "    )\n",
    "    \n",
    "    print(f\"Created gdf3 with {len(gdf3)} parallel segments\")\n",
    "    \n",
    "    return nodes_buffer, gdf3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nodes_buffer, gdf3 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e088ffc-f011-47d7-bcf3-ceea4fb879aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  parquet_path: data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\n",
      "  helper_parquet_path: data/helper_segments.parquet\n",
      "  buffer_distance: 5\n",
      "  parallel_threshold: 0.8\n",
      "  indicator_field_name: protected_bike_lane\n",
      "Starting process with buffer distance: 5m and parallel threshold: 0.8\n",
      "Reading parquet file (id, geometry_wkt fields only)...\n",
      "Successfully loaded 592136 rows with minimal fields\n",
      "Converting WKT to geometry objects...\n",
      "Creating spatial index and finding intersections...\n",
      "Performing spatial join...\n",
      "Renaming 'id_left' to 'id'\n",
      "Processing each reference geometry...\n",
      "Processing reference geometry 158/158\n",
      "Total nodes in buffer: 1215\n",
      "Parallel nodes found: 615\n",
      "Found 615 unique IDs for parallel segments\n",
      "Creating new helper file at data/helper_segments.parquet\n",
      "Created new helper file with 615 rows\n",
      "Percentage of GDF IDs with matching parquet ID: 389.24%\n",
      "Total processing time: 11.59 seconds (0.19 minutes)\n",
      "Created simplified parallel segments GeoDataFrame with 615 rows\n",
      "Added field 'protected_bike_lane' to helper parquet at data/helper_segments.parquet\n"
     ]
    }
   ],
   "source": [
    "# Step 2 v2.3 final var HELPER-PARQUET [FULL] [parquet]: Find parallel and mark IDs with new field = 1\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely import wkt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from shapely.geometry import CAP_STYLE\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'parquet_path': 'data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet',\n",
    "    'helper_parquet_path': 'data/helper_segments.parquet',\n",
    "    'buffer_distance': 5,  # Buffer distance in meters\n",
    "    'parallel_threshold': 0.8,  # Threshold for parallel score\n",
    "    'indicator_field_name': 'protected_bike_lane'  # Change this for each new analysis\n",
    "}\n",
    "\n",
    "def find_parallel_segments(gdf, config=CONFIG):\n",
    "    \"\"\"\n",
    "    Find parallel segments from a parquet file within a buffer distance of the input geometry.\n",
    "    Properly updates the helper parquet file with a new indicator field.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame containing the reference street geometries\n",
    "    config : dict\n",
    "        Configuration dictionary with parameters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    parallel_nodes_simple : GeoDataFrame\n",
    "        GeoDataFrame containing the parallel segments with basic information\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting process with buffer distance: {config['buffer_distance']}m and parallel threshold: {config['parallel_threshold']}\")\n",
    "    \n",
    "    # Create buffer for each reference geometry\n",
    "    gdf_buffer = gdf.copy()\n",
    "    gdf_buffer['geometry'] = gdf.geometry.apply(lambda x: x.buffer(config['buffer_distance'], cap_style=CAP_STYLE.flat))\n",
    "    gdf_buffer['ref_idx'] = gdf_buffer.index\n",
    "    \n",
    "    # Read the parquet file - only load id and geometry_wkt initially\n",
    "    print(f\"Reading parquet file (id, geometry_wkt fields only)...\")\n",
    "    parquet_fields = ['id', 'geometry_wkt']\n",
    "    \n",
    "    try:\n",
    "        parquet_table = pq.read_table(config['parquet_path'], columns=parquet_fields)\n",
    "        df_network = parquet_table.to_pandas()\n",
    "        print(f\"Successfully loaded {len(df_network)} rows with minimal fields\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet columns: {e}\")\n",
    "        print(\"Attempting to read all columns and then filter...\")\n",
    "        parquet_table = pq.read_table(config['parquet_path'])\n",
    "        df_network = parquet_table.to_pandas()\n",
    "        required_cols = ['id', 'geometry_wkt']\n",
    "        for col in required_cols:\n",
    "            if col not in df_network.columns:\n",
    "                raise ValueError(f\"Required column '{col}' not found in parquet file.\")\n",
    "        df_network = df_network[required_cols]\n",
    "        print(f\"Successfully loaded {len(df_network)} rows, filtered to minimal fields\")\n",
    "    \n",
    "    # Convert WKT to geometry objects\n",
    "    print(\"Converting WKT to geometry objects...\")\n",
    "    df_network['geometry'] = df_network['geometry_wkt'].apply(lambda x: wkt.loads(x) if x else None)\n",
    "    df_network = df_network.dropna(subset=['geometry'])\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    gdf_network = gpd.GeoDataFrame(df_network, geometry='geometry')\n",
    "    gdf_network.set_crs(gdf_buffer.crs, inplace=True)\n",
    "    \n",
    "    # Create spatial index and find potential intersections\n",
    "    print(\"Creating spatial index and finding intersections...\")\n",
    "    sindex = gdf_network.sindex\n",
    "    potential_matches_index = []\n",
    "    \n",
    "    for geom in gdf_buffer.geometry:\n",
    "        potential_matches_index.extend(list(sindex.intersection(geom.bounds)))\n",
    "    \n",
    "    potential_matches_index = list(set(potential_matches_index))\n",
    "    candidates = gdf_network.iloc[potential_matches_index].copy()\n",
    "    \n",
    "    # Perform spatial join\n",
    "    print(\"Performing spatial join...\")\n",
    "    nodes_in_buffer = gpd.sjoin(\n",
    "        candidates, \n",
    "        gdf_buffer, \n",
    "        how='inner', \n",
    "        predicate='intersects'\n",
    "    )\n",
    "    \n",
    "    # Fix id column if needed\n",
    "    if 'id' not in nodes_in_buffer.columns and 'id_left' in nodes_in_buffer.columns:\n",
    "        print(\"Renaming 'id_left' to 'id'\")\n",
    "        nodes_in_buffer['id'] = nodes_in_buffer['id_left']\n",
    "    \n",
    "    # Direction vector calculation function\n",
    "    def calculate_direction_vector(geom):\n",
    "        if isinstance(geom, Point) or len(geom.coords) < 2:\n",
    "            return None\n",
    "        \n",
    "        x = np.array([coord[0] for coord in geom.coords])\n",
    "        y = np.array([coord[1] for coord in geom.coords])\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(x.reshape(-1, 1), y)\n",
    "        \n",
    "        direction_vector = np.array([1, model.coef_[0]])\n",
    "        return direction_vector / np.linalg.norm(direction_vector)\n",
    "\n",
    "    # Parallel score calculation function\n",
    "    def calculate_parallel_score(geom, main_line):\n",
    "        try:\n",
    "            if isinstance(geom, Point):\n",
    "                return 0.0\n",
    "                \n",
    "            main_vector = calculate_direction_vector(main_line)\n",
    "            segment_vector = calculate_direction_vector(geom)\n",
    "            \n",
    "            if main_vector is None or segment_vector is None:\n",
    "                return 0.0\n",
    "                \n",
    "            dot_product = np.dot(main_vector, segment_vector)\n",
    "            cos_angle = abs(dot_product)\n",
    "            return float(cos_angle)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating parallel score: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Process each reference geometry to find parallel segments\n",
    "    print(\"Processing each reference geometry...\")\n",
    "    all_parallel = []\n",
    "    \n",
    "    for ref_idx, group in nodes_in_buffer.groupby('ref_idx'):\n",
    "        print(f\"Processing reference geometry {ref_idx+1}/{len(gdf)}\", end='\\r')\n",
    "        \n",
    "        ref_geom = gdf.iloc[ref_idx].geometry\n",
    "        \n",
    "        group = group.copy()\n",
    "        group['parallel_score'] = group.geometry.apply(\n",
    "            lambda x: calculate_parallel_score(x, ref_geom)\n",
    "        )\n",
    "        \n",
    "        parallel_segments = group[group['parallel_score'] > config['parallel_threshold']].copy()\n",
    "        \n",
    "        if not parallel_segments.empty:\n",
    "            parallel_segments = parallel_segments.sort_values('parallel_score', ascending=False)\n",
    "            all_parallel.append(parallel_segments)\n",
    "    \n",
    "    # Combine all parallel segments\n",
    "    if all_parallel:\n",
    "        parallel_nodes = pd.concat(all_parallel, ignore_index=True)\n",
    "        parallel_nodes = parallel_nodes.sort_values('parallel_score', ascending=False)\n",
    "        parallel_nodes = parallel_nodes.drop_duplicates(subset=['id'])\n",
    "    else:\n",
    "        parallel_nodes = gpd.GeoDataFrame(\n",
    "            [], \n",
    "            geometry='geometry', \n",
    "            crs=gdf.crs,\n",
    "            columns=nodes_in_buffer.columns if not nodes_in_buffer.empty else ['geometry']\n",
    "        )\n",
    "\n",
    "    print(f\"\\nTotal nodes in buffer: {len(nodes_in_buffer)}\")\n",
    "    print(f\"Parallel nodes found: {len(parallel_nodes)}\")\n",
    "    \n",
    "    # Extract the IDs of parallel segments\n",
    "    if 'id' in parallel_nodes.columns:\n",
    "        parallel_ids = set(parallel_nodes['id'].tolist())\n",
    "    elif 'id_left' in parallel_nodes.columns:\n",
    "        parallel_ids = set(parallel_nodes['id_left'].tolist())\n",
    "    else:\n",
    "        print(\"ERROR: No ID column found in parallel nodes.\")\n",
    "        return parallel_nodes\n",
    "    \n",
    "    print(f\"Found {len(parallel_ids)} unique IDs for parallel segments\")\n",
    "    \n",
    "    # Create a simplified version of parallel_nodes with just the essential info\n",
    "    parallel_nodes_simple = parallel_nodes[['id', 'geometry', 'parallel_score']].copy()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # THIS IS THE FIXED PART: Properly update the helper parquet file\n",
    "    # =========================================================================\n",
    "    \n",
    "    def properly_update_helper_parquet():\n",
    "        \"\"\"\n",
    "        Properly update the helper parquet file without losing existing data.\n",
    "        \"\"\"\n",
    "        output_path = config['helper_parquet_path']\n",
    "        field_name = config['indicator_field_name']\n",
    "        score_field = f\"{field_name}_score\"\n",
    "        \n",
    "        # Check if output directory exists\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Check if helper file already exists\n",
    "        helper_exists = os.path.exists(output_path)\n",
    "        \n",
    "        if helper_exists:\n",
    "            print(f\"Updating existing helper file at {output_path}\")\n",
    "            # Read existing helper file\n",
    "            helper_df = pd.read_parquet(output_path)\n",
    "            original_row_count = len(helper_df)\n",
    "            print(f\"Existing helper file has {original_row_count} rows\")\n",
    "            \n",
    "            # Get existing IDs in the helper file\n",
    "            existing_ids = set(helper_df['id'].tolist())\n",
    "            \n",
    "            # Find IDs that exist in parallel_ids but not in existing_ids\n",
    "            new_ids = parallel_ids - existing_ids\n",
    "            print(f\"Found {len(new_ids)} new IDs to add to helper file\")\n",
    "            \n",
    "            # Add new column to existing helper file (with default value 0)\n",
    "            if field_name not in helper_df.columns:\n",
    "                helper_df[field_name] = 0\n",
    "            \n",
    "            # Update values for existing rows\n",
    "            helper_df.loc[helper_df['id'].isin(parallel_ids), field_name] = 1\n",
    "            \n",
    "            # Add score field if we have scores\n",
    "            if len(parallel_nodes) > 0 and 'parallel_score' in parallel_nodes.columns:\n",
    "                # Create mapping of ID to score\n",
    "                id_to_score = {row['id']: row['parallel_score'] \n",
    "                              for _, row in parallel_nodes.iterrows() \n",
    "                              if 'id' in row and 'parallel_score' in row}\n",
    "                \n",
    "                # Add score column\n",
    "                if score_field not in helper_df.columns:\n",
    "                    helper_df[score_field] = np.nan\n",
    "                \n",
    "                # Update scores for existing rows\n",
    "                for id_val, score in id_to_score.items():\n",
    "                    helper_df.loc[helper_df['id'] == id_val, score_field] = score\n",
    "            \n",
    "            # Create DataFrame for new rows that don't exist yet\n",
    "            if new_ids:\n",
    "                new_rows = []\n",
    "                \n",
    "                for id_val in new_ids:\n",
    "                    # Create a dictionary for the new row with default values\n",
    "                    row_dict = {'id': id_val}\n",
    "                    \n",
    "                    # Set 0 for all existing indicator columns\n",
    "                    for col in helper_df.columns:\n",
    "                        if col != 'id' and not col.endswith('_score'):\n",
    "                            row_dict[col] = 0\n",
    "                    \n",
    "                    # Set 1 for the current indicator\n",
    "                    row_dict[field_name] = 1\n",
    "                    \n",
    "                    # Add score if available\n",
    "                    if len(parallel_nodes) > 0 and 'parallel_score' in parallel_nodes.columns:\n",
    "                        score_rows = parallel_nodes[parallel_nodes['id'] == id_val]\n",
    "                        if len(score_rows) > 0:\n",
    "                            row_dict[score_field] = score_rows.iloc[0]['parallel_score']\n",
    "                    \n",
    "                    new_rows.append(row_dict)\n",
    "                \n",
    "                # Create DataFrame and append to existing\n",
    "                if new_rows:\n",
    "                    new_df = pd.DataFrame(new_rows)\n",
    "                    helper_df = pd.concat([helper_df, new_df], ignore_index=True)\n",
    "            \n",
    "            # Write back to parquet\n",
    "            helper_df.to_parquet(output_path, index=False)\n",
    "            \n",
    "            print(f\"Updated helper file now has {len(helper_df)} rows (added {len(helper_df) - original_row_count} new rows)\")\n",
    "            print(f\"Total segments with {field_name}=1: {helper_df[field_name].sum()}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Creating new helper file at {output_path}\")\n",
    "            \n",
    "            # For a new file, we only need to include the parallel IDs\n",
    "            rows = []\n",
    "            \n",
    "            for id_val in parallel_ids:\n",
    "                row_dict = {\n",
    "                    'id': id_val,\n",
    "                    field_name: 1\n",
    "                }\n",
    "                \n",
    "                # Add score if available\n",
    "                #if len(parallel_nodes) > 0 and 'parallel_score' in parallel_nodes.columns:\n",
    "                    #score_rows = parallel_nodes[parallel_nodes['id'] == id_val]\n",
    "                    #if len(score_rows) > 0:\n",
    "                        #row_dict[score_field] = score_rows.iloc[0]['parallel_score']\n",
    "                \n",
    "                rows.append(row_dict)\n",
    "            \n",
    "            # Create DataFrame\n",
    "            helper_df = pd.DataFrame(rows)\n",
    "            \n",
    "            # Write to parquet\n",
    "            helper_df.to_parquet(output_path, index=False)\n",
    "            \n",
    "            print(f\"Created new helper file with {len(helper_df)} rows\")\n",
    "        \n",
    "        # Calculate percentage of GDF IDs with matches\n",
    "        total_gdf_ids = len(gdf)\n",
    "        matching_percentage = (len(parallel_ids) / total_gdf_ids) * 100 if total_gdf_ids > 0 else 0\n",
    "        print(f\"Percentage of GDF IDs with matching parquet ID: {matching_percentage:.2f}%\")\n",
    "        \n",
    "        return len(parallel_ids), total_gdf_ids\n",
    "    \n",
    "    # Call the fixed function to update helper parquet\n",
    "    total_parallel, total_gdf = properly_update_helper_parquet()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Total processing time: {processing_time:.2f} seconds ({processing_time/60:.2f} minutes)\")\n",
    "    \n",
    "    return parallel_nodes_simple\n",
    "\n",
    "def main():\n",
    "    # For command-line usage\n",
    "    config = CONFIG.copy()\n",
    "    \n",
    "    # Load your GeoDataFrame here or assume it's already defined in the notebook\n",
    "    # gdf = gpd.read_file('path_to_your_gdf.gpkg')\n",
    "    \n",
    "    print(\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    parallel_segments = find_parallel_segments(gdf, config)\n",
    "    \n",
    "    print(f\"Created simplified parallel segments GeoDataFrame with {len(parallel_segments)} rows\")\n",
    "    print(f\"Added field '{config['indicator_field_name']}' to helper parquet at {config['helper_parquet_path']}\")\n",
    "    \n",
    "    return parallel_segments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parallel_segments = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf50fe5c-c22d-4783-a193-2ae504450227",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_470af58f001d19c65b6cf464a2e37e7e {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "    \n",
       "                    &lt;style&gt;\n",
       "                        .foliumtooltip {\n",
       "                            \n",
       "                        }\n",
       "                       .foliumtooltip table{\n",
       "                            margin: auto;\n",
       "                        }\n",
       "                        .foliumtooltip tr{\n",
       "                            text-align: left;\n",
       "                        }\n",
       "                        .foliumtooltip th{\n",
       "                            padding: 2px; padding-right: 8px;\n",
       "                        }\n",
       "                    &lt;/style&gt;\n",
       "            \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_470af58f001d19c65b6cf464a2e37e7e&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_470af58f001d19c65b6cf464a2e37e7e = L.map(\n",
       "                &quot;map_470af58f001d19c65b6cf464a2e37e7e&quot;,\n",
       "                {\n",
       "                    center: [52.514326, 13.406973],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    ...{\n",
       "  &quot;zoom&quot;: 10,\n",
       "  &quot;zoomControl&quot;: true,\n",
       "  &quot;preferCanvas&quot;: false,\n",
       "}\n",
       "\n",
       "                }\n",
       "            );\n",
       "            L.control.scale().addTo(map_470af58f001d19c65b6cf464a2e37e7e);\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_8a5663a168cee32d328b9399564bc86b = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {\n",
       "  &quot;minZoom&quot;: 0,\n",
       "  &quot;maxZoom&quot;: 19,\n",
       "  &quot;maxNativeZoom&quot;: 19,\n",
       "  &quot;noWrap&quot;: false,\n",
       "  &quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;,\n",
       "  &quot;subdomains&quot;: &quot;abc&quot;,\n",
       "  &quot;detectRetina&quot;: false,\n",
       "  &quot;tms&quot;: false,\n",
       "  &quot;opacity&quot;: 1,\n",
       "}\n",
       "\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_8a5663a168cee32d328b9399564bc86b.addTo(map_470af58f001d19c65b6cf464a2e37e7e);\n",
       "        \n",
       "    \n",
       "            map_470af58f001d19c65b6cf464a2e37e7e.fitBounds(\n",
       "                [[52.51121599999999, 13.396294], [52.517436, 13.417652]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "        function geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.5, &quot;weight&quot;: 2};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_highlighter(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.75};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_pointToLayer(feature, latlng) {\n",
       "            var opts = {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;#3388ff&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;#3388ff&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;opacity&quot;: 1.0, &quot;radius&quot;: 2, &quot;stroke&quot;: true, &quot;weight&quot;: 3};\n",
       "            \n",
       "            let style = geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_styler(feature)\n",
       "            Object.assign(opts, style)\n",
       "            \n",
       "            return new L.CircleMarker(latlng, opts)\n",
       "        }\n",
       "\n",
       "        function geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "                mouseout: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                            geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972.resetStyle(e.target);\n",
       "                    }\n",
       "                },\n",
       "                mouseover: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        const highlightStyle = geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_highlighter(e.target.feature)\n",
       "                        e.target.setStyle(highlightStyle);\n",
       "                    }\n",
       "                },\n",
       "            });\n",
       "        };\n",
       "        var geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_onEachFeature,\n",
       "            \n",
       "                style: geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_styler,\n",
       "                pointToLayer: geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_pointToLayer,\n",
       "            ...{\n",
       "}\n",
       "        });\n",
       "\n",
       "        function geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_add (data) {\n",
       "            geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972_add({&quot;bbox&quot;: [13.396294, 52.51121599999999, 13.417652, 52.517436], &quot;features&quot;: [{&quot;bbox&quot;: [13.400953000000001, 52.511636, 13.401094000000002, 52.51169599999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.401094000000002, 52.511636], [13.400953000000001, 52.51169599999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;143749&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396828000000001, 52.514348999999996, 13.396955, 52.51489099999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396828000000001, 52.51489099999999], [13.396955, 52.514348999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;1&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25079&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396381999999997, 52.516394, 13.396509, 52.517017], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396381999999997, 52.517017], [13.396509, 52.516394]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;2&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25043&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.413628, 52.513453, 13.41412, 52.51361000000001], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.413628, 52.513453], [13.413831, 52.51351799999999], [13.41412, 52.51361000000001]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;3&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;313890&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396766, 52.51489099999999, 13.396828000000001, 52.51515699999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396828000000001, 52.51489099999999], [13.396766, 52.51515699999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;4&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;1142079&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396319, 52.51709099999999, 13.396369, 52.517317000000006], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396369, 52.51709099999999], [13.396336, 52.517236], [13.396319, 52.517317000000006]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;5&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;911891&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.411548000000002, 52.512789, 13.413457, 52.51339899999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.413457, 52.51339899999999], [13.411548000000002, 52.512789]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;6&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;511400&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.401158, 52.51149, 13.401438, 52.51160799999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.401438, 52.51149], [13.401401, 52.511503], [13.401169, 52.511604], [13.401158, 52.51160799999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;7&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;1089042&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.399505, 52.512073, 13.400028, 52.51228499999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.399505, 52.51228499999999], [13.400028, 52.512073]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;8&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;389999&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396369, 52.517017, 13.396381999999997, 52.51709099999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396369, 52.51709099999999], [13.396381999999997, 52.517017]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;9&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;563894&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.400606, 52.51169599999999, 13.400953000000001, 52.511835], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.400606, 52.511835], [13.400953000000001, 52.51169599999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;10&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;143750&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.413457, 52.51339899999999, 13.413485, 52.513408], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.413485, 52.513408], [13.413457, 52.51339899999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;11&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;911836&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396538, 52.51605899999999, 13.39657, 52.51624199999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396538, 52.51624199999999], [13.39657, 52.51605899999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;12&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;310245&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.39657, 52.515696999999996, 13.39665, 52.51605899999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.39665, 52.515696999999996], [13.39657, 52.51605899999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;13&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;310247&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396524000000001, 52.51624199999999, 13.396538, 52.516315], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396538, 52.51624199999999], [13.396524000000001, 52.516315]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;14&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;575237&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.413485, 52.513408, 13.413539, 52.513425], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.413485, 52.513408], [13.413539, 52.513425]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;15&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;317400&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.399375, 52.51228499999999, 13.399505, 52.51233799999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.399505, 52.51228499999999], [13.399375, 52.51233799999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;16&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;143745&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.405586, 52.51121599999999, 13.405838, 52.51123300000001], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.405586, 52.51123300000001], [13.405838, 52.51121599999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;17&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;511377&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.413539, 52.513425, 13.413628, 52.513453], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.413539, 52.513425], [13.413628, 52.513453]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;18&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;911835&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.401438, 52.51148200000001, 13.401457, 52.51149], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.401438, 52.51149], [13.401457, 52.51148200000001]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;19&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;877688&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396509, 52.516315, 13.396524000000001, 52.516394], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396524000000001, 52.516315], [13.396509, 52.516394]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;20&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25044&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.415451000000001, 52.514161, 13.416688, 52.51423999999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.416688, 52.514161], [13.415451000000001, 52.51423999999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;21&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;297150&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.398953, 52.51233799999999, 13.399375, 52.51250999999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.399375, 52.51233799999999], [13.399032, 52.51247899999999], [13.398953, 52.51250999999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;22&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;375692&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396294, 52.517317000000006, 13.396319, 52.517436], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396319, 52.517317000000006], [13.396314, 52.517340999999995], [13.396294, 52.517436]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;23&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;517146&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.403514, 52.511286999999996, 13.404599, 52.51132], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.404599, 52.511286999999996], [13.403514, 52.51132]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;24&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;117025&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.398071, 52.51250999999999, 13.398953, 52.51285899999998], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.398953, 52.51250999999999], [13.398529000000002, 52.51267299999999], [13.398206, 52.512808], [13.398071, 52.51285899999998]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;25&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25099&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396731000000003, 52.515233, 13.396748, 52.515311999999994], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396731000000003, 52.515311999999994], [13.396748, 52.515233]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;26&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;3101&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.40014, 52.511988, 13.400246, 52.51202999999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.400246, 52.511988], [13.40014, 52.51202999999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;27&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25092&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.397173, 52.512909999999984, 13.397948, 52.513344], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.397948, 52.512909999999984], [13.397332, 52.51316599999999], [13.397229, 52.513238999999984], [13.397173, 52.513344]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;28&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25085&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.39665, 52.515598000000004, 13.396671, 52.515696999999996], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396671, 52.515598000000004], [13.39665, 52.515696999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;29&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;1053541&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396671, 52.515311999999994, 13.396731000000003, 52.515598000000004], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396671, 52.515598000000004], [13.396731000000003, 52.515311999999994]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;30&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;1142084&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.400246, 52.511931, 13.400384, 52.511988], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.400384, 52.511931], [13.400246, 52.511988]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;31&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;143754&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.401094000000002, 52.51160799999999, 13.401158, 52.511636], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.401158, 52.51160799999999], [13.401094000000002, 52.511636]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;32&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;524448&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.405112000000003, 52.511271, 13.405179, 52.511274], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.405179, 52.511271], [13.405112000000003, 52.511274]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;33&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;127186&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396955, 52.513344, 13.397173, 52.514348999999996], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.397173, 52.513344], [13.396955, 52.514348999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;34&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25077&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.396748, 52.51515699999999, 13.396766, 52.515233], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.396766, 52.51515699999999], [13.396748, 52.515233]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;35&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;3100&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.416688, 52.51382300000001, 13.417652, 52.514161], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.416688, 52.514161], [13.416872, 52.514113], [13.417007, 52.514061], [13.417178, 52.51397399999998], [13.417438000000002, 52.51387], [13.417501, 52.51385], [13.417539, 52.51384], [13.417652, 52.51382300000001]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;36&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25401&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.40501, 52.51123699999999, 13.405176, 52.51124399999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.40501, 52.51124399999999], [13.405176, 52.51123699999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;37&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;417999&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.400028, 52.51202999999999, 13.40014, 52.512073], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.400028, 52.512073], [13.40014, 52.51202999999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;38&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25094&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.403349000000002, 52.511320999999995, 13.403453000000003, 52.51132299999998], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.403349000000002, 52.51132299999998], [13.403453000000003, 52.511320999999995]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;39&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25367&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.407618000000001, 52.511610999999995, 13.407685000000003, 52.51163199999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.407685000000003, 52.51163199999999], [13.407618000000001, 52.511610999999995]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;40&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;337866&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.403453000000003, 52.51132, 13.403514, 52.511320999999995], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.403514, 52.51132], [13.403453000000003, 52.511320999999995]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;41&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25366&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.400384, 52.511835, 13.400606, 52.511931], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.400384, 52.511931], [13.400606, 52.511835]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;42&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;537555&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.415274, 52.514217, 13.415451000000001, 52.51423999999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.415451000000001, 52.51423999999999], [13.415274, 52.514217]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;43&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25396&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.407685000000003, 52.51163199999999, 13.408646, 52.511948], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.407685000000003, 52.51163199999999], [13.408646, 52.511948]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;44&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;393791&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.408646, 52.511948, 13.408774000000001, 52.511990999999995], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.408646, 52.511948], [13.408696000000003, 52.51196499999999], [13.408774000000001, 52.511990999999995]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;45&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25374&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.404599, 52.51124399999999, 13.40501, 52.511286999999996], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.40501, 52.51124399999999], [13.404841, 52.51124799999999], [13.404599, 52.511286999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;46&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;495651&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.406164, 52.511255999999996, 13.407618000000001, 52.511610999999995], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.406164, 52.511255999999996], [13.406447, 52.511298000000004], [13.406778, 52.511359000000006], [13.407208, 52.511475999999995], [13.407618000000001, 52.511610999999995]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;47&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;337867&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.405176, 52.51123300000001, 13.405586, 52.51123699999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.405176, 52.51123699999999], [13.405586, 52.51123300000001]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;48&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;371466&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.397948, 52.51285899999998, 13.398071, 52.512909999999984], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.398071, 52.51285899999998], [13.397948, 52.512909999999984]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;49&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;698692&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.405838, 52.51121599999999, 13.406073, 52.511242], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.406073, 52.511242], [13.405927, 52.51122300000001], [13.405838, 52.51121599999999]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;50&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;511378&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.415005, 52.514159, 13.415274, 52.514217], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.415005, 52.514159], [13.415274, 52.514217]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;51&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25397&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.404599, 52.511286999999996, 13.405183, 52.511315999999994], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.405183, 52.51130599999999], [13.404846, 52.511315999999994], [13.404599, 52.511286999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;52&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;495652&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.41412, 52.51361000000001, 13.414638, 52.513830999999996], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.41412, 52.51361000000001], [13.4145, 52.51373200000001], [13.414574000000002, 52.513783999999994], [13.414638, 52.513830999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;53&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;920937&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.405183, 52.51123300000001, 13.405586, 52.51130599999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.405183, 52.51130599999999], [13.405586, 52.51123300000001]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;54&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;371467&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.414837, 52.514034, 13.415005, 52.514159], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.414837, 52.514034], [13.414931, 52.514110999999986], [13.415005, 52.514159]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;55&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;313881&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.406073, 52.511242, 13.406164, 52.511255999999996], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.406073, 52.511242], [13.406164, 52.511255999999996]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;56&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;961095&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.414638, 52.513830999999996, 13.414837, 52.514034], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.414638, 52.513830999999996], [13.414652, 52.513842], [13.414728, 52.51392499999999], [13.414837, 52.514034]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;57&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;313884&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.403271, 52.511286999999996, 13.403453000000003, 52.511320999999995], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.403271, 52.511286999999996], [13.403324, 52.51129199999999], [13.403453000000003, 52.511320999999995]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;58&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;25368&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.414388, 52.514046, 13.415005, 52.514159], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.414388, 52.514046], [13.414708, 52.514115], [13.414802, 52.51413799999999], [13.415005, 52.514159]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;59&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;313882&quot;}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [13.400028, 52.512073, 13.400092, 52.512253], &quot;geometry&quot;: {&quot;coordinates&quot;: [[13.400037, 52.512253], [13.400073, 52.512145], [13.400092, 52.512129], [13.400028, 52.512073]], &quot;type&quot;: &quot;LineString&quot;}, &quot;id&quot;: &quot;60&quot;, &quot;properties&quot;: {&quot;id&quot;: &quot;389998&quot;}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "    geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972.bindTooltip(\n",
       "    function(layer){\n",
       "    let div = L.DomUtil.create(&#x27;div&#x27;);\n",
       "    \n",
       "    let handleObject = feature=&gt;typeof(feature)==&#x27;object&#x27; ? JSON.stringify(feature) : feature;\n",
       "    let fields = [&quot;id&quot;];\n",
       "    let aliases = [&quot;id&quot;];\n",
       "    let table = &#x27;&lt;table&gt;&#x27; +\n",
       "        String(\n",
       "        fields.map(\n",
       "        (v,i)=&gt;\n",
       "        `&lt;tr&gt;\n",
       "            &lt;th&gt;${aliases[i]}&lt;/th&gt;\n",
       "            \n",
       "            &lt;td&gt;${handleObject(layer.feature.properties[v])}&lt;/td&gt;\n",
       "        &lt;/tr&gt;`).join(&#x27;&#x27;))\n",
       "    +&#x27;&lt;/table&gt;&#x27;;\n",
       "    div.innerHTML=table;\n",
       "    \n",
       "    return div\n",
       "    }\n",
       "    ,{\n",
       "  &quot;sticky&quot;: true,\n",
       "  &quot;className&quot;: &quot;foliumtooltip&quot;,\n",
       "});\n",
       "                     \n",
       "    \n",
       "            geo_json_2c2fcc74ef4b8017b2becfd6ad9b9972.addTo(map_470af58f001d19c65b6cf464a2e37e7e);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f6e4057afc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1: Check if necessary\n",
    "# gdf3.explore()\n",
    "\n",
    "# Create a simplified GeoDataFrame with only id and geometry\n",
    "gdf3_simple = gdf3[['id', 'geometry']].copy()\n",
    "\n",
    "# Create the interactive map\n",
    "gdf3_simple.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c0d965-12a2-4662-b006-7bb9f91169e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2825f7bc0a2044b89738d13a82e86fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[np.float64(52.505145999999996), np.float64(13.408959)], controls=(ZoomControl(options=['position',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b9a9d2a2b043d8b6a131dd389e0aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Analysis Controls:</b>'), Checkbox(value=False, description='Comparison Mode', s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3 v4.2: [FULL] Analyze values by month interactively + processing statistical values on the fly\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipyleaflet\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import json\n",
    "import warnings\n",
    "from shapely.geometry import Point, shape\n",
    "from scipy.stats import entropy\n",
    "from typing import Optional, Tuple\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Speed Statistics Calculator Class\n",
    "class SpeedStatistics:\n",
    "    \"\"\"Class to handle speed histogram statistics calculations\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bins: int = 32):\n",
    "        self.n_bins = n_bins\n",
    "        self.bin_edges = np.arange(0, n_bins + 1)\n",
    "        self.midpoints = (self.bin_edges[:-1] + self.bin_edges[1:]) / 2\n",
    "    \n",
    "    def parse_histogram(self, binned_speeds) -> Optional[np.ndarray]:\n",
    "        \"\"\"Safely parse histogram string to numpy array\"\"\"\n",
    "        if binned_speeds is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(binned_speeds, (list, np.ndarray)):\n",
    "            # If already a list or array, convert to numpy array\n",
    "            return np.array(binned_speeds)\n",
    "            \n",
    "        if pd.isna(binned_speeds):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            if isinstance(binned_speeds, str):\n",
    "                hist = eval(binned_speeds)\n",
    "            else:\n",
    "                hist = binned_speeds\n",
    "                \n",
    "            hist_array = np.array(hist)\n",
    "            if len(hist_array) != self.n_bins:\n",
    "                warnings.warn(f\"Expected {self.n_bins} bins, got {len(hist_array)}\")\n",
    "                return None\n",
    "            return hist_array\n",
    "            \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error parsing histogram: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_stats(self, binned_speeds) -> Tuple:\n",
    "        \"\"\"Calculate comprehensive statistics for a speed histogram\"\"\"\n",
    "        # Parse histogram\n",
    "        hist_array = self.parse_histogram(binned_speeds)\n",
    "        if hist_array is None:\n",
    "            return (np.nan,) * 15\n",
    "        \n",
    "        total_count = hist_array.sum()\n",
    "        if total_count == 0:\n",
    "            return (np.nan,) * 15\n",
    "            \n",
    "        # Calculate probabilities\n",
    "        probs = hist_array / total_count\n",
    "        \n",
    "        # Basic statistics\n",
    "        weighted_mean = np.sum(self.midpoints * probs)\n",
    "        weighted_var = np.sum(((self.midpoints - weighted_mean) ** 2) * probs)\n",
    "        weighted_std = np.sqrt(weighted_var)\n",
    "        \n",
    "        # Higher moments\n",
    "        if weighted_std > 0:\n",
    "            standardized_moments = (self.midpoints - weighted_mean) / weighted_std\n",
    "            skewness = np.sum((standardized_moments ** 3) * probs)\n",
    "            kurtosis = np.sum((standardized_moments ** 4) * probs) - 3\n",
    "        else:\n",
    "            skewness = np.nan\n",
    "            kurtosis = np.nan\n",
    "        \n",
    "        # Calculate CV\n",
    "        cv = (weighted_std / weighted_mean * 100) if weighted_mean > 0 else np.nan\n",
    "        \n",
    "        # Entropy (only for non-zero probabilities)\n",
    "        valid_probs = probs[probs > 0]\n",
    "        data_entropy = entropy(valid_probs, base=2) if len(valid_probs) > 0 else np.nan\n",
    "        \n",
    "        # Quantiles\n",
    "        cum_probs = np.cumsum(hist_array) / total_count\n",
    "        q1 = np.interp(0.25, cum_probs, self.midpoints)\n",
    "        q2 = np.interp(0.50, cum_probs, self.midpoints)\n",
    "        q3 = np.interp(0.75, cum_probs, self.midpoints)\n",
    "        q4 = np.interp(1.00, cum_probs, self.midpoints)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Speed range percentages\n",
    "        pct_0_5 = np.interp(5, self.midpoints, cum_probs, left=0) * 100\n",
    "        pct_5_15 = (np.interp(15, self.midpoints, cum_probs) - \n",
    "                   np.interp(5, self.midpoints, cum_probs)) * 100\n",
    "        pct_15_25 = (np.interp(25, self.midpoints, cum_probs) - \n",
    "                    np.interp(15, self.midpoints, cum_probs)) * 100\n",
    "        pct_25_plus = (1 - np.interp(25, self.midpoints, cum_probs, right=1)) * 100\n",
    "        \n",
    "        return tuple(round(x, 2) if not pd.isna(x) else x for x in [\n",
    "            weighted_mean, weighted_std, cv, skewness, kurtosis,\n",
    "            data_entropy, iqr, q1, q2, q3, q4,\n",
    "            pct_0_5, pct_5_15, pct_15_25, pct_25_plus\n",
    "        ])\n",
    "\n",
    "# Get available metrics and times for analysis\n",
    "def get_available_metrics_and_times(gdf):\n",
    "    \"\"\"Get available metrics and time periods from the data\"\"\"\n",
    "    # Find all columns that match the pattern YY-MM_metric\n",
    "    time_cols = sorted([col for col in gdf.columns \n",
    "                        if len(col.split('-')) > 1 and \n",
    "                        len(col.split('-')[0]) == 2 and \n",
    "                        len(col.split('-')[1].split('_')[0]) == 2])\n",
    "    \n",
    "    metrics = set()\n",
    "    time_periods = set()\n",
    "    \n",
    "    for col in time_cols:\n",
    "        parts = col.split('_', 1)\n",
    "        if len(parts) > 1:\n",
    "            yy_mm = parts[0]\n",
    "            metric = parts[1]\n",
    "            \n",
    "            yy, mm = yy_mm.split('-')\n",
    "            time_periods.add(f\"20{yy}-{mm}\")\n",
    "            metrics.add(metric)\n",
    "    \n",
    "    # Define core metrics (even if they don't exist in the data)\n",
    "    # We'll calculate these on-the-fly from speeds histograms if needed\n",
    "    core_metrics = {'speeds', 'speed', 'route_count', 'cv', 'ent', 'skew', 'kurt', 'std', 'iqr', \n",
    "                    'q1', 'q2', 'q3', 'q4', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p'}\n",
    "    \n",
    "    # Only include metrics that have corresponding speed or speeds histograms\n",
    "    available_metrics = {'route_count'}\n",
    "    for period in time_periods:\n",
    "        yy = period.split('-')[0][-2:]\n",
    "        mm = period.split('-')[1]\n",
    "        if f\"{yy}-{mm}_speed\" in gdf.columns:\n",
    "            available_metrics.add('speed')\n",
    "        if f\"{yy}-{mm}_speeds\" in gdf.columns:\n",
    "            available_metrics.add('speeds')\n",
    "        if f\"{yy}-{mm}_cv\" in gdf.columns:\n",
    "            available_metrics.add('cv')\n",
    "        if f\"{yy}-{mm}_ent\" in gdf.columns:\n",
    "            available_metrics.add('ent')\n",
    "        if f\"{yy}-{mm}_skew\" in gdf.columns:\n",
    "            available_metrics.add('skew')\n",
    "    \n",
    "    # Add derived metrics that we can calculate if not already in the data\n",
    "    derived_metrics = {'speed', 'cv', 'ent', 'skew', 'kurt', 'std', 'iqr', \n",
    "                      'q1', 'q2', 'q3', 'q4', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p'}\n",
    "    \n",
    "    # Merge available and derived metrics\n",
    "    filtered_metrics = available_metrics.union(derived_metrics)\n",
    "    \n",
    "    return sorted(list(filtered_metrics)), sorted(list(time_periods))\n",
    "\n",
    "# Process temporal data for analysis\n",
    "def process_temporal_data(gdf, metric, selected_dates):\n",
    "    \"\"\"Process temporal data for given metric and dates\"\"\"\n",
    "    if not metric or not selected_dates:\n",
    "        return None\n",
    "        \n",
    "    # Initialize the speed statistics calculator\n",
    "    calculator = SpeedStatistics()\n",
    "    \n",
    "    if metric == 'General Comparison':\n",
    "        # Expanded metrics list to include quantiles and speed ranges\n",
    "        metrics = ['speeds', 'cv', 'ent', 'skew', 'q1', 'q2', 'q3', 'iqr', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p']\n",
    "        results = {}\n",
    "        \n",
    "        # First get all the pre-calculated speed values\n",
    "        speed_values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speed_col = f\"{year}-{month}_speed\"\n",
    "            if speed_col in gdf.columns:\n",
    "                values = gdf[speed_col].dropna().tolist()\n",
    "                speed_values.extend(values)\n",
    "        \n",
    "        if speed_values:\n",
    "            results['speed'] = speed_values\n",
    "            \n",
    "        # For speeds histograms, sum them\n",
    "        histograms = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speeds_col = f\"{year}-{month}_speeds\"\n",
    "            if speeds_col in gdf.columns:\n",
    "                for hist_str in gdf[speeds_col]:\n",
    "                    if pd.notna(hist_str):\n",
    "                        hist = eval(hist_str) if isinstance(hist_str, str) else hist_str\n",
    "                        histograms.append(hist)\n",
    "        \n",
    "        if histograms:\n",
    "            results['speeds'] = np.sum(histograms, axis=0)\n",
    "            \n",
    "        # Metric index in stats tuple\n",
    "        metric_index = {\n",
    "            'cv': 2,       # coefficient of variation\n",
    "            'skew': 3,     # skewness\n",
    "            'ent': 5,      # entropy\n",
    "            'iqr': 6,      # interquartile range\n",
    "            'q1': 7,       # first quartile\n",
    "            'q2': 8,       # median\n",
    "            'q3': 9,       # third quartile\n",
    "            'p_0_5': 11,   # % speeds 0-5 km/h\n",
    "            'p_5_15': 12,  # % speeds 5-15 km/h\n",
    "            'p_15_25': 13, # % speeds 15-25 km/h\n",
    "            'p_25p': 14    # % speeds 25+ km/h\n",
    "        }\n",
    "        \n",
    "        # Try using pre-calculated metrics first, then calculate from histograms if needed\n",
    "        for m in metrics:\n",
    "            if m == 'speeds':\n",
    "                continue  # Already handled above\n",
    "                \n",
    "            values = []\n",
    "            for date in selected_dates:\n",
    "                year = date.split('-')[0][-2:]\n",
    "                month = date.split('-')[1]\n",
    "                metric_col = f\"{year}-{month}_{m}\"\n",
    "                if metric_col in gdf.columns:\n",
    "                    metric_values = gdf[metric_col].dropna().tolist()\n",
    "                    values.extend(metric_values)\n",
    "            \n",
    "            if values:\n",
    "                results[m] = values\n",
    "            elif histograms and m in metric_index:\n",
    "                # Calculate from histograms if we don't have pre-calculated values\n",
    "                calculated_values = []\n",
    "                for hist in histograms:\n",
    "                    stats = calculator.calculate_stats(hist)\n",
    "                    if not pd.isna(stats[metric_index[m]]):\n",
    "                        calculated_values.append(stats[metric_index[m]])\n",
    "                \n",
    "                if calculated_values:\n",
    "                    results[m] = calculated_values\n",
    "        \n",
    "        return results if results else None\n",
    "    \n",
    "    elif metric == 'speed':\n",
    "        # Try to use pre-calculated speed values first\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_speed\"\n",
    "            if col in gdf.columns:\n",
    "                values.extend(gdf[col].dropna().tolist())\n",
    "        \n",
    "        if values:\n",
    "            return values\n",
    "        \n",
    "        # If no pre-calculated values, calculate from histograms\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speeds_col = f\"{year}-{month}_speeds\"\n",
    "            \n",
    "            if speeds_col in gdf.columns:\n",
    "                for hist_str in gdf[speeds_col]:\n",
    "                    if pd.notna(hist_str):\n",
    "                        stats = calculator.calculate_stats(hist_str)\n",
    "                        if not pd.isna(stats[0]):  # Mean speed is the first value\n",
    "                            values.append(stats[0])\n",
    "        \n",
    "        return values if values else None\n",
    "    \n",
    "    elif metric == 'speeds':\n",
    "        # Sum all histograms for the speeds metric\n",
    "        histograms = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_speeds\"\n",
    "            if col in gdf.columns:\n",
    "                for hist_str in gdf[col]:\n",
    "                    if pd.notna(hist_str):\n",
    "                        hist = eval(hist_str) if isinstance(hist_str, str) else hist_str\n",
    "                        histograms.append(hist)\n",
    "        return np.sum(histograms, axis=0) if histograms else None\n",
    "    \n",
    "    elif metric in ['cv', 'ent', 'skew', 'kurt', 'std', 'iqr', \n",
    "                    'q1', 'q2', 'q3', 'q4', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p']:\n",
    "        # First try to get pre-calculated values\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_{metric}\"\n",
    "            if col in gdf.columns:\n",
    "                values.extend(gdf[col].dropna().tolist())\n",
    "        \n",
    "        if values:\n",
    "            return values\n",
    "        \n",
    "        # If no pre-calculated values, calculate from histograms\n",
    "        metric_index = {\n",
    "            'speed': 0,    # mean\n",
    "            'std': 1,      # standard deviation\n",
    "            'cv': 2,       # coefficient of variation  \n",
    "            'skew': 3,     # skewness\n",
    "            'kurt': 4,     # kurtosis\n",
    "            'ent': 5,      # entropy\n",
    "            'iqr': 6,      # interquartile range\n",
    "            'q1': 7,       # first quartile\n",
    "            'q2': 8,       # median\n",
    "            'q3': 9,       # third quartile\n",
    "            'q4': 10,      # maximum\n",
    "            'p_0_5': 11,   # % speeds 0-5 km/h\n",
    "            'p_5_15': 12,  # % speeds 5-15 km/h\n",
    "            'p_15_25': 13, # % speeds 15-25 km/h\n",
    "            'p_25p': 14    # % speeds 25+ km/h\n",
    "        }\n",
    "        \n",
    "        index = metric_index.get(metric, 0)\n",
    "        values = []\n",
    "        \n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speeds_col = f\"{year}-{month}_speeds\"\n",
    "            \n",
    "            if speeds_col in gdf.columns:\n",
    "                for hist_str in gdf[speeds_col]:\n",
    "                    if pd.notna(hist_str):\n",
    "                        stats = calculator.calculate_stats(hist_str)\n",
    "                        if not pd.isna(stats[index]):\n",
    "                            values.append(stats[index])\n",
    "        \n",
    "        return values if values else None\n",
    "    \n",
    "    else:\n",
    "        # For other metrics like route_count that may already exist\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_{metric}\"\n",
    "            if col in gdf.columns:\n",
    "                values.extend(gdf[col].dropna().tolist())\n",
    "        return values if values else None\n",
    "\n",
    "# Create interactive map\n",
    "def create_interactive_map(gdf, original_gdf=None):\n",
    "    \"\"\"Create interactive map with draw controls\"\"\"\n",
    "    gdf_wgs84 = gdf.to_crs('EPSG:4326')\n",
    "    center_lat = gdf_wgs84.total_bounds[[1,3]].mean()\n",
    "    center_lon = gdf_wgs84.total_bounds[[0,2]].mean()\n",
    "    \n",
    "    m = ipyleaflet.Map(\n",
    "        center=(center_lat, center_lon),\n",
    "        zoom=14,\n",
    "        scroll_wheel_zoom=True\n",
    "    )\n",
    "    \n",
    "    draw_control = ipyleaflet.DrawControl(\n",
    "        polygon={\"shapeOptions\": {\"color\": \"#ff0000\"}},\n",
    "        rectangle={\"shapeOptions\": {\"color\": \"#ff0000\"}},\n",
    "        circle={\"shapeOptions\": {\"color\": \"#ff0000\"}},\n",
    "        circlemarker={},\n",
    "        polyline={},\n",
    "        marker={}\n",
    "    )\n",
    "    m.add_control(draw_control)\n",
    "    \n",
    "    # Style based on segment type if 'is_original' column exists\n",
    "    if 'is_original' in gdf_wgs84.columns:\n",
    "        # Speed field for coloring\n",
    "        speed_field = '2304-2412_speed'\n",
    "        if speed_field not in gdf_wgs84.columns:\n",
    "            speed_fields = [col for col in gdf_wgs84.columns if col.endswith('_speed')]\n",
    "            if speed_fields:\n",
    "                speed_field = speed_fields[0]\n",
    "            else:\n",
    "                speed_field = None\n",
    "        \n",
    "        # Setup color normalization for speed values if available\n",
    "        if speed_field:\n",
    "            valid_speeds = gdf_wgs84[gdf_wgs84['is_original'] == False][speed_field].dropna()\n",
    "            if len(valid_speeds) > 0:\n",
    "                norm = plt.Normalize(valid_speeds.min(), valid_speeds.max())\n",
    "            else:\n",
    "                norm = plt.Normalize(0, 30)\n",
    "            cmap = plt.cm.viridis\n",
    "        \n",
    "        def style_callback(feature):\n",
    "            # Check if this is an original segment\n",
    "            is_original = feature['properties'].get('is_original', False)\n",
    "            \n",
    "            if is_original:\n",
    "                return {\n",
    "                    'color': '#FF0000',  # Red for original segments\n",
    "                    'weight': 5,\n",
    "                    'opacity': 1.0,\n",
    "                    'dashArray': '5,5'  # Dashed line for original segments\n",
    "                }\n",
    "            else:\n",
    "                # For parallel segments, use speed-based coloring if available\n",
    "                if speed_field and speed_field in feature['properties']:\n",
    "                    speed = feature['properties'][speed_field]\n",
    "                    if pd.notna(speed):\n",
    "                        color = mcolors.rgb2hex(cmap(norm(speed)))\n",
    "                    else:\n",
    "                        color = '#808080'  # Gray for unknown speed\n",
    "                else:\n",
    "                    color = '#0000FF'  # Blue for parallel segments with no speed data\n",
    "                    \n",
    "                return {\n",
    "                    'color': color,\n",
    "                    'weight': 3,\n",
    "                    'opacity': 0.7\n",
    "                }\n",
    "    else:\n",
    "        # Use the standard speed-based styling\n",
    "        # Use the new field name as specified\n",
    "        speed_field = '2304-2412_speed'\n",
    "        \n",
    "        # If speed field doesn't exist, calculate it from the speed histogram\n",
    "        if speed_field not in gdf_wgs84.columns and '2304-2412_speeds' in gdf_wgs84.columns:\n",
    "            calculator = SpeedStatistics()\n",
    "            gdf_wgs84[speed_field] = gdf_wgs84['2304-2412_speeds'].apply(\n",
    "                lambda x: calculator.calculate_stats(x)[0] if pd.notna(x) else None\n",
    "            )\n",
    "        \n",
    "        # Fallback options if still not available\n",
    "        if speed_field not in gdf_wgs84.columns:\n",
    "            # Try to find any existing speed field\n",
    "            speed_fields = [col for col in gdf_wgs84.columns if col.endswith('_speed')]\n",
    "            if speed_fields:\n",
    "                speed_field = speed_fields[0]\n",
    "            else:\n",
    "                # Or calculate from any speeds histogram field\n",
    "                speeds_fields = [col for col in gdf_wgs84.columns if col.endswith('_speeds')]\n",
    "                if speeds_fields:\n",
    "                    speed_field = speeds_fields[0].replace('_speeds', '_speed')\n",
    "                    calculator = SpeedStatistics()\n",
    "                    gdf_wgs84[speed_field] = gdf_wgs84[speeds_fields[0]].apply(\n",
    "                        lambda x: calculator.calculate_stats(x)[0] if pd.notna(x) else None\n",
    "                    )\n",
    "                else:\n",
    "                    # If nothing else, use a default column\n",
    "                    speed_field = gdf_wgs84.columns[0]\n",
    "        \n",
    "        # Filter out NaN values for normalization\n",
    "        valid_speeds = gdf_wgs84[speed_field].dropna()\n",
    "        if len(valid_speeds) > 0:\n",
    "            norm = plt.Normalize(valid_speeds.min(), valid_speeds.max())\n",
    "        else:\n",
    "            # Fallback if no valid speed values\n",
    "            norm = plt.Normalize(0, 30)  # Reasonable default for km/h\n",
    "        \n",
    "        cmap = plt.cm.viridis\n",
    "\n",
    "        def style_callback(feature):\n",
    "            speed = feature['properties'][speed_field] if speed_field in feature['properties'] else None\n",
    "            color = '#808080' if pd.isna(speed) else mcolors.rgb2hex(cmap(norm(speed)))\n",
    "            return {'color': color, 'weight': 3, 'opacity': 0.7}\n",
    "\n",
    "    geo_json = ipyleaflet.GeoJSON(\n",
    "        data=json.loads(gdf_wgs84.to_json()),\n",
    "        style={'weight': 3, 'opacity': 0.7},\n",
    "        hover_style={'color': 'red', 'fillOpacity': 0.9},\n",
    "        style_callback=style_callback\n",
    "    )\n",
    "    m.add_layer(geo_json)\n",
    "    \n",
    "    # Add the original geometries if provided and not already included\n",
    "    if original_gdf is not None and 'is_original' not in gdf_wgs84.columns:\n",
    "        # Convert to same CRS\n",
    "        original_wgs84 = original_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "        # Add as a highlighted layer\n",
    "        original_json = ipyleaflet.GeoJSON(\n",
    "            data=json.loads(original_wgs84.to_json()),\n",
    "            style={\n",
    "                'color': '#FF0000',  # Red color\n",
    "                'weight': 5,         # Thicker line\n",
    "                'opacity': 1.0,      # Fully opaque\n",
    "                'dashArray': '5,5'   # Dashed line pattern\n",
    "            },\n",
    "            hover_style={\n",
    "                'color': '#FF00FF',  # Magenta on hover\n",
    "                'weight': 7\n",
    "            },\n",
    "            name='Original Reference'\n",
    "        )\n",
    "        m.add_layer(original_json)\n",
    "    \n",
    "    return m, draw_control\n",
    "\n",
    "# Create analysis widgets\n",
    "def create_analysis_widgets(gdf, m, draw_control):\n",
    "    \"\"\"Create analysis widgets with comparison mode\"\"\"\n",
    "    global ORIGINAL_GDF\n",
    "    ORIGINAL_GDF = gdf.copy()\n",
    "    \n",
    "    metrics, time_periods = get_available_metrics_and_times(gdf)\n",
    "    all_metrics = ['General Comparison'] + metrics\n",
    "    \n",
    "    # Create widgets\n",
    "    comparison_mode = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Comparison Mode',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    metric_selector = widgets.Dropdown(\n",
    "        options=all_metrics,\n",
    "        value=all_metrics[0],\n",
    "        description='Metric:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    time_selection1 = widgets.SelectMultiple(\n",
    "        options=time_periods,\n",
    "        description='Period 1:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': 'auto'}\n",
    "    )\n",
    "    \n",
    "    time_selection2 = widgets.Box([\n",
    "        widgets.SelectMultiple(\n",
    "            options=time_periods,\n",
    "            description='Period 2:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout={'width': 'auto'}\n",
    "        )\n",
    "    ], layout=widgets.Layout(display='none'))\n",
    "    \n",
    "    # Add segment type filter if 'is_original' column exists\n",
    "    if 'is_original' in gdf.columns:\n",
    "        segment_filter = widgets.Dropdown(\n",
    "            options=[('All Segments', 'all'), \n",
    "                     ('Original Segments Only', 'original'), \n",
    "                     ('Parallel Segments Only', 'parallel')],\n",
    "            value='all',\n",
    "            description='Segments:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "    else:\n",
    "        segment_filter = None\n",
    "    \n",
    "    analyze_selection_button = widgets.Button(\n",
    "        description='Analyze Selection',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    analyze_all_button = widgets.Button(\n",
    "        description='Analyze All',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    clear_button = widgets.Button(\n",
    "        description='Clear Selection',\n",
    "        button_style='warning'\n",
    "    )\n",
    "    \n",
    "    stats_output = widgets.Output()\n",
    "    plot_output = widgets.Output()\n",
    "    \n",
    "    # Toggle comparison mode\n",
    "    def on_comparison_toggle(change):\n",
    "        time_selection2.layout.display = 'flex' if change.new else 'none'\n",
    "    \n",
    "    comparison_mode.observe(on_comparison_toggle, names='value')\n",
    "\n",
    "    def analyze_data(gdf_to_analyze):\n",
    "        \"\"\"Analyze data based on selected metrics and time periods\"\"\"\n",
    "        selected_metric = metric_selector.value\n",
    "        selected_dates1 = time_selection1.value\n",
    "        selected_dates2 = time_selection2.children[0].value if comparison_mode.value else None\n",
    "        \n",
    "        # Filter by segment type if applicable\n",
    "        if segment_filter is not None:\n",
    "            filter_value = segment_filter.value\n",
    "            if filter_value == 'original':\n",
    "                gdf_to_analyze = gdf_to_analyze[gdf_to_analyze['is_original'] == True]\n",
    "            elif filter_value == 'parallel':\n",
    "                gdf_to_analyze = gdf_to_analyze[gdf_to_analyze['is_original'] == False]\n",
    "        \n",
    "        if not selected_dates1 or (comparison_mode.value and not selected_dates2):\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(\"Please select time periods for comparison\")\n",
    "            return\n",
    "                \n",
    "        with plot_output:\n",
    "            plot_output.clear_output()\n",
    "            result1 = process_temporal_data(gdf_to_analyze, selected_metric, selected_dates1)\n",
    "            result2 = process_temporal_data(gdf_to_analyze, selected_metric, selected_dates2) if comparison_mode.value else None\n",
    "            \n",
    "            if result1 is None or (comparison_mode.value and result2 is None):\n",
    "                print(\"No data available for selected combination\")\n",
    "                return\n",
    "            \n",
    "            if comparison_mode.value:\n",
    "                # Comparison mode visualizations\n",
    "                if selected_metric == 'General Comparison':\n",
    "                    fig = plt.figure(figsize=(20, 12))\n",
    "                    \n",
    "                    # Speed distributions\n",
    "                    plt.subplot(2, 4, 1)\n",
    "                    plt.bar(range(len(result1['speeds'])), result1['speeds'])\n",
    "                    plt.title(f'Speed Distribution\\n{\", \".join(selected_dates1)}')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                    plt.subplot(2, 4, 2)\n",
    "                    plt.bar(range(len(result2['speeds'])), result2['speeds'])\n",
    "                    plt.title(f'Speed Distribution\\n{\", \".join(selected_dates2)}')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                    # Other metrics side by side\n",
    "                    metrics = ['cv', 'ent', 'skew']\n",
    "                    for i, m in enumerate(metrics):\n",
    "                        plt.subplot(2, 4, i+3)\n",
    "                        plt.hist([result1[m], result2[m]], bins=30, label=[', '.join(selected_dates1), ', '.join(selected_dates2)])\n",
    "                        plt.title(f'{m.capitalize()} Distribution')\n",
    "                        plt.xlabel('Value')\n",
    "                        plt.ylabel('Count')\n",
    "                        plt.legend()\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    # Print comparison statistics\n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Create descriptive period labels\n",
    "                        period1_label = f\"{', '.join(selected_dates1)}\"\n",
    "                        period2_label = f\"{', '.join(selected_dates2)}\" if selected_dates2 else \"\"\n",
    "                        \n",
    "                        # Calculate route count sums for both periods\n",
    "                        route_count1 = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count1 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        route_count2 = 0\n",
    "                        for date in selected_dates2:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count2 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        # Use pre-calculated speed values if available\n",
    "                        if 'speed' in result1 and 'speed' in result2:\n",
    "                            speed1 = np.mean(result1['speed'])\n",
    "                            speed2 = np.mean(result2['speed'])\n",
    "                        else:\n",
    "                            # Fallback to calculating from histograms\n",
    "                            speed1 = sum(i * c for i, c in enumerate(result1['speeds'])) / sum(result1['speeds']) if sum(result1['speeds']) > 0 else 0\n",
    "                            speed2 = sum(i * c for i, c in enumerate(result2['speeds'])) / sum(result2['speeds']) if sum(result2['speeds']) > 0 else 0\n",
    "                        \n",
    "                        # Basic stats dictionary\n",
    "                        stats_dict = {\n",
    "                            'Period': [period1_label, period2_label],\n",
    "                            'Speed': [speed1, speed2],\n",
    "                            'CV': [np.mean(result1['cv']), np.mean(result2['cv'])],\n",
    "                            'Entropy': [np.mean(result1['ent']), np.mean(result2['ent'])],\n",
    "                            'Skewness': [np.mean(result1['skew']), np.mean(result2['skew'])],\n",
    "                            'Route Count': [route_count1, route_count2]\n",
    "                        }\n",
    "                        \n",
    "                        # Add quantiles\n",
    "                        if 'q1' in result1 and 'q1' in result2:\n",
    "                            stats_dict['Q1'] = [np.mean(result1['q1']), np.mean(result2['q1'])]\n",
    "                        if 'q2' in result1 and 'q2' in result2:\n",
    "                            stats_dict['Median'] = [np.mean(result1['q2']), np.mean(result2['q2'])]\n",
    "                        if 'q3' in result1 and 'q3' in result2:\n",
    "                            stats_dict['Q3'] = [np.mean(result1['q3']), np.mean(result2['q3'])]\n",
    "                        if 'iqr' in result1 and 'iqr' in result2:\n",
    "                            stats_dict['IQR'] = [np.mean(result1['iqr']), np.mean(result2['iqr'])]\n",
    "                        \n",
    "                        # Create the DataFrame with basic stats for display\n",
    "                        df_stats = pd.DataFrame(stats_dict).round(2)\n",
    "                        \n",
    "                        # Display human-readable format\n",
    "                        print(\"\\nComparison Statistics:\")\n",
    "                        print(df_stats.to_string(index=False))\n",
    "                        \n",
    "                        # For Excel copy-paste, combine main statistics and speed percentages in one section\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        \n",
    "                        # Combined headers for all statistics\n",
    "                        headers = ['Period', 'Speed', 'CV', 'Entropy', 'Skewness', 'Route Count']\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            headers.extend(['Q1', 'Median', 'Q3', 'IQR'])\n",
    "                        \n",
    "                        # Add speed percentage headers\n",
    "                        if all(key in result1 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']) and all(key in result2 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            headers.extend(['0-5 km/h', '5-15 km/h', '15-25 km/h', '25+ km/h'])\n",
    "                        \n",
    "                        print('\\t'.join(headers))\n",
    "                        \n",
    "                        # First data row with all metrics\n",
    "                        row1 = [period1_label, f\"{speed1:.2f}\", f\"{np.mean(result1['cv']):.2f}\", \n",
    "                                f\"{np.mean(result1['ent']):.2f}\", f\"{np.mean(result1['skew']):.2f}\",\n",
    "                                f\"{route_count1}\"]\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            row1.extend([f\"{np.mean(result1['q1']):.2f}\", f\"{np.mean(result1['q2']):.2f}\", \n",
    "                                        f\"{np.mean(result1['q3']):.2f}\", f\"{np.mean(result1['iqr']):.2f}\"])\n",
    "                        \n",
    "                        # Add speed percentages if available\n",
    "                        if all(key in result1 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            row1.extend([\n",
    "                                f\"{np.mean(result1['p_0_5']):.2f}\", \n",
    "                                f\"{np.mean(result1['p_5_15']):.2f}\", \n",
    "                                f\"{np.mean(result1['p_15_25']):.2f}\", \n",
    "                                f\"{np.mean(result1['p_25p']):.2f}\"\n",
    "                            ])\n",
    "                        \n",
    "                        print('\\t'.join(row1))\n",
    "                        \n",
    "                        # Second data row with all metrics\n",
    "                        row2 = [period2_label, f\"{speed2:.2f}\", f\"{np.mean(result2['cv']):.2f}\", \n",
    "                                f\"{np.mean(result2['ent']):.2f}\", f\"{np.mean(result2['skew']):.2f}\",\n",
    "                                f\"{route_count2}\"]\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            row2.extend([f\"{np.mean(result2['q1']):.2f}\", f\"{np.mean(result2['q2']):.2f}\", \n",
    "                                        f\"{np.mean(result2['q3']):.2f}\", f\"{np.mean(result2['iqr']):.2f}\"])\n",
    "                        \n",
    "                        # Add speed percentages if available\n",
    "                        if all(key in result2 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            row2.extend([\n",
    "                                f\"{np.mean(result2['p_0_5']):.2f}\", \n",
    "                                f\"{np.mean(result2['p_5_15']):.2f}\", \n",
    "                                f\"{np.mean(result2['p_15_25']):.2f}\", \n",
    "                                f\"{np.mean(result2['p_25p']):.2f}\"\n",
    "                            ])\n",
    "                        \n",
    "                        print('\\t'.join(row2))\n",
    "                        print(\"----------------------------------------\")\n",
    "                        \n",
    "                elif selected_metric == 'speeds':\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    ax1.bar(range(len(result1)), result1)\n",
    "                    ax1.set_title(f'Speed Distribution\\n{\", \".join(selected_dates1)}')\n",
    "                    ax1.set_xlabel('Speed (km/h)')\n",
    "                    ax1.set_ylabel('Count')\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    ax2.bar(range(len(result2)), result2)\n",
    "                    ax2.set_title(f'Speed Distribution\\n{\", \".join(selected_dates2)}')\n",
    "                    ax2.set_xlabel('Speed (km/h)')\n",
    "                    ax2.set_ylabel('Count')\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route counts\n",
    "                        route_count1 = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count1 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        route_count2 = 0\n",
    "                        for date in selected_dates2:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count2 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        print(f\"\\n{', '.join(selected_dates1)}:\")\n",
    "                        print(f\"Total measurements: {sum(result1):,}\")\n",
    "                        print(f\"Route count: {route_count1}\")\n",
    "                        mean_speed1 = sum(i * count for i, count in enumerate(result1)) / sum(result1) if sum(result1) > 0 else 0\n",
    "                        print(f\"Mean speed: {mean_speed1:.2f} km/h\")\n",
    "                        \n",
    "                        print(f\"\\n{', '.join(selected_dates2)}:\")\n",
    "                        print(f\"Total measurements: {sum(result2):,}\")\n",
    "                        print(f\"Route count: {route_count2}\")\n",
    "                        mean_speed2 = sum(i * count for i, count in enumerate(result2)) / sum(result2) if sum(result2) > 0 else 0\n",
    "                        print(f\"Mean speed: {mean_speed2:.2f} km/h\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Period', 'Measurements', 'Route Count', 'Mean Speed']))\n",
    "                        print(f\"{', '.join(selected_dates1)}\\t{sum(result1)}\\t{route_count1}\\t{mean_speed1:.2f}\")\n",
    "                        print(f\"{', '.join(selected_dates2)}\\t{sum(result2)}\\t{route_count2}\\t{mean_speed2:.2f}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                \n",
    "                else:\n",
    "                    # Other metrics comparison\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    ax1.hist(result1, bins=30)\n",
    "                    ax1.set_title(f'{selected_metric.capitalize()} Distribution\\n{\", \".join(selected_dates1)}')\n",
    "                    ax1.set_xlabel('Value')\n",
    "                    ax1.set_ylabel('Count')\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    ax2.hist(result2, bins=30)\n",
    "                    ax2.set_title(f'{selected_metric.capitalize()} Distribution\\n{\", \".join(selected_dates2)}')\n",
    "                    ax2.set_xlabel('Value')\n",
    "                    ax2.set_ylabel('Count')\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route counts\n",
    "                        route_count1 = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count1 += gdf_to_analyze[route_count_col].sum()\n",
    "                                \n",
    "                        route_count2 = 0\n",
    "                        for date in selected_dates2:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count2 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        print(f\"\\n{', '.join(selected_dates1)} {selected_metric}: {np.mean(result1):.2f}\")\n",
    "                        print(f\"Route count: {route_count1}\")\n",
    "                        print(f\"{', '.join(selected_dates2)} {selected_metric}: {np.mean(result2):.2f}\")\n",
    "                        print(f\"Route count: {route_count2}\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Period', selected_metric, 'Route Count']))\n",
    "                        print(f\"{', '.join(selected_dates1)}\\t{np.mean(result1):.2f}\\t{route_count1}\")\n",
    "                        print(f\"{', '.join(selected_dates2)}\\t{np.mean(result2):.2f}\\t{route_count2}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "            else:\n",
    "                # Single period visualizations\n",
    "                if selected_metric == 'General Comparison':\n",
    "                    plt.figure(figsize=(15, 10))\n",
    "                    \n",
    "                    plt.subplot(2, 2, 1)\n",
    "                    plt.bar(range(len(result1['speeds'])), result1['speeds'])\n",
    "                    plt.title(f'Speed Distribution\\n{\", \".join(selected_dates1)}')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                    metrics = ['cv', 'ent', 'skew']\n",
    "                    for i, m in enumerate(metrics, 2):\n",
    "                        plt.subplot(2, 2, i)\n",
    "                        plt.hist(result1[m], bins=30)\n",
    "                        plt.title(f'{m.capitalize()} Distribution')\n",
    "                        plt.xlabel('Value')\n",
    "                        plt.ylabel('Count')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route count sum\n",
    "                        route_count = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        # Use pre-calculated speed values if available\n",
    "                        if 'speed' in result1:\n",
    "                            speed = np.mean(result1['speed'])\n",
    "                        else:\n",
    "                            # Fallback to calculating from histograms\n",
    "                            speed = sum(i * c for i, c in enumerate(result1['speeds'])) / sum(result1['speeds']) if sum(result1['speeds']) > 0 else 0\n",
    "                        \n",
    "                        # Basic stats dictionary (using traditional format for human readability)\n",
    "                        stats_dict = {\n",
    "                            'Speed': speed,\n",
    "                            'CV': np.mean(result1['cv']),\n",
    "                            'Entropy': np.mean(result1['ent']),\n",
    "                            'Skewness': np.mean(result1['skew']),\n",
    "                            'Route Count': route_count\n",
    "                        }\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'q1' in result1:\n",
    "                            stats_dict['Q1'] = np.mean(result1['q1'])\n",
    "                        if 'q2' in result1:\n",
    "                            stats_dict['Median'] = np.mean(result1['q2'])\n",
    "                        if 'q3' in result1:\n",
    "                            stats_dict['Q3'] = np.mean(result1['q3'])\n",
    "                        if 'iqr' in result1:\n",
    "                            stats_dict['IQR'] = np.mean(result1['iqr'])\n",
    "                        \n",
    "                        # Display human-readable format\n",
    "                        print(\"\\nMetric Statistics:\")\n",
    "                        df_stats = pd.DataFrame([stats_dict]).round(2)\n",
    "                        print(df_stats.to_string())\n",
    "                        \n",
    "                        # For Excel copy-paste, combine all statistics in one section\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        \n",
    "                        # Create a comprehensive table with metrics and speed percentages in one section\n",
    "                        # Start with the main metrics\n",
    "                        metrics = ['Speed', 'CV', 'Entropy', 'Skewness', 'Route Count']\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            metrics.extend(['Q1', 'Median', 'Q3', 'IQR'])\n",
    "                        \n",
    "                        # Add speed percentage metrics if available\n",
    "                        if all(key in result1 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            metrics.extend(['0-5 km/h', '5-15 km/h', '15-25 km/h', '25+ km/h'])\n",
    "                        \n",
    "                        # Create the header\n",
    "                        print('\\t'.join(['Metric', 'Value']))\n",
    "                        \n",
    "                        # Print values for each metric\n",
    "                        for metric in metrics:\n",
    "                            if metric == 'Speed':\n",
    "                                value = f\"{stats_dict['Speed']:.2f}\"\n",
    "                            elif metric == 'CV':\n",
    "                                value = f\"{stats_dict['CV']:.2f}\"\n",
    "                            elif metric == 'Entropy':\n",
    "                                value = f\"{stats_dict['Entropy']:.2f}\"\n",
    "                            elif metric == 'Skewness':\n",
    "                                value = f\"{stats_dict['Skewness']:.2f}\"\n",
    "                            elif metric == 'Route Count':\n",
    "                                value = f\"{stats_dict['Route Count']}\"\n",
    "                            # Quantiles\n",
    "                            elif metric in ['Q1', 'Median', 'Q3', 'IQR'] and metric in stats_dict:\n",
    "                                value = f\"{stats_dict[metric]:.2f}\"\n",
    "                            # Speed ranges    \n",
    "                            elif metric == '0-5 km/h' and 'p_0_5' in result1:\n",
    "                                value = f\"{np.mean(result1['p_0_5']):.2f}\"\n",
    "                            elif metric == '5-15 km/h' and 'p_5_15' in result1:\n",
    "                                value = f\"{np.mean(result1['p_5_15']):.2f}\" \n",
    "                            elif metric == '15-25 km/h' and 'p_15_25' in result1:\n",
    "                                value = f\"{np.mean(result1['p_15_25']):.2f}\"\n",
    "                            elif metric == '25+ km/h' and 'p_25p' in result1:\n",
    "                                value = f\"{np.mean(result1['p_25p']):.2f}\"\n",
    "                            else:\n",
    "                                value = \"N/A\"\n",
    "                                \n",
    "                            print(f\"{metric}\\t{value}\")\n",
    "                            \n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                elif selected_metric == 'speeds':\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.bar(range(len(result1)), result1)\n",
    "                    plt.title(f'Speed Distribution\\n({\", \".join(selected_dates1)})')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route count\n",
    "                        route_count = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count += gdf_to_analyze[route_count_col].sum()\n",
    "                                \n",
    "                        print(f\"Total measurements: {sum(result1):,}\")\n",
    "                        print(f\"Route count: {route_count}\")\n",
    "                        mean_speed = sum(i * count for i, count in enumerate(result1)) / sum(result1) if sum(result1) > 0 else 0\n",
    "                        print(f\"Mean speed: {mean_speed:.2f} km/h\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Measurements', 'Route Count', 'Mean Speed']))\n",
    "                        print(f\"{sum(result1)}\\t{route_count}\\t{mean_speed:.2f}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                \n",
    "                else:\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.hist(result1, bins=30)\n",
    "                    plt.title(f'{selected_metric.capitalize()} Distribution\\n({\", \".join(selected_dates1)})')\n",
    "                    plt.xlabel('Value')\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route count\n",
    "                        route_count = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        print(f\"\\n{selected_metric}: {np.mean(result1):.2f}\")\n",
    "                        print(f\"Route count: {route_count}\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Metric', 'Value', 'Route Count']))\n",
    "                        print(f\"{selected_metric}\\t{np.mean(result1):.2f}\\t{route_count}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "    def on_analyze_selection_click(b):\n",
    "        try:\n",
    "            drawn_features = draw_control.data\n",
    "            if not drawn_features:\n",
    "                with stats_output:\n",
    "                    stats_output.clear_output()\n",
    "                    print(\"Please draw at least one shape\")\n",
    "                return\n",
    "            \n",
    "            selection_shapes = [shape(feature['geometry']) for feature in drawn_features]\n",
    "            combined_shape = selection_shapes[0]\n",
    "            for s in selection_shapes[1:]:\n",
    "                combined_shape = combined_shape.union(s)\n",
    "            \n",
    "            gdf_wgs84 = ORIGINAL_GDF.to_crs('EPSG:4326')\n",
    "            selected_gdf = gdf_wgs84[gdf_wgs84.intersects(combined_shape)]\n",
    "            \n",
    "            if len(selected_gdf) == 0:\n",
    "                with stats_output:\n",
    "                    stats_output.clear_output()\n",
    "                    print(\"No features found in selection\")\n",
    "                return\n",
    "            \n",
    "            analyze_data(selected_gdf)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(f\"Error analyzing selection: {str(e)}\")\n",
    "\n",
    "    def on_analyze_all_click(b):\n",
    "        try:\n",
    "            analyze_data(ORIGINAL_GDF.to_crs('EPSG:4326'))\n",
    "        except Exception as e:\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(f\"Error analyzing all features: {str(e)}\")\n",
    "    \n",
    "    def on_clear_click(b):\n",
    "        draw_control.clear()\n",
    "        stats_output.clear_output()\n",
    "        plot_output.clear_output()\n",
    "        \n",
    "    analyze_selection_button.on_click(on_analyze_selection_click)\n",
    "    analyze_all_button.on_click(on_analyze_all_click)\n",
    "    clear_button.on_click(on_clear_click)\n",
    "    \n",
    "    # Arrange widgets\n",
    "    control_widgets = [\n",
    "        widgets.HTML(value=\"<b>Analysis Controls:</b>\"),\n",
    "        comparison_mode,\n",
    "        metric_selector,\n",
    "        time_selection1,\n",
    "        time_selection2\n",
    "    ]\n",
    "    \n",
    "    # Add segment filter if available\n",
    "    if segment_filter is not None:\n",
    "        control_widgets.insert(3, segment_filter)\n",
    "    \n",
    "    control_widgets.append(widgets.HBox([analyze_selection_button, analyze_all_button, clear_button]))\n",
    "    control_widgets.append(stats_output)\n",
    "    control_widgets.append(plot_output)\n",
    "    \n",
    "    controls = widgets.VBox(control_widgets)\n",
    "    \n",
    "    return controls\n",
    "\n",
    "def initialize_analysis(gdf, original_gdf=None):\n",
    "    \"\"\"Initialize the analysis environment with optional original geometries\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The segments to analyze (gdf3)\n",
    "    original_gdf : GeoDataFrame, optional\n",
    "        The original reference geometry\n",
    "    \"\"\"\n",
    "    # Create map and controls\n",
    "    # m, draw_control = create_interactive_map(gdf, original_gdf)\n",
    "    m, draw_control = create_interactive_map(gdf)\n",
    "    controls = create_analysis_widgets(gdf, m, draw_control)\n",
    "    \n",
    "    # Display the map and controls\n",
    "    display(m)\n",
    "    display(controls)\n",
    "    \n",
    "    # Don't return anything to avoid \"None\" output\n",
    "    return None\n",
    "\n",
    "# Initialize the analysis tools with your existing gdf3 data\n",
    "# Here's how you would use it with original gdf overlay:\n",
    "# initialize_analysis(gdf3, original_gdf=gdf)\n",
    "\n",
    "# Or without original reference geometry:\n",
    "initialize_analysis(gdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee01c8c-0678-42d9-9ff2-2e0628d14d1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3552588496c0483dbd010b2b202598d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[np.float64(52.505145999999996), np.float64(13.408959)], controls=(ZoomControl(options=['position',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace485d2065d4ac797790ac69f63cfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Analysis Controls:</b>'), Checkbox(value=False, description='Comparison Mode', s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3 v4.3 final: [FULL] Analyze values by month interactively with length weighting + processing statistical values on the fly\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipyleaflet\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import json\n",
    "import warnings\n",
    "from shapely.geometry import Point, shape\n",
    "from scipy.stats import entropy\n",
    "from typing import Optional, Tuple\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Speed Statistics Calculator Class\n",
    "class SpeedStatistics:\n",
    "    \"\"\"Class to handle speed histogram statistics calculations\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bins: int = 32):\n",
    "        self.n_bins = n_bins\n",
    "        self.bin_edges = np.arange(0, n_bins + 1)\n",
    "        self.midpoints = (self.bin_edges[:-1] + self.bin_edges[1:]) / 2\n",
    "    \n",
    "    def parse_histogram(self, binned_speeds) -> Optional[np.ndarray]:\n",
    "        \"\"\"Safely parse histogram string to numpy array\"\"\"\n",
    "        if binned_speeds is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(binned_speeds, (list, np.ndarray)):\n",
    "            # If already a list or array, convert to numpy array\n",
    "            return np.array(binned_speeds)\n",
    "            \n",
    "        if pd.isna(binned_speeds):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            if isinstance(binned_speeds, str):\n",
    "                hist = eval(binned_speeds)\n",
    "            else:\n",
    "                hist = binned_speeds\n",
    "                \n",
    "            hist_array = np.array(hist)\n",
    "            if len(hist_array) != self.n_bins:\n",
    "                warnings.warn(f\"Expected {self.n_bins} bins, got {len(hist_array)}\")\n",
    "                return None\n",
    "            return hist_array\n",
    "            \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error parsing histogram: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_stats(self, binned_speeds, length_m: float = 1.0) -> Tuple:\n",
    "        \"\"\"Calculate comprehensive statistics for a speed histogram with length weighting\"\"\"\n",
    "        # Parse histogram\n",
    "        hist_array = self.parse_histogram(binned_speeds)\n",
    "        if hist_array is None:\n",
    "            return (np.nan,) * 15\n",
    "        \n",
    "        # Apply length weighting\n",
    "        weighted_hist = hist_array * length_m\n",
    "        total_count = weighted_hist.sum()\n",
    "        if total_count == 0:\n",
    "            return (np.nan,) * 15\n",
    "            \n",
    "        # Calculate probabilities\n",
    "        probs = weighted_hist / total_count\n",
    "        \n",
    "        # Basic statistics\n",
    "        weighted_mean = np.sum(self.midpoints * probs)\n",
    "        weighted_var = np.sum(((self.midpoints - weighted_mean) ** 2) * probs)\n",
    "        weighted_std = np.sqrt(weighted_var)\n",
    "        \n",
    "        # Higher moments\n",
    "        if weighted_std > 0:\n",
    "            standardized_moments = (self.midpoints - weighted_mean) / weighted_std\n",
    "            skewness = np.sum((standardized_moments ** 3) * probs)\n",
    "            kurtosis = np.sum((standardized_moments ** 4) * probs) - 3\n",
    "        else:\n",
    "            skewness = np.nan\n",
    "            kurtosis = np.nan\n",
    "        \n",
    "        # Calculate CV\n",
    "        cv = (weighted_std / weighted_mean * 100) if weighted_mean > 0 else np.nan\n",
    "        \n",
    "        # Entropy (only for non-zero probabilities)\n",
    "        valid_probs = probs[probs > 0]\n",
    "        data_entropy = entropy(valid_probs, base=2) if len(valid_probs) > 0 else np.nan\n",
    "        \n",
    "        # Quantiles\n",
    "        cum_probs = np.cumsum(weighted_hist) / total_count\n",
    "        q1 = np.interp(0.25, cum_probs, self.midpoints)\n",
    "        q2 = np.interp(0.50, cum_probs, self.midpoints)\n",
    "        q3 = np.interp(0.75, cum_probs, self.midpoints)\n",
    "        q4 = np.interp(1.00, cum_probs, self.midpoints)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Speed range percentages\n",
    "        pct_0_5 = np.interp(5, self.midpoints, cum_probs, left=0) * 100\n",
    "        pct_5_15 = (np.interp(15, self.midpoints, cum_probs) - \n",
    "                   np.interp(5, self.midpoints, cum_probs)) * 100\n",
    "        pct_15_25 = (np.interp(25, self.midpoints, cum_probs) - \n",
    "                    np.interp(15, self.midpoints, cum_probs)) * 100\n",
    "        pct_25_plus = (1 - np.interp(25, self.midpoints, cum_probs, right=1)) * 100\n",
    "        \n",
    "        return tuple(round(x, 2) if not pd.isna(x) else x for x in [\n",
    "            weighted_mean, weighted_std, cv, skewness, kurtosis,\n",
    "            data_entropy, iqr, q1, q2, q3, q4,\n",
    "            pct_0_5, pct_5_15, pct_15_25, pct_25_plus\n",
    "        ])\n",
    "\n",
    "# Helper function to format date ranges\n",
    "def format_date_range(dates):\n",
    "    \"\"\"Format a list of dates into a range string\"\"\"\n",
    "    if not dates:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to datetime and sort\n",
    "    date_objs = sorted([pd.to_datetime(date) for date in dates])\n",
    "    \n",
    "    # Get the first and last date\n",
    "    first_date = date_objs[0]\n",
    "    last_date = date_objs[-1]\n",
    "    \n",
    "    # Format as \"YY-MM to YY-MM\"\n",
    "    return f\"{first_date.strftime('%y-%m')} to {last_date.strftime('%y-%m')}\"\n",
    "\n",
    "# Get available metrics and times for analysis\n",
    "def get_available_metrics_and_times(gdf):\n",
    "    \"\"\"Get available metrics and time periods from the data\"\"\"\n",
    "    # Find all columns that match the pattern YY-MM_metric\n",
    "    time_cols = sorted([col for col in gdf.columns \n",
    "                        if len(col.split('-')) > 1 and \n",
    "                        len(col.split('-')[0]) == 2 and \n",
    "                        len(col.split('-')[1].split('_')[0]) == 2])\n",
    "    \n",
    "    metrics = set()\n",
    "    time_periods = set()\n",
    "    \n",
    "    for col in time_cols:\n",
    "        parts = col.split('_', 1)\n",
    "        if len(parts) > 1:\n",
    "            yy_mm = parts[0]\n",
    "            metric = parts[1]\n",
    "            \n",
    "            yy, mm = yy_mm.split('-')\n",
    "            time_periods.add(f\"20{yy}-{mm}\")\n",
    "            metrics.add(metric)\n",
    "    \n",
    "    # Define core metrics (even if they don't exist in the data)\n",
    "    # We'll calculate these on-the-fly from speeds histograms if needed\n",
    "    core_metrics = {'speeds', 'speed', 'route_count', 'cv', 'ent', 'skew', 'kurt', 'std', 'iqr', \n",
    "                    'q1', 'q2', 'q3', 'q4', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p'}\n",
    "    \n",
    "    # Only include metrics that have corresponding speed or speeds histograms\n",
    "    available_metrics = {'route_count'}\n",
    "    for period in time_periods:\n",
    "        yy = period.split('-')[0][-2:]\n",
    "        mm = period.split('-')[1]\n",
    "        if f\"{yy}-{mm}_speed\" in gdf.columns:\n",
    "            available_metrics.add('speed')\n",
    "        if f\"{yy}-{mm}_speeds\" in gdf.columns:\n",
    "            available_metrics.add('speeds')\n",
    "        if f\"{yy}-{mm}_cv\" in gdf.columns:\n",
    "            available_metrics.add('cv')\n",
    "        if f\"{yy}-{mm}_ent\" in gdf.columns:\n",
    "            available_metrics.add('ent')\n",
    "        if f\"{yy}-{mm}_skew\" in gdf.columns:\n",
    "            available_metrics.add('skew')\n",
    "    \n",
    "    # Add derived metrics that we can calculate if not already in the data\n",
    "    derived_metrics = {'speed', 'cv', 'ent', 'skew', 'kurt', 'std', 'iqr', \n",
    "                      'q1', 'q2', 'q3', 'q4', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p'}\n",
    "    \n",
    "    # Merge available and derived metrics\n",
    "    filtered_metrics = available_metrics.union(derived_metrics)\n",
    "    \n",
    "    return sorted(list(filtered_metrics)), sorted(list(time_periods))\n",
    "\n",
    "# Process temporal data for analysis\n",
    "def process_temporal_data(gdf, metric, selected_dates):\n",
    "    \"\"\"Process temporal data for given metric and dates with length weighting\"\"\"\n",
    "    if not metric or not selected_dates:\n",
    "        return None\n",
    "        \n",
    "    # Initialize the speed statistics calculator\n",
    "    calculator = SpeedStatistics()\n",
    "    \n",
    "    if metric == 'General Comparison':\n",
    "        # Expanded metrics list to include quantiles and speed ranges\n",
    "        metrics = ['speeds', 'cv', 'ent', 'skew', 'q1', 'q2', 'q3', 'iqr', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p']\n",
    "        results = {}\n",
    "        \n",
    "        # First get all the pre-calculated speed values\n",
    "        speed_values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speed_col = f\"{year}-{month}_speed\"\n",
    "            if speed_col in gdf.columns:\n",
    "                values = gdf[speed_col].dropna().tolist()\n",
    "                speed_values.extend(values)\n",
    "        \n",
    "        if speed_values:\n",
    "            results['speed'] = speed_values\n",
    "            \n",
    "        # For speeds histograms, sum them with length weighting\n",
    "        histograms = []\n",
    "        lengths = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speeds_col = f\"{year}-{month}_speeds\"\n",
    "            if speeds_col in gdf.columns:\n",
    "                for idx, hist_str in enumerate(gdf[speeds_col]):\n",
    "                    if pd.notna(hist_str):\n",
    "                        hist = eval(hist_str) if isinstance(hist_str, str) else hist_str\n",
    "                        histograms.append(hist)\n",
    "                        lengths.append(gdf.iloc[idx]['length_m'] if 'length_m' in gdf.columns else 1.0)\n",
    "        \n",
    "        if histograms:\n",
    "            # First ensure all histograms have the same length (should be 32 based on SpeedStatistics class)\n",
    "            expected_length = 32\n",
    "            uniform_histograms = []\n",
    "            \n",
    "            for hist, length in zip(histograms, lengths):\n",
    "                # Check if the histogram has the expected length\n",
    "                if len(hist) == expected_length:\n",
    "                    uniform_histograms.append(np.array(hist) * length)\n",
    "                else:\n",
    "                    # Log warning and skip or pad the histogram\n",
    "                    warnings.warn(f\"Histogram with unexpected length {len(hist)} found. Expected {expected_length}.\")\n",
    "                    # Option 1: Skip this histogram\n",
    "                    # continue\n",
    "                    \n",
    "                    # Option 2: Pad or truncate the histogram to expected length\n",
    "                    if len(hist) < expected_length:\n",
    "                        # Pad with zeros\n",
    "                        padded_hist = np.pad(hist, (0, expected_length - len(hist)), 'constant')\n",
    "                        uniform_histograms.append(padded_hist * length)\n",
    "                    else:\n",
    "                        # Truncate\n",
    "                        uniform_histograms.append(np.array(hist[:expected_length]) * length)\n",
    "    \n",
    "            # Only sum if we have valid histograms\n",
    "            if uniform_histograms:\n",
    "                # Convert to numpy array first to ensure proper shape for summation\n",
    "                uniform_array = np.array(uniform_histograms)\n",
    "                results['speeds'] = np.sum(uniform_array, axis=0)\n",
    "            else:\n",
    "                # If no valid histograms, just create an empty one\n",
    "                results['speeds'] = np.zeros(expected_length)\n",
    "            \n",
    "        # Metric index in stats tuple\n",
    "        metric_index = {\n",
    "            'cv': 2,       # coefficient of variation\n",
    "            'skew': 3,     # skewness\n",
    "            'ent': 5,      # entropy\n",
    "            'iqr': 6,      # interquartile range\n",
    "            'q1': 7,       # first quartile\n",
    "            'q2': 8,       # median\n",
    "            'q3': 9,       # third quartile\n",
    "            'p_0_5': 11,   # % speeds 0-5 km/h\n",
    "            'p_5_15': 12,  # % speeds 5-15 km/h\n",
    "            'p_15_25': 13, # % speeds 15-25 km/h\n",
    "            'p_25p': 14    # % speeds 25+ km/h\n",
    "        }\n",
    "        \n",
    "        # Try using pre-calculated metrics first, then calculate from histograms if needed\n",
    "        for m in metrics:\n",
    "            if m == 'speeds':\n",
    "                continue  # Already handled above\n",
    "                \n",
    "            values = []\n",
    "            for date in selected_dates:\n",
    "                year = date.split('-')[0][-2:]\n",
    "                month = date.split('-')[1]\n",
    "                metric_col = f\"{year}-{month}_{m}\"\n",
    "                if metric_col in gdf.columns:\n",
    "                    metric_values = gdf[metric_col].dropna().tolist()\n",
    "                    values.extend(metric_values)\n",
    "            \n",
    "            if values:\n",
    "                results[m] = values\n",
    "            elif histograms and m in metric_index:\n",
    "                # Calculate from histograms if we don't have pre-calculated values\n",
    "                calculated_values = []\n",
    "                for hist, length in zip(histograms, lengths):\n",
    "                    stats = calculator.calculate_stats(hist, length)\n",
    "                    if not pd.isna(stats[metric_index[m]]):\n",
    "                        calculated_values.append(stats[metric_index[m]])\n",
    "                \n",
    "                if calculated_values:\n",
    "                    results[m] = calculated_values\n",
    "        \n",
    "        return results if results else None\n",
    "    \n",
    "    elif metric == 'speed':\n",
    "        # Try to use pre-calculated speed values first\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_speed\"\n",
    "            if col in gdf.columns:\n",
    "                values.extend(gdf[col].dropna().tolist())\n",
    "        \n",
    "        if values:\n",
    "            return values\n",
    "        \n",
    "        # If no pre-calculated values, calculate from histograms\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speeds_col = f\"{year}-{month}_speeds\"\n",
    "            \n",
    "            if speeds_col in gdf.columns:\n",
    "                for hist_str in gdf[speeds_col]:\n",
    "                    if pd.notna(hist_str):\n",
    "                        stats = calculator.calculate_stats(hist_str, gdf.loc[gdf[speeds_col] == hist_str, 'length_m'].values[0] if 'length_m' in gdf.columns else 1.0)\n",
    "                        if not pd.isna(stats[0]):  # Mean speed is the first value\n",
    "                            values.append(stats[0])\n",
    "        \n",
    "        return values if values else None\n",
    "    \n",
    "    elif metric == 'speeds':\n",
    "        # Sum all histograms for the speeds metric with length weighting\n",
    "        histograms = []\n",
    "        lengths = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_speeds\"\n",
    "            if col in gdf.columns:\n",
    "                for idx, hist_str in enumerate(gdf[col]):\n",
    "                    if pd.notna(hist_str):\n",
    "                        hist = eval(hist_str) if isinstance(hist_str, str) else hist_str\n",
    "                        histograms.append(hist)\n",
    "                        lengths.append(gdf.iloc[idx]['length_m'] if 'length_m' in gdf.columns else 1.0)\n",
    "        return np.sum([hist * length for hist, length in zip(histograms, lengths)], axis=0) if histograms else None\n",
    "    \n",
    "    elif metric in ['cv', 'ent', 'skew', 'kurt', 'std', 'iqr', \n",
    "                    'q1', 'q2', 'q3', 'q4', 'p_0_5', 'p_5_15', 'p_15_25', 'p_25p']:\n",
    "        # First try to get pre-calculated values\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_{metric}\"\n",
    "            if col in gdf.columns:\n",
    "                values.extend(gdf[col].dropna().tolist())\n",
    "        \n",
    "        if values:\n",
    "            return values\n",
    "        \n",
    "        # If no pre-calculated values, calculate from histograms\n",
    "        metric_index = {\n",
    "            'speed': 0,    # mean\n",
    "            'std': 1,      # standard deviation\n",
    "            'cv': 2,       # coefficient of variation  \n",
    "            'skew': 3,     # skewness\n",
    "            'kurt': 4,     # kurtosis\n",
    "            'ent': 5,      # entropy\n",
    "            'iqr': 6,      # interquartile range\n",
    "            'q1': 7,       # first quartile\n",
    "            'q2': 8,       # median\n",
    "            'q3': 9,       # third quartile\n",
    "            'q4': 10,      # maximum\n",
    "            'p_0_5': 11,   # % speeds 0-5 km/h\n",
    "            'p_5_15': 12,  # % speeds 5-15 km/h\n",
    "            'p_15_25': 13, # % speeds 15-25 km/h\n",
    "            'p_25p': 14    # % speeds 25+ km/h\n",
    "        }\n",
    "        \n",
    "        index = metric_index.get(metric, 0)\n",
    "        values = []\n",
    "        \n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            speeds_col = f\"{year}-{month}_speeds\"\n",
    "            \n",
    "            if speeds_col in gdf.columns:\n",
    "                for hist_str in gdf[speeds_col]:\n",
    "                    if pd.notna(hist_str):\n",
    "                        stats = calculator.calculate_stats(hist_str, gdf.loc[gdf[speeds_col] == hist_str, 'length_m'].values[0] if 'length_m' in gdf.columns else 1.0)\n",
    "                        if not pd.isna(stats[index]):\n",
    "                            values.append(stats[index])\n",
    "        \n",
    "        return values if values else None\n",
    "    \n",
    "    else:\n",
    "        # For other metrics like route_count that may already exist\n",
    "        values = []\n",
    "        for date in selected_dates:\n",
    "            year = date.split('-')[0][-2:]\n",
    "            month = date.split('-')[1]\n",
    "            col = f\"{year}-{month}_{metric}\"\n",
    "            if col in gdf.columns:\n",
    "                values.extend(gdf[col].dropna().tolist())\n",
    "        return values if values else None\n",
    "\n",
    "# Create interactive map\n",
    "def create_interactive_map(gdf, original_gdf=None):\n",
    "    \"\"\"Create interactive map with draw controls\"\"\"\n",
    "    gdf_wgs84 = gdf.to_crs('EPSG:4326')\n",
    "    center_lat = gdf_wgs84.total_bounds[[1,3]].mean()\n",
    "    center_lon = gdf_wgs84.total_bounds[[0,2]].mean()\n",
    "    \n",
    "    m = ipyleaflet.Map(\n",
    "        center=(center_lat, center_lon),\n",
    "        zoom=14,\n",
    "        scroll_wheel_zoom=True\n",
    "    )\n",
    "    \n",
    "    draw_control = ipyleaflet.DrawControl(\n",
    "        polygon={\"shapeOptions\": {\"color\": \"#ff0000\"}},\n",
    "        rectangle={\"shapeOptions\": {\"color\": \"#ff0000\"}},\n",
    "        circle={\"shapeOptions\": {\"color\": \"#ff0000\"}},\n",
    "        circlemarker={},\n",
    "        polyline={},\n",
    "        marker={}\n",
    "    )\n",
    "    m.add_control(draw_control)\n",
    "    \n",
    "    # Style based on segment type if 'is_original' column exists\n",
    "    if 'is_original' in gdf_wgs84.columns:\n",
    "        # Speed field for coloring\n",
    "        speed_field = '2304-2412_speed'\n",
    "        if speed_field not in gdf_wgs84.columns:\n",
    "            speed_fields = [col for col in gdf_wgs84.columns if col.endswith('_speed')]\n",
    "            if speed_fields:\n",
    "                speed_field = speed_fields[0]\n",
    "            else:\n",
    "                speed_field = None\n",
    "        \n",
    "        # Setup color normalization for speed values if available\n",
    "        if speed_field:\n",
    "            valid_speeds = gdf_wgs84[gdf_wgs84['is_original'] == False][speed_field].dropna()\n",
    "            if len(valid_speeds) > 0:\n",
    "                norm = plt.Normalize(valid_speeds.min(), valid_speeds.max())\n",
    "            else:\n",
    "                norm = plt.Normalize(0, 30)\n",
    "            cmap = plt.cm.viridis\n",
    "        \n",
    "        def style_callback(feature):\n",
    "            # Check if this is an original segment\n",
    "            is_original = feature['properties'].get('is_original', False)\n",
    "            \n",
    "            if is_original:\n",
    "                return {\n",
    "                    'color': '#FF0000',  # Red for original segments\n",
    "                    'weight': 5,\n",
    "                    'opacity': 1.0,\n",
    "                    'dashArray': '5,5'  # Dashed line for original segments\n",
    "                }\n",
    "            else:\n",
    "                # For parallel segments, use speed-based coloring if available\n",
    "                if speed_field and speed_field in feature['properties']:\n",
    "                    speed = feature['properties'][speed_field]\n",
    "                    if pd.notna(speed):\n",
    "                        color = mcolors.rgb2hex(cmap(norm(speed)))\n",
    "                    else:\n",
    "                        color = '#808080'  # Gray for unknown speed\n",
    "                else:\n",
    "                    color = '#0000FF'  # Blue for parallel segments with no speed data\n",
    "                    \n",
    "                return {\n",
    "                    'color': color,\n",
    "                    'weight': 3,\n",
    "                    'opacity': 0.7\n",
    "                }\n",
    "    else:\n",
    "        # Use the standard speed-based styling\n",
    "        # Use the new field name as specified\n",
    "        speed_field = '2304-2412_speed'\n",
    "        \n",
    "        # If speed field doesn't exist, calculate it from the speed histogram\n",
    "        if speed_field not in gdf_wgs84.columns and '2304-2412_speeds' in gdf_wgs84.columns:\n",
    "            calculator = SpeedStatistics()\n",
    "            gdf_wgs84[speed_field] = gdf_wgs84['2304-2412_speeds'].apply(\n",
    "                lambda x: calculator.calculate_stats(x)[0] if pd.notna(x) else None\n",
    "            )\n",
    "        \n",
    "        # Fallback options if still not available\n",
    "        if speed_field not in gdf_wgs84.columns:\n",
    "            # Try to find any existing speed field\n",
    "            speed_fields = [col for col in gdf_wgs84.columns if col.endswith('_speed')]\n",
    "            if speed_fields:\n",
    "                speed_field = speed_fields[0]\n",
    "            else:\n",
    "                # Or calculate from any speeds histogram field\n",
    "                speeds_fields = [col for col in gdf_wgs84.columns if col.endswith('_speeds')]\n",
    "                if speeds_fields:\n",
    "                    speed_field = speeds_fields[0].replace('_speeds', '_speed')\n",
    "                    calculator = SpeedStatistics()\n",
    "                    gdf_wgs84[speed_field] = gdf_wgs84[speeds_fields[0]].apply(\n",
    "                        lambda x: calculator.calculate_stats(x)[0] if pd.notna(x) else None\n",
    "                    )\n",
    "                else:\n",
    "                    # If nothing else, use a default column\n",
    "                    speed_field = gdf_wgs84.columns[0]\n",
    "        \n",
    "        # Filter out NaN values for normalization\n",
    "        valid_speeds = gdf_wgs84[speed_field].dropna()\n",
    "        if len(valid_speeds) > 0:\n",
    "            norm = plt.Normalize(valid_speeds.min(), valid_speeds.max())\n",
    "        else:\n",
    "            # Fallback if no valid speed values\n",
    "            norm = plt.Normalize(0, 30)  # Reasonable default for km/h\n",
    "        \n",
    "        cmap = plt.cm.viridis\n",
    "\n",
    "        def style_callback(feature):\n",
    "            speed = feature['properties'][speed_field] if speed_field in feature['properties'] else None\n",
    "            color = '#808080' if pd.isna(speed) else mcolors.rgb2hex(cmap(norm(speed)))\n",
    "            return {'color': color, 'weight': 3, 'opacity': 0.7}\n",
    "\n",
    "    geo_json = ipyleaflet.GeoJSON(\n",
    "        data=json.loads(gdf_wgs84.to_json()),\n",
    "        style={'weight': 3, 'opacity': 0.7},\n",
    "        hover_style={'color': 'red', 'fillOpacity': 0.9},\n",
    "        style_callback=style_callback\n",
    "    )\n",
    "    m.add_layer(geo_json)\n",
    "    \n",
    "    # Add the original geometries if provided and not already included\n",
    "    if original_gdf is not None and 'is_original' not in gdf_wgs84.columns:\n",
    "        # Convert to same CRS\n",
    "        original_wgs84 = original_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "        # Add as a highlighted layer\n",
    "        original_json = ipyleaflet.GeoJSON(\n",
    "            data=json.loads(original_wgs84.to_json()),\n",
    "            style={\n",
    "                'color': '#FF0000',  # Red color\n",
    "                'weight': 5,         # Thicker line\n",
    "                'opacity': 1.0,      # Fully opaque\n",
    "                'dashArray': '5,5'   # Dashed line pattern\n",
    "            },\n",
    "            hover_style={\n",
    "                'color': '#FF00FF',  # Magenta on hover\n",
    "                'weight': 7\n",
    "            },\n",
    "            name='Original Reference'\n",
    "        )\n",
    "        m.add_layer(original_json)\n",
    "    \n",
    "    return m, draw_control\n",
    "\n",
    "# Create analysis widgets\n",
    "def create_analysis_widgets(gdf, m, draw_control):\n",
    "    \"\"\"Create analysis widgets with comparison mode\"\"\"\n",
    "    global ORIGINAL_GDF\n",
    "    ORIGINAL_GDF = gdf.copy()\n",
    "    \n",
    "    metrics, time_periods = get_available_metrics_and_times(gdf)\n",
    "    all_metrics = ['General Comparison'] + metrics\n",
    "    \n",
    "    # Create widgets\n",
    "    comparison_mode = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Comparison Mode',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    metric_selector = widgets.Dropdown(\n",
    "        options=all_metrics,\n",
    "        value=all_metrics[0],\n",
    "        description='Metric:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    time_selection1 = widgets.SelectMultiple(\n",
    "        options=time_periods,\n",
    "        description='Period 1:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': 'auto'}\n",
    "    )\n",
    "    \n",
    "    time_selection2 = widgets.Box([\n",
    "        widgets.SelectMultiple(\n",
    "            options=time_periods,\n",
    "            description='Period 2:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout={'width': 'auto'}\n",
    "        )\n",
    "    ], layout=widgets.Layout(display='none'))\n",
    "    \n",
    "    # Add segment type filter if 'is_original' column exists\n",
    "    if 'is_original' in gdf.columns:\n",
    "        segment_filter = widgets.Dropdown(\n",
    "            options=[('All Segments', 'all'), \n",
    "                     ('Original Segments Only', 'original'), \n",
    "                     ('Parallel Segments Only', 'parallel')],\n",
    "            value='all',\n",
    "            description='Segments:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "    else:\n",
    "        segment_filter = None\n",
    "    \n",
    "    analyze_selection_button = widgets.Button(\n",
    "        description='Analyze Selection',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    analyze_all_button = widgets.Button(\n",
    "        description='Analyze All',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    clear_button = widgets.Button(\n",
    "        description='Clear Selection',\n",
    "        button_style='warning'\n",
    "    )\n",
    "    \n",
    "    stats_output = widgets.Output()\n",
    "    plot_output = widgets.Output()\n",
    "    \n",
    "    # Toggle comparison mode\n",
    "    def on_comparison_toggle(change):\n",
    "        time_selection2.layout.display = 'flex' if change.new else 'none'\n",
    "    \n",
    "    comparison_mode.observe(on_comparison_toggle, names='value')\n",
    "\n",
    "    def analyze_data(gdf_to_analyze):\n",
    "        \"\"\"Analyze data based on selected metrics and time periods with improved legend formatting\"\"\"\n",
    "        selected_metric = metric_selector.value\n",
    "        selected_dates1 = time_selection1.value\n",
    "        selected_dates2 = time_selection2.children[0].value if comparison_mode.value else None\n",
    "        \n",
    "        # Filter by segment type if applicable\n",
    "        if segment_filter is not None:\n",
    "            filter_value = segment_filter.value\n",
    "            if filter_value == 'original':\n",
    "                gdf_to_analyze = gdf_to_analyze[gdf_to_analyze['is_original'] == True]\n",
    "            elif filter_value == 'parallel':\n",
    "                gdf_to_analyze = gdf_to_analyze[gdf_to_analyze['is_original'] == False]\n",
    "        \n",
    "        if not selected_dates1 or (comparison_mode.value and not selected_dates2):\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(\"Please select time periods for comparison\")\n",
    "            return\n",
    "                \n",
    "        with plot_output:\n",
    "            plot_output.clear_output()\n",
    "            result1 = process_temporal_data(gdf_to_analyze, selected_metric, selected_dates1)\n",
    "            result2 = process_temporal_data(gdf_to_analyze, selected_metric, selected_dates2) if comparison_mode.value else None\n",
    "            \n",
    "            if result1 is None or (comparison_mode.value and result2 is None):\n",
    "                print(\"No data available for selected combination\")\n",
    "                return\n",
    "            \n",
    "            if comparison_mode.value:\n",
    "                # Comparison mode visualizations\n",
    "                if selected_metric == 'General Comparison':\n",
    "                    fig = plt.figure(figsize=(20, 12))\n",
    "                    \n",
    "                    # Speed distributions\n",
    "                    plt.subplot(2, 4, 1)\n",
    "                    plt.bar(range(len(result1['speeds'])), result1['speeds'])\n",
    "                    plt.title(f'Speed Distribution\\n{format_date_range(selected_dates1)}')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                    plt.subplot(2, 4, 2)\n",
    "                    plt.bar(range(len(result2['speeds'])), result2['speeds'])\n",
    "                    plt.title(f'Speed Distribution\\n{format_date_range(selected_dates2)}')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                    # Other metrics side by side\n",
    "                    metrics = ['cv', 'ent', 'skew']\n",
    "                    for i, m in enumerate(metrics):\n",
    "                        plt.subplot(2, 4, i+3)\n",
    "                        plt.hist([result1[m], result2[m]], bins=30, label=[format_date_range(selected_dates1), format_date_range(selected_dates2)])\n",
    "                        plt.title(f'{m.capitalize()} Distribution')\n",
    "                        plt.xlabel('Value')\n",
    "                        plt.ylabel('Count')\n",
    "                        plt.legend()\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Print comparison statistics\n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Create descriptive period labels\n",
    "                        period1_label = format_date_range(selected_dates1)\n",
    "                        period2_label = format_date_range(selected_dates2) if selected_dates2 else \"\"\n",
    "                        \n",
    "                        # Calculate route count sums for both periods\n",
    "                        route_count1 = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count1 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        route_count2 = 0\n",
    "                        for date in selected_dates2:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count2 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        # Use pre-calculated speed values if available\n",
    "                        if 'speed' in result1 and 'speed' in result2:\n",
    "                            speed1 = np.mean(result1['speed'])\n",
    "                            speed2 = np.mean(result2['speed'])\n",
    "                        else:\n",
    "                            # Fallback to calculating from histograms\n",
    "                            speed1 = sum(i * c for i, c in enumerate(result1['speeds'])) / sum(result1['speeds']) if sum(result1['speeds']) > 0 else 0\n",
    "                            speed2 = sum(i * c for i, c in enumerate(result2['speeds'])) / sum(result2['speeds']) if sum(result2['speeds']) > 0 else 0\n",
    "                        \n",
    "                        # Basic stats dictionary\n",
    "                        stats_dict = {\n",
    "                            'Period': [period1_label, period2_label],\n",
    "                            'Speed': [speed1, speed2],\n",
    "                            'CV': [np.mean(result1['cv']), np.mean(result2['cv'])],\n",
    "                            'Entropy': [np.mean(result1['ent']), np.mean(result2['ent'])],\n",
    "                            'Skewness': [np.mean(result1['skew']), np.mean(result2['skew'])],\n",
    "                            'Route Count': [route_count1, route_count2]\n",
    "                        }\n",
    "                        \n",
    "                        # Add quantiles\n",
    "                        if 'q1' in result1 and 'q1' in result2:\n",
    "                            stats_dict['Q1'] = [np.mean(result1['q1']), np.mean(result2['q1'])]\n",
    "                        if 'q2' in result1 and 'q2' in result2:\n",
    "                            stats_dict['Median'] = [np.mean(result1['q2']), np.mean(result2['q2'])]\n",
    "                        if 'q3' in result1 and 'q3' in result2:\n",
    "                            stats_dict['Q3'] = [np.mean(result1['q3']), np.mean(result2['q3'])]\n",
    "                        if 'iqr' in result1 and 'iqr' in result2:\n",
    "                            stats_dict['IQR'] = [np.mean(result1['iqr']), np.mean(result2['iqr'])]\n",
    "                        \n",
    "                        # Create the DataFrame with basic stats for display\n",
    "                        df_stats = pd.DataFrame(stats_dict).round(2)\n",
    "                        \n",
    "                        # Display human-readable format\n",
    "                        print(\"\\nComparison Statistics:\")\n",
    "                        print(df_stats.to_string(index=False))\n",
    "                        \n",
    "                        # For Excel copy-paste, combine main statistics and speed percentages in one section\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        \n",
    "                        # Combined headers for all statistics\n",
    "                        headers = ['Period', 'Speed', 'CV', 'Entropy', 'Skewness', 'Route Count']\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            headers.extend(['Q1', 'Median', 'Q3', 'IQR'])\n",
    "                        \n",
    "                        # Add speed percentage headers\n",
    "                        if all(key in result1 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']) and all(key in result2 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            headers.extend(['0-5 km/h', '5-15 km/h', '15-25 km/h', '25+ km/h'])\n",
    "                        \n",
    "                        print('\\t'.join(headers))\n",
    "                        \n",
    "                        # First data row with all metrics\n",
    "                        row1 = [period1_label, f\"{speed1:.2f}\", f\"{np.mean(result1['cv']):.2f}\", \n",
    "                                f\"{np.mean(result1['ent']):.2f}\", f\"{np.mean(result1['skew']):.2f}\",\n",
    "                                f\"{route_count1}\"]\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            row1.extend([f\"{np.mean(result1['q1']):.2f}\", f\"{np.mean(result1['q2']):.2f}\", \n",
    "                                        f\"{np.mean(result1['q3']):.2f}\", f\"{np.mean(result1['iqr']):.2f}\"])\n",
    "                        \n",
    "                        # Add speed percentages if available\n",
    "                        if all(key in result1 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            row1.extend([\n",
    "                                f\"{np.mean(result1['p_0_5']):.2f}\", \n",
    "                                f\"{np.mean(result1['p_5_15']):.2f}\", \n",
    "                                f\"{np.mean(result1['p_15_25']):.2f}\", \n",
    "                                f\"{np.mean(result1['p_25p']):.2f}\"\n",
    "                            ])\n",
    "                        \n",
    "                        print('\\t'.join(row1))\n",
    "                        \n",
    "                        # Second data row with all metrics\n",
    "                        row2 = [period2_label, f\"{speed2:.2f}\", f\"{np.mean(result2['cv']):.2f}\", \n",
    "                                f\"{np.mean(result2['ent']):.2f}\", f\"{np.mean(result2['skew']):.2f}\",\n",
    "                                f\"{route_count2}\"]\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            row2.extend([f\"{np.mean(result2['q1']):.2f}\", f\"{np.mean(result2['q2']):.2f}\", \n",
    "                                        f\"{np.mean(result2['q3']):.2f}\", f\"{np.mean(result2['iqr']):.2f}\"])\n",
    "                        \n",
    "                        # Add speed percentages if available\n",
    "                        if all(key in result2 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            row2.extend([\n",
    "                                f\"{np.mean(result2['p_0_5']):.2f}\", \n",
    "                                f\"{np.mean(result2['p_5_15']):.2f}\", \n",
    "                                f\"{np.mean(result2['p_15_25']):.2f}\", \n",
    "                                f\"{np.mean(result2['p_25p']):.2f}\"\n",
    "                            ])\n",
    "                        \n",
    "                        print('\\t'.join(row2))\n",
    "                        print(\"----------------------------------------\")\n",
    "                        \n",
    "                elif selected_metric == 'speeds':\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    ax1.bar(range(len(result1)), result1)\n",
    "                    ax1.set_title(f'Speed Distribution\\n{format_date_range(selected_dates1)}')\n",
    "                    ax1.set_xlabel('Speed (km/h)')\n",
    "                    ax1.set_ylabel('Count')\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    ax2.bar(range(len(result2)), result2)\n",
    "                    ax2.set_title(f'Speed Distribution\\n{format_date_range(selected_dates2)}')\n",
    "                    ax2.set_xlabel('Speed (km/h)')\n",
    "                    ax2.set_ylabel('Count')\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route counts\n",
    "                        route_count1 = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count1 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        route_count2 = 0\n",
    "                        for date in selected_dates2:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count2 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        print(f\"\\n{format_date_range(selected_dates1)}:\")\n",
    "                        print(f\"Total measurements: {sum(result1):,}\")\n",
    "                        print(f\"Route count: {route_count1}\")\n",
    "                        mean_speed1 = sum(i * count for i, count in enumerate(result1)) / sum(result1) if sum(result1) > 0 else 0\n",
    "                        print(f\"Mean speed: {mean_speed1:.2f} km/h\")\n",
    "                        \n",
    "                        print(f\"\\n{format_date_range(selected_dates2)}:\")\n",
    "                        print(f\"Total measurements: {sum(result2):,}\")\n",
    "                        print(f\"Route count: {route_count2}\")\n",
    "                        mean_speed2 = sum(i * count for i, count in enumerate(result2)) / sum(result2) if sum(result2) > 0 else 0\n",
    "                        print(f\"Mean speed: {mean_speed2:.2f} km/h\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Period', 'Measurements', 'Route Count', 'Mean Speed']))\n",
    "                        print(f\"{format_date_range(selected_dates1)}\\t{sum(result1)}\\t{route_count1}\\t{mean_speed1:.2f}\")\n",
    "                        print(f\"{format_date_range(selected_dates2)}\\t{sum(result2)}\\t{route_count2}\\t{mean_speed2:.2f}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                \n",
    "                else:\n",
    "                    # Other metrics comparison\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    ax1.hist(result1, bins=30)\n",
    "                    ax1.set_title(f'{selected_metric.capitalize()} Distribution\\n{format_date_range(selected_dates1)}')\n",
    "                    ax1.set_xlabel('Value')\n",
    "                    ax1.set_ylabel('Count')\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    ax2.hist(result2, bins=30)\n",
    "                    ax2.set_title(f'{selected_metric.capitalize()} Distribution\\n{format_date_range(selected_dates2)}')\n",
    "                    ax2.set_xlabel('Value')\n",
    "                    ax2.set_ylabel('Count')\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route counts\n",
    "                        route_count1 = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count1 += gdf_to_analyze[route_count_col].sum()\n",
    "                                \n",
    "                        route_count2 = 0\n",
    "                        for date in selected_dates2:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count2 += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        print(f\"\\n{format_date_range(selected_dates1)} {selected_metric}: {np.mean(result1):.2f}\")\n",
    "                        print(f\"Route count: {route_count1}\")\n",
    "                        print(f\"{format_date_range(selected_dates2)} {selected_metric}: {np.mean(result2):.2f}\")\n",
    "                        print(f\"Route count: {route_count2}\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Period', selected_metric, 'Route Count']))\n",
    "                        print(f\"{format_date_range(selected_dates1)}\\t{np.mean(result1):.2f}\\t{route_count1}\")\n",
    "                        print(f\"{format_date_range(selected_dates2)}\\t{np.mean(result2):.2f}\\t{route_count2}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "            else:\n",
    "                # Single period visualizations\n",
    "                if selected_metric == 'General Comparison':\n",
    "                    plt.figure(figsize=(15, 10))\n",
    "                    \n",
    "                    plt.subplot(2, 2, 1)\n",
    "                    plt.bar(range(len(result1['speeds'])), result1['speeds'])\n",
    "                    plt.title(f'Speed Distribution\\n{format_date_range(selected_dates1)}')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                    metrics = ['cv', 'ent', 'skew']\n",
    "                    for i, m in enumerate(metrics, 2):\n",
    "                        plt.subplot(2, 2, i)\n",
    "                        plt.hist(result1[m], bins=30)\n",
    "                        plt.title(f'{m.capitalize()} Distribution')\n",
    "                        plt.xlabel('Value')\n",
    "                        plt.ylabel('Count')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route count sum\n",
    "                        route_count = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        # Use pre-calculated speed values if available\n",
    "                        if 'speed' in result1:\n",
    "                            speed = np.mean(result1['speed'])\n",
    "                        else:\n",
    "                            # Fallback to calculating from histograms\n",
    "                            speed = sum(i * c for i, c in enumerate(result1['speeds'])) / sum(result1['speeds']) if sum(result1['speeds']) > 0 else 0\n",
    "                        \n",
    "                        # Basic stats dictionary (using traditional format for human readability)\n",
    "                        stats_dict = {\n",
    "                            'Speed': speed,\n",
    "                            'CV': np.mean(result1['cv']),\n",
    "                            'Entropy': np.mean(result1['ent']),\n",
    "                            'Skewness': np.mean(result1['skew']),\n",
    "                            'Route Count': route_count\n",
    "                        }\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'q1' in result1:\n",
    "                            stats_dict['Q1'] = np.mean(result1['q1'])\n",
    "                        if 'q2' in result1:\n",
    "                            stats_dict['Median'] = np.mean(result1['q2'])\n",
    "                        if 'q3' in result1:\n",
    "                            stats_dict['Q3'] = np.mean(result1['q3'])\n",
    "                        if 'iqr' in result1:\n",
    "                            stats_dict['IQR'] = np.mean(result1['iqr'])\n",
    "                        \n",
    "                        # Display human-readable format\n",
    "                        print(\"\\nMetric Statistics:\")\n",
    "                        df_stats = pd.DataFrame([stats_dict]).round(2)\n",
    "                        print(df_stats.to_string())\n",
    "                        \n",
    "                        # For Excel copy-paste, combine all statistics in one section\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        \n",
    "                        # Create a comprehensive table with metrics and speed percentages in one section\n",
    "                        # Start with the main metrics\n",
    "                        metrics = ['Speed', 'CV', 'Entropy', 'Skewness', 'Route Count']\n",
    "                        \n",
    "                        # Add quantiles if available\n",
    "                        if 'Q1' in stats_dict:\n",
    "                            metrics.extend(['Q1', 'Median', 'Q3', 'IQR'])\n",
    "                        \n",
    "                        # Add speed percentage metrics if available\n",
    "                        if all(key in result1 for key in ['p_0_5', 'p_5_15', 'p_15_25', 'p_25p']):\n",
    "                            metrics.extend(['0-5 km/h', '5-15 km/h', '15-25 km/h', '25+ km/h'])\n",
    "                        \n",
    "                        # Create the header\n",
    "                        print('\\t'.join(['Metric', 'Value']))\n",
    "                        \n",
    "                        # Print values for each metric\n",
    "                        for metric in metrics:\n",
    "                            if metric == 'Speed':\n",
    "                                value = f\"{stats_dict['Speed']:.2f}\"\n",
    "                            elif metric == 'CV':\n",
    "                                value = f\"{stats_dict['CV']:.2f}\"\n",
    "                            elif metric == 'Entropy':\n",
    "                                value = f\"{stats_dict['Entropy']:.2f}\"\n",
    "                            elif metric == 'Skewness':\n",
    "                                value = f\"{stats_dict['Skewness']:.2f}\"\n",
    "                            elif metric == 'Route Count':\n",
    "                                value = f\"{stats_dict['Route Count']}\"\n",
    "                            # Quantiles\n",
    "                            elif metric in ['Q1', 'Median', 'Q3', 'IQR'] and metric in stats_dict:\n",
    "                                value = f\"{stats_dict[metric]:.2f}\"\n",
    "                            # Speed ranges    \n",
    "                            elif metric == '0-5 km/h' and 'p_0_5' in result1:\n",
    "                                value = f\"{np.mean(result1['p_0_5']):.2f}\"\n",
    "                            elif metric == '5-15 km/h' and 'p_5_15' in result1:\n",
    "                                value = f\"{np.mean(result1['p_5_15']):.2f}\" \n",
    "                            elif metric == '15-25 km/h' and 'p_15_25' in result1:\n",
    "                                value = f\"{np.mean(result1['p_15_25']):.2f}\"\n",
    "                            elif metric == '25+ km/h' and 'p_25p' in result1:\n",
    "                                value = f\"{np.mean(result1['p_25p']):.2f}\"\n",
    "                            else:\n",
    "                                value = \"N/A\"\n",
    "                                \n",
    "                            print(f\"{metric}\\t{value}\")\n",
    "                            \n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                elif selected_metric == 'speeds':\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.bar(range(len(result1)), result1)\n",
    "                    plt.title(f'Speed Distribution\\n({format_date_range(selected_dates1)})')\n",
    "                    plt.xlabel('Speed (km/h)')\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route count\n",
    "                        route_count = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count += gdf_to_analyze[route_count_col].sum()\n",
    "                                \n",
    "                        print(f\"Total measurements: {sum(result1):,}\")\n",
    "                        print(f\"Route count: {route_count}\")\n",
    "                        mean_speed = sum(i * count for i, count in enumerate(result1)) / sum(result1) if sum(result1) > 0 else 0\n",
    "                        print(f\"Mean speed: {mean_speed:.2f} km/h\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Measurements', 'Route Count', 'Mean Speed']))\n",
    "                        print(f\"{sum(result1)}\\t{route_count}\\t{mean_speed:.2f}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                \n",
    "                else:\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.hist(result1, bins=30)\n",
    "                    plt.title(f'{selected_metric.capitalize()} Distribution\\n({format_date_range(selected_dates1)})')\n",
    "                    plt.xlabel('Value')\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    with stats_output:\n",
    "                        stats_output.clear_output()\n",
    "                        print(f\"\\nAnalyzing {len(gdf_to_analyze)} features\")\n",
    "                        \n",
    "                        # Calculate route count\n",
    "                        route_count = 0\n",
    "                        for date in selected_dates1:\n",
    "                            year = date.split('-')[0][-2:]\n",
    "                            month = date.split('-')[1]\n",
    "                            route_count_col = f\"{year}-{month}_route_count\"\n",
    "                            if route_count_col in gdf_to_analyze.columns:\n",
    "                                route_count += gdf_to_analyze[route_count_col].sum()\n",
    "                        \n",
    "                        print(f\"\\n{selected_metric}: {np.mean(result1):.2f}\")\n",
    "                        print(f\"Route count: {route_count}\")\n",
    "                        \n",
    "                        # Excel copy-paste format\n",
    "                        print(\"\\nFor Excel (copy entire section below):\")\n",
    "                        print(\"----------------------------------------\")\n",
    "                        print('\\t'.join(['Metric', 'Value', 'Route Count']))\n",
    "                        print(f\"{selected_metric}\\t{np.mean(result1):.2f}\\t{route_count}\")\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "    def on_analyze_selection_click(b):\n",
    "        try:\n",
    "            drawn_features = draw_control.data\n",
    "            if not drawn_features:\n",
    "                with stats_output:\n",
    "                    stats_output.clear_output()\n",
    "                    print(\"Please draw at least one shape\")\n",
    "                return\n",
    "            \n",
    "            selection_shapes = [shape(feature['geometry']) for feature in drawn_features]\n",
    "            combined_shape = selection_shapes[0]\n",
    "            for s in selection_shapes[1:]:\n",
    "                combined_shape = combined_shape.union(s)\n",
    "            \n",
    "            gdf_wgs84 = ORIGINAL_GDF.to_crs('EPSG:4326')\n",
    "            selected_gdf = gdf_wgs84[gdf_wgs84.intersects(combined_shape)]\n",
    "            \n",
    "            if len(selected_gdf) == 0:\n",
    "                with stats_output:\n",
    "                    stats_output.clear_output()\n",
    "                    print(\"No features found in selection\")\n",
    "                return\n",
    "            \n",
    "            analyze_data(selected_gdf)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(f\"Error analyzing selection: {str(e)}\")\n",
    "\n",
    "    def on_analyze_all_click(b):\n",
    "        try:\n",
    "            analyze_data(ORIGINAL_GDF.to_crs('EPSG:4326'))\n",
    "        except Exception as e:\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(f\"Error analyzing all features: {str(e)}\")\n",
    "    \n",
    "    def on_clear_click(b):\n",
    "        draw_control.clear()\n",
    "        stats_output.clear_output()\n",
    "        plot_output.clear_output()\n",
    "        \n",
    "    analyze_selection_button.on_click(on_analyze_selection_click)\n",
    "    analyze_all_button.on_click(on_analyze_all_click)\n",
    "    clear_button.on_click(on_clear_click)\n",
    "    \n",
    "    # Arrange widgets\n",
    "    control_widgets = [\n",
    "        widgets.HTML(value=\"<b>Analysis Controls:</b>\"),\n",
    "        comparison_mode,\n",
    "        metric_selector,\n",
    "        time_selection1,\n",
    "        time_selection2\n",
    "    ]\n",
    "    \n",
    "    # Add segment filter if available\n",
    "    if segment_filter is not None:\n",
    "        control_widgets.insert(3, segment_filter)\n",
    "    \n",
    "    control_widgets.append(widgets.HBox([analyze_selection_button, analyze_all_button, clear_button]))\n",
    "    control_widgets.append(stats_output)\n",
    "    control_widgets.append(plot_output)\n",
    "    \n",
    "    controls = widgets.VBox(control_widgets)\n",
    "    \n",
    "    return controls\n",
    "\n",
    "def initialize_analysis(gdf, original_gdf=None):\n",
    "    \"\"\"Initialize the analysis environment with optional original geometries\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The segments to analyze (gdf3)\n",
    "    original_gdf : GeoDataFrame, optional\n",
    "        The original reference geometry\n",
    "    \"\"\"\n",
    "    # Create map and controls\n",
    "    m, draw_control = create_interactive_map(gdf, original_gdf)\n",
    "    controls = create_analysis_widgets(gdf, m, draw_control)\n",
    "    \n",
    "    # Display the map and controls\n",
    "    display(m)\n",
    "    display(controls)\n",
    "    \n",
    "    # Don't return anything to avoid \"None\" output\n",
    "    return None\n",
    "\n",
    "# Initialize the analysis tools with your existing gdf3 data\n",
    "# Here's how you would use it with original gdf overlay:\n",
    "# initialize_analysis(gdf3, original_gdf=gdf)\n",
    "\n",
    "# Or without original reference geometry:\n",
    "initialize_analysis(gdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f300bf-6aca-45f3-aedc-fb3374771b54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Tried reading 423234 bytes starting at position 220996934 from file but only got 113",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_col_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m column. Stats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, stats\n\u001b[0;32m---> 58\u001b[0m success, stats \u001b[38;5;241m=\u001b[39m \u001b[43madd_column_with_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdf3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFS_wallstr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124mJoin Statistics:\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124m- Total IDs in GDF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_ids_in_gdf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124m- Matched IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids_found_in_parquet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124m- Unmatched IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munmatched_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36madd_column_with_stats\u001b[0;34m(parquet_path, gdf, new_col_name, id_field)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mParquetFile(parquet_path) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(reader\u001b[38;5;241m.\u001b[39mnum_row_groups):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Load just IDs from this chunk\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         chunk_ids \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_row_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mid_field\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcolumn(id_field)\n\u001b[1;32m     23\u001b[0m         chunk_ids_pd \u001b[38;5;241m=\u001b[39m chunk_ids\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Find matches and track statistics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.12/site-packages/pyarrow/parquet/core.py:467\u001b[0m, in \u001b[0;36mParquetFile.read_row_group\u001b[0;34m(self, i, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mRead a single row group from a Parquet file.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03manimal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m column_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_column_indices(\n\u001b[1;32m    466\u001b[0m     columns, use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[0;32m--> 467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_row_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.12/site-packages/pyarrow/_parquet.pyx:1655\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_row_group\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.12/site-packages/pyarrow/_parquet.pyx:1685\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_row_groups\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Tried reading 423234 bytes starting at position 220996934 from file but only got 113"
     ]
    }
   ],
   "source": [
    "# Step 4: Join Parallel Segments (gdf3) with parquet file\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "def add_column_with_stats(parquet_path, gdf, new_col_name, id_field='id'):\n",
    "    \"\"\"\n",
    "    Adds column with join statistics, processing row groups individually.\n",
    "    Returns: (success: bool, stats: dict)\n",
    "    \"\"\"\n",
    "    # 1. Prepare matching IDs\n",
    "    gdf_ids = set(gdf[id_field])\n",
    "    total_gdf_ids = len(gdf_ids)\n",
    "    matched_ids = set()\n",
    "    \n",
    "    # 2. Process file in chunks\n",
    "    writer = None\n",
    "    with pq.ParquetFile(parquet_path) as reader:\n",
    "        for i in range(reader.num_row_groups):\n",
    "            # Load just IDs from this chunk\n",
    "            chunk_ids = reader.read_row_group(i, columns=[id_field]).column(id_field)\n",
    "            chunk_ids_pd = chunk_ids.to_pandas()\n",
    "            \n",
    "            # Find matches and track statistics\n",
    "            matches = chunk_ids_pd.isin(gdf_ids)\n",
    "            matched_ids.update(chunk_ids_pd[matches].unique())\n",
    "            \n",
    "            # Create flag column\n",
    "            flags = pa.array(matches.astype('int8'))\n",
    "            \n",
    "            # Read full row group and append column\n",
    "            chunk = reader.read_row_group(i)\n",
    "            chunk = chunk.append_column(new_col_name, flags)\n",
    "            \n",
    "            # Initialize writer if first chunk\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(parquet_path, chunk.schema)\n",
    "            \n",
    "            writer.write_table(chunk)\n",
    "    \n",
    "    # 3. Calculate statistics\n",
    "    stats = {\n",
    "        'total_ids_in_gdf': total_gdf_ids,\n",
    "        'ids_found_in_parquet': len(matched_ids),\n",
    "        'match_percentage': round(len(matched_ids) / total_gdf_ids * 100, 2) if total_gdf_ids else 0,\n",
    "        'unmatched_ids': total_gdf_ids - len(matched_ids)\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"Added {new_col_name} column. Stats: {stats}\")\n",
    "    return True, stats\n",
    "\n",
    "\n",
    "success, stats = add_column_with_stats(\n",
    "    \"data/network_all_months_plus_25833_length_with_fahrradstrasse.parquet\",\n",
    "    gdf3,\n",
    "    \"FS_wallstr\",\n",
    "    id_field='id'\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "Join Statistics:\n",
    "- Total IDs in GDF: {stats['total_ids_in_gdf']}\n",
    "- Matched IDs: {stats['ids_found_in_parquet']} ({stats['match_percentage']}%)\n",
    "- Unmatched IDs: {stats['unmatched_ids']}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
